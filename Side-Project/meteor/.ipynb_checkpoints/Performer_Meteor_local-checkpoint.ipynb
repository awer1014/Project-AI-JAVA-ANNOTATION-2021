{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\W.R_Chen\\AppData\\Local\\Temp\\ipykernel_16700\\337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = \"D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\\code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelset = \"1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QqK8pCD5y-j6"
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import sys\n",
    "sys.path.append(\"D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\\code\")\n",
    "sys.path.append(\"D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\\code\\keras_performer\")\n",
    "sys.path.append(\"D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\\code\\keras_position_wise_feed_forward\")\n",
    "sys.path.append(\"D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\\code\\tensorflow_fast_attention\")\n",
    "import unittest\n",
    "import numpy as np\n",
    "import math\n",
    "from keras_performer import performer_mask as pfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6zwRDujGy-j7"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"rb\")\n",
    "    dt = pickle.load(a_file)\n",
    "    return dt\n",
    "\n",
    "def load(model_name):\n",
    "    from keras_performer import performer_mask\n",
    "    from tensorflow import keras\n",
    "    from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "    from keras_pos_embd import TrigPosEmbedding\n",
    "    from tensorflow_fast_attention.fast_attention_2 import  softmax_kernel_transformation,Attention, SelfAttention    \n",
    "    from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "    co = performer_mask.get_custom_objects()  #需要更正是哪一版的performer\n",
    "    co[\"softmax_kernel_transformation\"] = softmax_kernel_transformation #新增softmax_kernel_transformation\n",
    "    \n",
    "    model = keras.models.load_model(model_name, custom_objects= co)\n",
    "    s = loadDictionary(drive_path + '/source_token_dict.pickle')\n",
    "    t = loadDictionary(drive_path + '/target_token_dict.pickle')\n",
    "    t_inv = loadDictionary(drive_path + '/target_token_dict_inv.pickle')\n",
    "    return model, s, t, t_inv\n",
    "    \n",
    "def decomposeModel(model):\n",
    "    encoder_model = model.get_layer('encoder_model')\n",
    "    classifier_decoder_model = model.get_layer('classifier_decoder_model')\n",
    "    print(encoder_model.summary())\n",
    "    print(classifier_decoder_model.summary())\n",
    "    return encoder_model, classifier_decoder_model\n",
    "        \n",
    "def loadMaxLen(filename):     \n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSSvF0zjy-j8"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hZHM2QkZy-j-",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\result\\model\u0007gain3_checkpoint_1000.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 229>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(Avg_Meteor_print\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m x\u001b[38;5;241m=\u001b[39mTestTranslate()\n\u001b[1;32m--> 229\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mTestTranslate.test_translate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_translate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     model, source_token_dict, target_token_dict, target_token_dict_inv \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCode\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject-AI-JAVA-ANNOTATION-2021\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSide-Project\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmeteor\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43mesult\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;130;43;01m\\a\u001b[39;49;00m\u001b[38;5;124;43mgain3_checkpoint_1000.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#Predict\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#load traing/test data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     source_max_len_loaded \u001b[38;5;241m=\u001b[39m loadMaxLen(drive_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data_reduced/source_max_len.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mload\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     15\u001b[0m co \u001b[38;5;241m=\u001b[39m performer_mask\u001b[38;5;241m.\u001b[39mget_custom_objects()  \u001b[38;5;66;03m#需要更正是哪一版的performer\u001b[39;00m\n\u001b[0;32m     16\u001b[0m co[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax_kernel_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m softmax_kernel_transformation \u001b[38;5;66;03m#新增softmax_kernel_transformation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mco\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m s \u001b[38;5;241m=\u001b[39m loadDictionary(drive_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/source_token_dict.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m t \u001b[38;5;241m=\u001b[39m loadDictionary(drive_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/target_token_dict.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\saving\\save.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    205\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(filepath_str, \u001b[38;5;28mcompile\u001b[39m, options)\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\result\\model\u0007gain3_checkpoint_1000.h5"
     ]
    }
   ],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "        \n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        \n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load('D:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Side-Project\\meteor\\result\\model\\again3_checkpoint_1000.h5') \n",
    "        #Predict\n",
    "        #load traing/test data\n",
    "        source_max_len_loaded = loadMaxLen(drive_path+'train_data_reduced/source_max_len.txt')\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        #print('source_max_len_loaded : ', source_max_len_loaded)\n",
    "        #print('max_seq_len : ',max_seq_len)\n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"train_data_reduced/x_train[0]_0.npy\"))\n",
    "        print('encode_input : ',len(encode_input))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"train_data_reduced/x_train[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"train_data_reduced/y_train[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"train_data_reduced/y_train[1]_0.npy\"))\n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_train_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        print('encode_input : ',encode_input)\n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 1001\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            lmmodelans, errortypes, decoded = pfr.decode(\n",
    "                model,# 拆解成encoder ,decoder\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "        '''   \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        '''\n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_train_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path+'meteor/' + 'checkpoint_'+ 1000 + '_train_type.txt'\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_train_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path  + 'checkpoint_'+ modelset + '_train_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_train_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gwfGyDUy-kC"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niuzp92iy-kD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"checkpoint_\" + modelset + \".h5\")\n",
    "        # Predict\n",
    "        #load traing/test data \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"x_validation[0]_0.npy\"))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"x_validation[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"y_validation[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"y_validation[1]_0.npy\"))\n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_validation_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        \n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 512\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            errortypes, decoded = pfr.decode(\n",
    "                model,\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "        '''   \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        '''\n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_validation_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path + 'checkpoint_'+ modelset + '_validation_type.txt'\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_validation_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path + 'checkpoint_'+ modelset + '_validation_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_validation_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okMR3IfZy-kF"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_v8Sa-wy-kF"
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"checkpoint_\" + modelset + \".h5\")\n",
    "        # Predict\n",
    "        #load traing/test data \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"x_test[0]_0.npy\"))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"x_test[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"y_test[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"y_test[1]_0.npy\"))  \n",
    "        \n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_test_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        \n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 512\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            errortypes, decoded = pfr.decode(\n",
    "                model,\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "         \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        \n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_test_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path + 'checkpoint_'+ modelset + '_test_type.txt'\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_test_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path + 'checkpoint_'+ modelset + '_test_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_test_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Performer_Meteor_206.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
