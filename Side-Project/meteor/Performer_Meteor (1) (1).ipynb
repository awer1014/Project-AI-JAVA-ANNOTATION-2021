{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shiu\\AppData\\Local\\Temp\\ipykernel_14216\\337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = 'C:/Users/shiu/Desktop/206computer/10366021/code/'\n",
    "#modelset = \"1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QqK8pCD5y-j6"
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import sys\n",
    "sys.path.append('C:/Users/shiu/Desktop/206computer/10366021/code')\n",
    "sys.path.append('C:/Users/shiu/Desktop/206computer/10366021/code/keras_position_wise_feed_forward')\n",
    "sys.path.append('C:/Users/shiu/Desktop/206computer/10366021/code/tensorflow_fast_attention')\n",
    "sys.path.append('C:/Users/shiu/Desktop/206computer/10366021/code/keras_performer')\n",
    "import unittest\n",
    "import numpy as np\n",
    "import math\n",
    "from keras_performer import performer_mask as pfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6zwRDujGy-j7"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"rb\")\n",
    "    dt = pickle.load(a_file)\n",
    "    return dt\n",
    "\n",
    "def load(model_name):\n",
    "    from keras_performer import performer_mask\n",
    "    from tensorflow import keras\n",
    "    from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "    from keras_pos_embd import TrigPosEmbedding\n",
    "    from tensorflow_fast_attention.fast_attention_2 import  softmax_kernel_transformation,Attention, SelfAttention    \n",
    "    from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "    co = performer_mask.get_custom_objects()  #需要更正是哪一版的performer\n",
    "    co[\"softmax_kernel_transformation\"] = softmax_kernel_transformation #新增softmax_kernel_transformation\n",
    "    \n",
    "    model = keras.models.load_model(model_name, custom_objects= co)\n",
    "    s = loadDictionary(drive_path + 'train_data_reduced/source_token_dict.pickle')\n",
    "    t = loadDictionary(drive_path + 'train_data_reduced/target_token_dict.pickle')\n",
    "    t_inv = loadDictionary(drive_path + 'train_data_reduced/target_token_dict_inv.pickle')\n",
    "    return model, s, t, t_inv\n",
    "    \n",
    "def decomposeModel(model):\n",
    "    encoder_model = model.get_layer('encoder_model')\n",
    "    classifier_decoder_model = model.get_layer('classifier_decoder_model')\n",
    "    print(encoder_model.summary())\n",
    "    print(classifier_decoder_model.summary())\n",
    "    return encoder_model, classifier_decoder_model\n",
    "        \n",
    "def loadMaxLen(filename):     \n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSSvF0zjy-j8"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hZHM2QkZy-j-",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "output: Tensor(\"cast_mask/SelectV2:0\", shape=(None, 1001), dtype=float32)\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "encode_input :  1\n",
      "encode_input :  [[  1   3  37 ...   0   0   0]\n",
      " [  1   3   8 ...   0   0   0]\n",
      " [  1   3  37 ...   0   0   0]\n",
      " ...\n",
      " [  1   3 671 ...   0   0   0]\n",
      " [  1   3  37 ...   0   0   0]\n",
      " [  1   3  37 ...   0   0   0]]\n",
      "~~~~~~batch_size :  1001\n",
      "<class 'int'> 1001\n",
      "Predict inputs: (1001, 1001) (1001, 1001) (1001, 776)\n",
      "batch_inputs: [[ 1  3 37 ...  0  0  0]\n",
      " [ 1  3  8 ...  0  0  0]\n",
      " [ 1  3 37 ...  0  0  0]\n",
      " ...\n",
      " [ 1  3  8 ...  0  0  0]\n",
      " [ 1  3 37 ...  0  0  0]\n",
      " [ 1  3 37 ...  0  0  0]]\n",
      "batch_inputs: [[ 1  3 37 ...  0  0  0]\n",
      " [ 1  3  8 ...  0  0  0]\n",
      " [ 1  3 37 ...  0  0  0]\n",
      " ...\n",
      " [ 1  3  8 ...  0  0  0]\n",
      " [ 1  3 37 ...  0  0  0]\n",
      " [ 1  3 37 ...  0  0  0]]\n",
      "batch_outputs: [[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "--------------------------------------------------------------------\n",
      "output: Tensor(\"model/cast_mask/SelectV2:0\", shape=(None, 1001), dtype=float32)\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 1001, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "attention_output shape: (None, 776, 32)\n",
      "attention again\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[2,776,1428] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/Decoder-Output/Softmax\n (defined at C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\activations.py:80)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_341320]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/Decoder-Output/Softmax:\nIn[0] model/Decoder-Output/BiasAdd (defined at C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\backend.py:5976)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2900, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3301, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\shiu\\AppData\\Local\\Temp\\ipykernel_14216\\559408342.py\", line 229, in <cell line: 229>\n>>>     x.test_translate()\n>>> \n>>>   File \"C:\\Users\\shiu\\AppData\\Local\\Temp\\ipykernel_14216\\559408342.py\", line 98, in test_translate\n>>>     lmmodelans, errortypes, decoded = pfr.decode(\n>>> \n>>>   File \"C:\\Users/shiu/Desktop/206computer/10366021/code\\keras_performer\\performer_mask.py\", line 905, in decode\n>>>     predictErrorTypes, predicts = model.predict([np.array(lmmodel), np.array(batch_inputs), np.array(batch_outputs)], batch_size=2)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict\n>>>     tmp_batch_outputs = self.predict_function(iterator)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1586, in predict_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1576, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1569, in run_step\n>>>     outputs = model.predict_step(data)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1537, in predict_step\n>>>     return self(x, training=False)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\functional.py\", line 414, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\functional.py\", line 550, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras_embed_sim\\embeddings.py\", line 99, in call\n>>>     return keras.activations.softmax(outputs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\activations.py\", line 80, in softmax\n>>>     output = tf.nn.softmax(x, axis=axis)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 229>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(Avg_Meteor_print\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m x\u001b[38;5;241m=\u001b[39mTestTranslate()\n\u001b[1;32m--> 229\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mTestTranslate.test_translate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m data_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1001\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(encode_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mdata_size)):\n\u001b[1;32m---> 98\u001b[0m     lmmodelans, errortypes, decoded \u001b[38;5;241m=\u001b[39m \u001b[43mpfr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# 拆解成encoder ,decoder\u001b[39;49;00m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencode_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\u001b[39;49;00m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_token_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<START>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_token_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<END>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_token_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<PAD>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_max_len\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m#[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     result_err\u001b[38;5;241m.\u001b[39mappend(errortypes)\n",
      "File \u001b[1;32mC:\\Users/shiu/Desktop/206computer/10366021/code\\keras_performer\\performer_mask.py:905\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(model, tokens, start_token, end_token, pad_token, top_k, temperature, max_len, max_repeat, max_repeat_block)\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_inputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m,np\u001b[38;5;241m.\u001b[39marray(batch_inputs))\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_outputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m,np\u001b[38;5;241m.\u001b[39marray(batch_outputs))\n\u001b[1;32m--> 905\u001b[0m         predictErrorTypes, predicts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlmmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(predicts)): \u001b[38;5;66;03m#for i'th predict\u001b[39;00m\n\u001b[0;32m    907\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m top_k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    908\u001b[0m \u001b[38;5;66;03m#                print(\"predicts[i][-1]: \", predicts[i][-1])\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py:1751\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1750\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 1751\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1753\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[2,776,1428] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/Decoder-Output/Softmax\n (defined at C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\activations.py:80)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_341320]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/Decoder-Output/Softmax:\nIn[0] model/Decoder-Output/BiasAdd (defined at C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\backend.py:5976)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2900, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3301, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\shiu\\AppData\\Local\\Temp\\ipykernel_14216\\559408342.py\", line 229, in <cell line: 229>\n>>>     x.test_translate()\n>>> \n>>>   File \"C:\\Users\\shiu\\AppData\\Local\\Temp\\ipykernel_14216\\559408342.py\", line 98, in test_translate\n>>>     lmmodelans, errortypes, decoded = pfr.decode(\n>>> \n>>>   File \"C:\\Users/shiu/Desktop/206computer/10366021/code\\keras_performer\\performer_mask.py\", line 905, in decode\n>>>     predictErrorTypes, predicts = model.predict([np.array(lmmodel), np.array(batch_inputs), np.array(batch_outputs)], batch_size=2)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict\n>>>     tmp_batch_outputs = self.predict_function(iterator)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1586, in predict_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1576, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1569, in run_step\n>>>     outputs = model.predict_step(data)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\training.py\", line 1537, in predict_step\n>>>     return self(x, training=False)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\functional.py\", line 414, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\functional.py\", line 550, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras_embed_sim\\embeddings.py\", line 99, in call\n>>>     return keras.activations.softmax(outputs)\n>>> \n>>>   File \"C:\\Users\\shiu\\anaconda3\\envs\\python3711\\lib\\site-packages\\keras\\activations.py\", line 80, in softmax\n>>>     output = tf.nn.softmax(x, axis=axis)\n>>> "
     ]
    }
   ],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "        \n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        \n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load('C:/Users/shiu/Desktop/result/model/again3_checkpoint_1000.h5') \n",
    "        # Predict\n",
    "        #load traing/test data\n",
    "        source_max_len_loaded = loadMaxLen(drive_path+'train_data_reduced/source_max_len.txt')\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        #print('source_max_len_loaded : ', source_max_len_loaded)\n",
    "        #print('max_seq_len : ',max_seq_len)\n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"train_data_reduced/x_train[0]_0.npy\"))\n",
    "        print('encode_input : ',len(encode_input))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"train_data_reduced/x_train[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"train_data_reduced/y_train[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"train_data_reduced/y_train[1]_0.npy\"))\n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_train_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        print('encode_input : ',encode_input)\n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 1001\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            lmmodelans, errortypes, decoded = pfr.decode(\n",
    "                model,# 拆解成encoder ,decoder\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "        '''   \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        '''\n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_train_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path+'meteor/' + 'checkpoint_'+ 1000 + '_train_type.txt'\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_train_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path  + 'checkpoint_'+ modelset + '_train_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_train_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gwfGyDUy-kC"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niuzp92iy-kD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"checkpoint_\" + modelset + \".h5\")\n",
    "        # Predict\n",
    "        #load traing/test data \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"x_validation[0]_0.npy\"))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"x_validation[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"y_validation[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"y_validation[1]_0.npy\"))\n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_validation_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        \n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 512\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            errortypes, decoded = pfr.decode(\n",
    "                model,\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "        '''   \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        '''\n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_validation_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path + 'checkpoint_'+ modelset + '_validation_type.txt'\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_validation_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path + 'checkpoint_'+ modelset + '_validation_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_validation_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okMR3IfZy-kF"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_v8Sa-wy-kF"
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"checkpoint_\" + modelset + \".h5\")\n",
    "        # Predict\n",
    "        #load traing/test data \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"x_test[0]_0.npy\"))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"x_test[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"y_test[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"y_test[1]_0.npy\"))  \n",
    "        \n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_test_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        \n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 512\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            errortypes, decoded = pfr.decode(\n",
    "                model,\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "         \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        \n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_test_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path + 'checkpoint_'+ modelset + '_test_type.txt'\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_test_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path + 'checkpoint_'+ modelset + '_test_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_test_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Performer_Meteor_206.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
