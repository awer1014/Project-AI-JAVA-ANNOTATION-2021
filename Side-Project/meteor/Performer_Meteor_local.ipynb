{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = \"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelset = \"1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqK8pCD5y-j6"
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import sys\n",
    "sys.path.append(\"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/code\")\n",
    "sys.path.append(\"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/code/keras_performer\")\n",
    "sys.path.append(\"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/code/keras_position_wise_feed_forward\")\n",
    "sys.path.append(\"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/code/tensorflow_fast_attention\")\n",
    "import unittest\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import math\n",
    "import nltk\n",
    "from keras_performer import performer_mask as pfr\n",
    "from sklearn import metrics\n",
    "import nltk.translate.meteor_score as meteor\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zwRDujGy-j7"
   },
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length\n",
    "\n",
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length\n",
    "        \n",
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x\n",
    "\n",
    "def parseSentence(x):\n",
    "    tokenizer =  RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        elif (ord(x[i])>=48 and ord(x[i])<=57):\n",
    "            inp=\"D\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])                \n",
    "            \n",
    "        elif state==\"ASCII\":\t\n",
    "            if inp==\"D\" or inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs) #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"NUMBER\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\t\t\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs) #nltk.word_tokenize(chrs)\n",
    "    return replaceTAGS(tokens)\n",
    "\n",
    "def loadDictionary(file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"rb\")\n",
    "    dt = pickle.load(a_file)\n",
    "    return dt\n",
    "\n",
    "def load(model_name):\n",
    "    from keras_performer import performer_mask\n",
    "    from tensorflow import keras\n",
    "    from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "    from keras_pos_embd import TrigPosEmbedding\n",
    "    from tensorflow_fast_attention.fast_attention_2 import  softmax_kernel_transformation,Attention, SelfAttention    \n",
    "    from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "    co = performer_mask.get_custom_objects()  #需要更正是哪一版的performer\n",
    "    co[\"softmax_kernel_transformation\"] = softmax_kernel_transformation #新增softmax_kernel_transformation\n",
    "    \n",
    "    model = keras.models.load_model(model_name, custom_objects= co)\n",
    "    s = loadDictionary(\"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/train_data_reduced/source_token_dict.pickle\")\n",
    "    t = loadDictionary(\"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/train_data_reduced/target_token_dict.pickle\")\n",
    "    t_inv = loadDictionary(\"D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/train_data_reduced/target_token_dict_inv.pickle\")\n",
    "    return model, s, t, t_inv\n",
    "    \n",
    "def decomposeModel(model):\n",
    "    encoder_model = model.get_layer('encoder_model')\n",
    "    classifier_decoder_model = model.get_layer('classifier_decoder_model')\n",
    "    print(encoder_model.summary())\n",
    "    print(classifier_decoder_model.summary())\n",
    "    return encoder_model, classifier_decoder_model\n",
    "        \n",
    "def loadMaxLen(filename):     \n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSSvF0zjy-j8"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZHM2QkZy-j-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "        \n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        \n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load('D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/meteor/result/model/again3_checkpoint_1000.h5') \n",
    "        #Predict\n",
    "        #load traing/test data\n",
    "        source_max_len_loaded = loadMaxLen(drive_path+'train_data_reduced/source_max_len.txt')\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        #print('source_max_len_loaded : ', source_max_len_loaded)\n",
    "        #print('max_seq_len : ',max_seq_len)\n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"train_data_reduced/x_validation[0]_0.npy\"))\n",
    "        print('encode_input : ',len(encode_input))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"train_data_reduced/x_validation[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"train_data_reduced/y_validation[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"train_data_reduced/y_validation[1]_0.npy\"))\n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_train_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "        \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        print('encode_input : ',encode_input)\n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 1001\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            errortypes, decoded = pfr.decode(\n",
    "                model,# 拆解成encoder ,decoder\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "        '''   \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        '''\n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_train_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path + \"checkpoint_train_type.txt\"\n",
    "\n",
    "        with open(TypeOutput, 'w', encoding=\"utf-8\") as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.fbeta_score(decode_output1[0][i*data_size+j], errortypes_gate[j], beta=2)\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_train_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path  + 'checkpoint_train_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w',encoding=\"utf-8\") as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_train_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], [str(ps_predicted)]), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gwfGyDUy-kC"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niuzp92iy-kD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"checkpoint_\" + modelset + \".h5\")\n",
    "        # Predict\n",
    "        #load traing/test data \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"x_validation[0]_0.npy\"))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"x_validation[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"y_validation[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"y_validation[1]_0.npy\"))\n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_validation_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        \n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 512\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            errortypes, decoded = pfr.decode(\n",
    "                model,\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "        '''   \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        '''\n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_validation_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path + 'checkpoint_'+ modelset + '_validation_type.txt'\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_validation_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path + 'checkpoint_'+ modelset + '_validation_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_validation_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okMR3IfZy-kF"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_v8Sa-wy-kF"
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        #model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"test_model_\" + modelset + \".h5\")\n",
    "        model, source_token_dict, target_token_dict, target_token_dict_inv = load(drive_path + \"checkpoint_\" + modelset + \".h5\")\n",
    "        # Predict\n",
    "        #load traing/test data \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        max_seq_len = int(source_max_len_loaded[0])       \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output1 = []\n",
    "        decode_output2 = []\n",
    "                    \n",
    "        encode_input.append(loadTestTrainData(drive_path + \"x_test[0]_0.npy\"))\n",
    "        decode_input.append(loadTestTrainData(drive_path + \"x_test[1]_0.npy\"))        \n",
    "        decode_output1.append(loadTestTrainData(drive_path + \"y_test[0]_0.npy\"))\n",
    "        decode_output2.append(loadTestTrainData(drive_path + \"y_test[1]_0.npy\"))  \n",
    "        \n",
    "        '''\n",
    "        print(\"encode_input_loaded[0].shape:\", encode_input_loaded[0].shape)\n",
    "        print(\"decode_input_loaded[0].shape:\", decode_input_loaded[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2_loaded[0].shape:\", decode_output2_loaded[0].shape)\n",
    "\n",
    "        print(\"encode_input_loaded len:\", len(encode_input_loaded))\n",
    "        print(\"decode_input_loaded len:\", len(decode_input_loaded))\n",
    "        print(\"decode_output1 len:\", len(decode_output1))\n",
    "        print(\"decode_output2_loaded len:\", len(decode_output2_loaded))\n",
    "        \n",
    "        print(\"encode_input_loaded[0] len:\", len(encode_input_loaded[0]))\n",
    "        print(\"decode_input_loaded[0] len:\", len(decode_input_loaded[0]))\n",
    "        print(\"decode_output1[0] len:\", len(decode_output1[0]))\n",
    "        print(\"decode_output2_loaded[0] len:\", len(decode_output2_loaded[0]))\n",
    "        \n",
    "        print(\"encode_input_loaded[0][0] len:\", len(encode_input_loaded[0][0]))\n",
    "        print(\"decode_input_loaded[0][0] len:\", len(decode_input_loaded[0][0]))\n",
    "        print(\"decode_output1[0][0] len:\", len(decode_output1[0][0]))\n",
    "        print(\"decode_output2_loaded[0][0] len:\", len(decode_output2_loaded[0][0]))    \n",
    "        \n",
    "        encode_input = []\n",
    "        decode_input = []\n",
    "        decode_output2 = []\n",
    "        \n",
    "        d_two = [] \n",
    "        for i in range(len(encode_input_loaded[0])):          \n",
    "            d_two.append(encode_input_loaded[0][i][0:max_seq_len])              \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        encode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #encode_input = np.array(encode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_input_loaded[0])):          \n",
    "            d_two.append(decode_input_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_input.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_input = np.array(decode_input)\n",
    "        \n",
    "        d_two = []\n",
    "        for i in range(len(decode_output2_loaded[0])):          \n",
    "            d_two.append(decode_output2_loaded[0][i][0:max_seq_len])  \n",
    "        #switch to np array\n",
    "        d_two = np.asarray(d_two)\n",
    "        #give np array\n",
    "        decode_output2.append(d_two)\n",
    "        #switch to np array\n",
    "        #decode_output2 = np.array(decode_output2, dtype=object)\n",
    "        \n",
    "        print(\"encode_input[0].shape:\", encode_input[0].shape)\n",
    "        print(\"decode_input[0].shape:\", decode_input[0].shape)\n",
    "        print(\"decode_output1[0].shape:\", decode_output1[0].shape)\n",
    "        print(\"decode_output2[0].shape:\", decode_output2[0].shape)\n",
    "        '''\n",
    "        #recover the target tokens for training data\n",
    "        decode_output2[0] = np.squeeze(decode_output2[0])\n",
    "        decode_output2_list = decode_output2[0].tolist()\n",
    "        target_test_tokens = [ [ target_token_dict_inv[token] for token in sample if target_token_dict_inv[token] not in ['<END>', '<PAD>']] for sample in decode_output2_list ] \n",
    "      \n",
    "        source_max_len = max_seq_len\n",
    "        \n",
    "        encode_input = encode_input[0] \n",
    "        \n",
    "        result_err = []\n",
    "        result_dec = []   \n",
    "        data_size = 512\n",
    "        \n",
    "        for i in range(math.ceil(encode_input.shape[0]/data_size)):\n",
    "            errortypes, decoded = pfr.decode(\n",
    "                model,\n",
    "                encode_input.tolist()[i*data_size:(i+1)*data_size], #[code1, code2]==> ['class', 'A',...], ['class' 'B' ...]\n",
    "                start_token=target_token_dict['<START>'],\n",
    "                end_token=target_token_dict['<END>'],\n",
    "                pad_token=target_token_dict['<PAD>'],\n",
    "                max_len=source_max_len\n",
    "            )\n",
    "            #[dec1, dec2] like[ ['class', 'A',...], ['class' 'B' ...] ]\n",
    "            result_err.append(errortypes)\n",
    "            result_dec.append(decoded)\n",
    "         \n",
    "        print(\"len(result_err):\", len(result_err))\n",
    "        print(\"len(result_dec):\", len(result_dec))\n",
    "        print(\"len(result_err[0]):\", len(result_err[0]))\n",
    "        print(\"len(result_dec[0]):\", len(result_dec[0]))    \n",
    "        \n",
    "        Gate = 0.5\n",
    "\n",
    "        #TypeOutput = drive_path + modelset + '_test_type.txt' #錯誤類型輸出\n",
    "        TypeOutput = drive_path + \"checkpoint_\"+ modelset + \"_test_type.txt\"\n",
    "\n",
    "        with open(TypeOutput, 'w') as f :\n",
    "            Sum_Recall = 0.0\n",
    "            Sum_Precision = 0.0\n",
    "            Sum_F_Score = 0.0\n",
    "                     \n",
    "            for i in range(len(result_err)):# error classify\n",
    "                errortypes_gate = np.array([list(map(lambda e: 1 if np.sign(e-Gate)>0 else 0, t)) for t in result_err[i]])\n",
    "                \n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_err_len = data_size\n",
    "                else:\n",
    "                    result_err_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                    \n",
    "                for j in range(result_err_len):                                \n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "   \n",
    "                    std = \"=====標準答案=====\" + str(decode_output1[0][i*data_size+j])\n",
    "                    print(std)\n",
    "                    f.write(std+\"\\n\")\n",
    "\n",
    "                    pred = \"=====預測答案=====\" + str(errortypes_gate[j])\n",
    "                    print(pred)\n",
    "                    f.write(pred+\"\\n\")\n",
    "\n",
    "                    precision = metrics.precision_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Precision += precision\n",
    "                    recall = metrics.recall_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_Recall += recall\n",
    "                    f_score = metrics.f1_score(decode_output1[0][i*data_size+j], errortypes_gate[j])\n",
    "                    Sum_F_Score += f_score\n",
    "\n",
    "                    precision_print = \"Precision : \" + str(precision)\n",
    "                    print(precision_print)\n",
    "                    f.write(precision_print+\"\\n\")\n",
    "\n",
    "                    recall_print = \"Recall : \" + str(recall)\n",
    "                    print(recall_print)\n",
    "                    f.write(recall_print+\"\\n\")\n",
    "\n",
    "                    f_score_print = \"F score : \" + str(f_score)\n",
    "                    print(f_score_print)\n",
    "                    f.write(f_score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "            Avg_Precision = round(Sum_Precision/encode_input.shape[0], 4) #平均預測\n",
    "            Avg_Recall = round(Sum_Recall/encode_input.shape[0], 4) #平均召回\n",
    "            Avg_F_Score = round(Sum_F_Score/encode_input.shape[0], 4) #平均 F 值\n",
    "\n",
    "            Avg_Precision_print = \"Avg_Precision : \" + str(Avg_Precision)\n",
    "            print(Avg_Precision_print)\n",
    "            f.write(Avg_Precision_print+\"\\n\")\n",
    "\n",
    "            Avg_Recall_print = \"Avg_Recall : \" + str(Avg_Recall)\n",
    "            print(Avg_Recall_print)\n",
    "            f.write(Avg_Recall_print+\"\\n\")\n",
    "\n",
    "            Avg_F_Score_print = \"Avg_F_Score : \" + str(Avg_F_Score)\n",
    "            print(Avg_F_Score_print)\n",
    "            f.write(Avg_F_Score_print+\"\\n\")\n",
    "\n",
    "            print(\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        #MessOutput = drive_path + modelset + '_test_meteor.txt' #錯誤訊息輸出\n",
    "        MessOutput = drive_path + 'checkpoint_'+ modelset + '_test_meteor.txt'\n",
    "\n",
    "        with open(MessOutput, 'w') as f :\n",
    "            Sum_Meteor = 0.0\n",
    "            for i in range(len(result_dec)): #for error messages OK!\n",
    "                if encode_input.shape[0] % data_size == 0: \n",
    "                    result_dec_len = data_size\n",
    "                else:\n",
    "                    result_dec_len = data_size if i < encode_input.shape[0] // data_size else encode_input.shape[0] % data_size\n",
    "                for j in range(result_dec_len):\n",
    "                    Num = \"===== \" + str(i*data_size+j) + \" =====\"\n",
    "                    print(Num)\n",
    "                    f.write(Num+\"\\n\")\n",
    "\n",
    "                    standard = ''.join(target_test_tokens[i*data_size+j])\n",
    "                    standard_print = \"=====標準答案=====\" + standard\n",
    "                    ps_standard = parseSentence(standard)\n",
    "                    print(standard_print)\n",
    "                    f.write(standard_print+\"\\n\")\n",
    "\n",
    "                    predicted = ''.join(map(lambda x: target_token_dict_inv[x], result_dec[i][j][1:-1]))\n",
    "                    predicted_print = \"=====預測答案=====\" + predicted\n",
    "                    ps_predicted = parseSentence(predicted)\n",
    "                    print(predicted_print)\n",
    "                    f.write(predicted_print+\"\\n\")\n",
    "\n",
    "                    Meteor_Score = round(meteor.meteor_score([str(ps_standard)], str(ps_predicted)), 4)\n",
    "                    Sum_Meteor += Meteor_Score\n",
    "                    Meteor_Score_print = \"Meteor_Score : \" + str(Meteor_Score)\n",
    "                    print(Meteor_Score_print)\n",
    "                    f.write(Meteor_Score_print+\"\\n\")\n",
    "\n",
    "                    print(\" \")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "            Avg_Meteor = round(Sum_Meteor/encode_input.shape[0],4)\n",
    "            Avg_Meteor_print = \"Avg_Meteor_Score : \" + str(Avg_Meteor)\n",
    "            print(Avg_Meteor_print)\n",
    "            f.write(Avg_Meteor_print+\"\\n\")\n",
    "        \n",
    "        \n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Performer_Meteor_206.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
