{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "achZsSjjc6hw"
   },
   "source": [
    "# 安裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62WsE2_OblO2",
    "outputId": "b6780e6d-dbd0-4536-c6ed-1608cd94d5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-pos-embd in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from keras-pos-embd) (1.21.5)\n",
      "Requirement already satisfied: keras-multi-head in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (0.29.0)\n",
      "Requirement already satisfied: keras-self-attention==0.51.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from keras-multi-head) (0.51.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from keras-self-attention==0.51.0->keras-multi-head) (1.21.5)\n",
      "Collecting keras-layer-normalization\n",
      "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from keras-layer-normalization) (1.21.5)\n",
      "Building wheels for collected packages: keras-layer-normalization\n",
      "  Building wheel for keras-layer-normalization (setup.py): started\n",
      "  Building wheel for keras-layer-normalization (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=ef53e5ca17a9c889d12830244d0d702fee9906452e69acd65602efe691ca3189\n",
      "  Stored in directory: c:\\users\\w.r_chen\\appdata\\local\\pip\\cache\\wheels\\c1\\df\\15\\a88cdf68ce687574649f65063a743123e1bee79932b6eea3b6\n",
      "Successfully built keras-layer-normalization\n",
      "Installing collected packages: keras-layer-normalization\n",
      "Successfully installed keras-layer-normalization-0.16.0\n",
      "Collecting keras-position-wise-feed-forward\n",
      "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from keras-position-wise-feed-forward) (1.21.5)\n",
      "Building wheels for collected packages: keras-position-wise-feed-forward\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py): started\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=77688e99badf0013fb7e7d836f32ee2722ef6ca270e57dcd3bc5528ab79f393f\n",
      "  Stored in directory: c:\\users\\w.r_chen\\appdata\\local\\pip\\cache\\wheels\\20\\36\\25\\efb605ab1742a179274a6f7cb113da1c6758f45e212b59bb4d\n",
      "Successfully built keras-position-wise-feed-forward\n",
      "Installing collected packages: keras-position-wise-feed-forward\n",
      "Successfully installed keras-position-wise-feed-forward-0.8.0\n",
      "Requirement already satisfied: keras-embed-sim in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from keras-embed-sim) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting alibi\n",
      "  Downloading alibi-0.7.0-py3-none-any.whl (445 kB)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (3.5.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (4.64.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.1.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (1.7.3)\n",
      "Collecting spacy[lookups]<4.0.0,>=2.0.0\n",
      "  Downloading spacy-3.3.1-cp39-cp39-win_amd64.whl (11.7 MB)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=0.22.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (1.0.2)\n",
      "Requirement already satisfied: scikit-image!=0.17.1,<0.20,>=0.14.2 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (0.19.2)\n",
      "Requirement already satisfied: attrs<22.0.0,>=19.2.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (21.4.0)\n",
      "Collecting transformers<5.0.0,>=4.7.0\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (4.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (2.27.1)\n",
      "Requirement already satisfied: Pillow<10.0,>=5.4.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (9.0.1)\n",
      "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from alibi) (1.4.2)\n",
      "Collecting dill<0.4.0,>=0.3.0\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=0.23.3->alibi) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.0.0->alibi) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi) (3.3)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2.7.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2.9.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (1.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2021.7.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from scikit-learn<2.0.0,>=0.22.0->alibi) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from scikit-learn<2.0.0,>=0.22.0->alibi) (1.1.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.11.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (61.2.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.8-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting spacy-lookups-data<1.1.0,>=1.0.3\n",
      "  Downloading spacy_lookups_data-1.0.3-py2.py3-none-any.whl (98.5 MB)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.28.1->alibi) (0.4.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.7.0->alibi) (2022.3.15)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.7.0->alibi) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.7.0->alibi) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy[lookups]<4.0.0,>=2.0.0->alibi) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (from jinja2->spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, tokenizers, spacy-lookups-data, spacy, huggingface-hub, transformers, dill, alibi\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed alibi-0.7.0 blis-0.7.8 catalogue-2.0.7 cymem-2.0.6 dill-0.3.5.1 huggingface-hub-0.8.1 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.2 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.1 spacy-legacy-3.0.9 spacy-loggers-1.0.2 spacy-lookups-data-1.0.3 srsly-2.4.3 thinc-8.0.17 tokenizers-0.12.1 transformers-4.20.1 typer-0.4.2 wasabi-0.9.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=252b88cf22f0ec73acf68d5335809ebd0fa492363254d4e0150dcf23c6536e12\n",
      "  Stored in directory: c:\\users\\w.r_chen\\appdata\\local\\pip\\cache\\wheels\\f6\\d8\\b0\\15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n",
      "Requirement already satisfied: nvidia-ml-py3 in c:\\users\\w.r_chen\\anaconda3\\lib\\site-packages (7.352.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-pos-embd\n",
    "!pip install keras-multi-head\n",
    "!pip install keras-layer-normalization\n",
    "!pip install keras-position-wise-feed-forward\n",
    "!pip install keras-embed-sim\n",
    "!pip install matplotlib\n",
    "!pip install alibi\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T3lB0BWKKEf"
   },
   "source": [
    "# #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKJ7DZipcBw4",
    "outputId": "c585973a-69b6-41a7-c295-41669d4631cd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#drive_path = 'C:/模型 ver. 2/encoder_num=4, decoder_num=4/embed_dim=' + Embed_Dim + '/head_num=' + Head_Num + '/learning_rate=' + Learning_Rate + '/lossWeights_1-' + LossWeights + '/'\n",
    "drive_path = 'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHUD7liWc434"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_KERAS']= \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "twUnzwRtdBXN"
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import sys\n",
    "sys.path.append(r'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Men-len-1000')\n",
    "sys.path.append(r'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/甲班模型')\n",
    "sys.path.append(r'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/performer')\n",
    "sys.path.append(r'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/performer/keras_position_wise_feed_forward')\n",
    "sys.path.append(r'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/tensorflow_fast_attention')\n",
    "sys.path.append(r'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/performer\\keras_performer')\n",
    "from alibi.explainers import IntegratedGradients\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import unittest\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import pynvml\n",
    "from keras_performer import performer_ver_3 as tfr\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ftrZJUOjlJQC"
   },
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "  with open(filename, 'wb') as f:\n",
    "    np.save(f, data)\n",
    "\n",
    "def saveDictionary(dt, file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"wb\")\n",
    "    pickle.dump(dt, a_file)\n",
    "    a_file.close()\n",
    "\n",
    "def loadDictionary(file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"rb\")\n",
    "    dt = pickle.load(a_file)\n",
    "    return dt\n",
    "\n",
    "        \n",
    "def loadMaxLen(filename):     \n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "    \n",
    "def plotTrainingLoss(history):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right') \n",
    "    plt.show()\n",
    "\n",
    "def plotAttributionAcc(attr_mse_train):\n",
    "    plt.plot(attr_mse_train)\n",
    "    plt.title('model Attribution accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('loop')\n",
    "    plt.legend(['accuracy'], loc='upper left') \n",
    "    plt.show()\n",
    "def plotTrainingAcc(history):\n",
    "    plt.plot(history['sparse_categorical_accuracy'])\n",
    "    plt.plot(history['val_sparse_categorical_accuracy'])\n",
    "    plt.title('model Decoder-Output accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy'], loc='upper left') \n",
    "    plt.show()\n",
    "    \n",
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f, allow_pickle=True)\n",
    "        return a    \n",
    "def plotTrainingEFFOLoss(history):\n",
    "    plt.plot(history.history['error_feed_forward_output1_loss'])\n",
    "    plt.plot(history.history['val_error_feed_forward_output1_loss'])\n",
    "    plt.title('model error_feed_forward_output1 loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right') \n",
    "    plt.show()\n",
    "    \n",
    "def plotTrainingDOLoss(history):\n",
    "    plt.plot(history.history['Decoder-Output_loss'])\n",
    "    plt.plot(history.history['val_Decoder-Output_loss'])\n",
    "    plt.title('model Decoder-Output loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right') \n",
    "    plt.show()\n",
    "    \n",
    "def plotTrainingEFFOBinAcc(history):\n",
    "    plt.plot(history.history['error_feed_forward_output1_binary_accuracy'])\n",
    "    plt.plot(history.history['val_error_feed_forward_output1_binary_accuracy'])\n",
    "    plt.title('model error_feed_forward_output1 binary_accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy'], loc='upper left') \n",
    "    plt.show()\n",
    "    \n",
    "def plotTrainingDOAcc(history):\n",
    "    plt.plot(history.history['Decoder-Output_sparse_categorical_accuracy'])\n",
    "    plt.plot(history.history['val_Decoder-Output_sparse_categorical_accuracy'])\n",
    "    plt.title('model Decoder-Output accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy'], loc='upper left') \n",
    "    plt.show()\n",
    "    \n",
    "def myAcc(y_true, y_pred):#not necessarily used\n",
    "    global p_msg\n",
    "    tf.config.run_functions_eagerly(True) \n",
    "    #tf.print(\"y_true:\", y_true, output_stream=sys.stderr)\n",
    "    #tf.print(\"y_pred:\", y_pred, output_stream=sys.stderr)\n",
    "    #t_msg = y_true.numpy().tolist()\n",
    "    p_msg = y_pred #.numpy().tolist()\n",
    "    \n",
    "    acc = K.cast(K.equal(K.max(y_true, axis=-1),\n",
    "        K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n",
    "        K.floatx())\n",
    "    #print(\"metric acc:\" , acc)\n",
    "    return acc\n",
    "\n",
    "def newmodel(model):\n",
    "    inp = model.layers[2].input\n",
    "    opt = model.output\n",
    "    new_model = keras.models.Model(inputs = [inp], outputs = [opt])\n",
    "    return new_model\n",
    "\n",
    "def grad(sample, model):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(sample)\n",
    "        loss = model(sample)\n",
    "        target = tf.reduce_max(loss, axis=1, keepdims = True)\n",
    "    gt = g.gradient(target, sample)  \n",
    "    return gt\n",
    "\n",
    "def grad2(sample, model):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(sample)\n",
    "        with tf.GradientTape() as gg:\n",
    "            gg.watch(sample)\n",
    "            loss = model(sample)\n",
    "        ggt = gg.gradient(loss, sample)\n",
    "    gt = g.gradient(ggt, sample)\n",
    "    return gt\n",
    "\n",
    "def attrx_grad(sample, ig, mk, new_model, n_steps):\n",
    "    zk_grad_x = ig / sample\n",
    "    #print('zk_grad_x shape:', zk_grad_x.shape)\n",
    "    \n",
    "    zk_grad2_zk = 0\n",
    "    for i in range(n_steps):\n",
    "        #zk = x * mk\n",
    "        zk = sample * mk[i]\n",
    "        ans = grad2(zk, new_model) * (mk[i] * mk[i])\n",
    "        zk_grad2_zk += ans\n",
    "    #print('zk_grad2_zk shape: ', zk_grad2_zk.shape)\n",
    "    attrx_grad = zk_grad_x + (sample/n_steps) + zk_grad2_zk\n",
    "    return attrx_grad\n",
    "def displayMemory():\n",
    "    import nvidia_smi\n",
    "\n",
    "    nvidia_smi.nvmlInit()\n",
    "\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "    # card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "    print(\"Total memory:\", info.total)\n",
    "    print(\"Free memory:\", info.free)\n",
    "    print(\"Used memory:\", info.used)\n",
    "\n",
    "    nvidia_smi.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4NG1PGqXQkm"
   },
   "source": [
    "# 讀取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l-X0MKCDRoGz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs:  {'name': 'self_attention_1', 'trainable': True, 'dtype': 'float32'}\n",
      "kwargs:  {'name': 'self_attention_3', 'trainable': True, 'dtype': 'float32'}\n",
      "kwargs:  {'name': 'self_attention_5', 'trainable': True, 'dtype': 'float32'}\n",
      "kwargs:  {'name': 'self_attention_7', 'trainable': True, 'dtype': 'float32'}\n",
      "kwargs:  {'name': 'self_attention_9', 'trainable': True, 'dtype': 'float32'}\n",
      "kwargs:  {'name': 'self_attention_11', 'trainable': True, 'dtype': 'float32'}\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmax_seq_len=999\\nmodel1 = tfr.get_model(\\n  max_input_len=(max_seq_len, max_seq_len), \\n  token_num=max(len(source_token_dict),len(target_token_dict)),\\n  embed_dim=32,\\n  encoder_num=6,\\n  #decoder_num=6,\\n  head_num=4,\\n  hidden_dim=128,\\n  use_same_embed=False\\n)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(model_name):\n",
    "\n",
    "  from keras_performer import performer_ver_3\n",
    "  from tensorflow import keras\n",
    "  from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "  from keras_pos_embd import TrigPosEmbedding\n",
    "  from tensorflow_fast_attention.fast_attention_2 import softmax_kernel_transformation,  Attention, SelfAttention\n",
    "  from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "  co = performer_ver_3.get_custom_objects()\n",
    "  co['softmax_kernel_transformation']= softmax_kernel_transformation\n",
    "  path = 'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/甲班模型/'\n",
    "  model = keras.models.load_model(path + model_name, custom_objects= co)\n",
    "  s = loadDictionary(path + 'source_token_dict.pickle')\n",
    "  t = loadDictionary(path + 'target_token_dict.pickle')\n",
    "  t_inv = loadDictionary(path + 'target_token_dict_inv.pickle')\n",
    "  return model, s, t, t_inv\n",
    "\n",
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "  with open(filename, 'rb') as f:\n",
    "    a = np.load(f, allow_pickle=True)\n",
    "    return a\n",
    "model, source_token_dict, target_token_dict, target_token_dict_inv = load(\"model.h5\")\n",
    "#model.summary()\n",
    "'''\n",
    "max_seq_len=999\n",
    "model1 = tfr.get_model(\n",
    "  max_input_len=(max_seq_len, max_seq_len), \n",
    "  token_num=max(len(source_token_dict),len(target_token_dict)),\n",
    "  embed_dim=32,\n",
    "  encoder_num=6,\n",
    "  #decoder_num=6,\n",
    "  head_num=4,\n",
    "  hidden_dim=128,\n",
    "  use_same_embed=False\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from alibi.api.defaults import DEFAULT_DATA_INTGRAD, DEFAULT_META_INTGRAD\n",
    "from alibi.utils.approximation_methods import approximation_parameters\n",
    "from alibi.api.interfaces import Explainer, Explanation\n",
    "from typing import Callable, Union, List, Tuple, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_valid_output_shape_type: List = [tuple, list]\n",
    "\n",
    "\n",
    "def _compute_convergence_delta(model: Union[tf.keras.models.Model],\n",
    "                               input_dtypes: List[tf.DType],\n",
    "                               attributions: List[np.ndarray],\n",
    "                               start_point: Union[List[np.ndarray], np.ndarray],\n",
    "                               end_point: Union[List[np.ndarray], np.ndarray],\n",
    "                               forward_kwargs: Optional[dict],\n",
    "                               target: Optional[List[int]],\n",
    "                               _is_list: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes convergence deltas for each data point. Convergence delta measures how close the sum of all attributions\n",
    "    is to the difference between the model output at the baseline and the model output at the data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    input_dtypes\n",
    "        List with data types of the inputs.\n",
    "    attributions\n",
    "        Attributions assigned by the integrated gradients method to each feature.\n",
    "    start_point\n",
    "        Baselines.\n",
    "    end_point\n",
    "        Data points.\n",
    "    forward_kwargs\n",
    "        Input keywords args.\n",
    "    target\n",
    "        Target for which the gradients are calculated for classification models.\n",
    "    _is_list\n",
    "        Whether the model's input is a list (multiple inputs) or a np array (single input).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Convergence deltas for each data point.\n",
    "    \"\"\"\n",
    "    if _is_list:\n",
    "        start_point = [tf.convert_to_tensor(start_point[k], dtype=input_dtypes[k]) for k in range(len(input_dtypes))]\n",
    "        end_point = [tf.convert_to_tensor(end_point[k], dtype=input_dtypes[k]) for k in range(len(input_dtypes))]\n",
    "\n",
    "    else:\n",
    "        start_point = tf.convert_to_tensor(start_point)\n",
    "        end_point = tf.convert_to_tensor(end_point)\n",
    "\n",
    "    def _sum_rows(inp):\n",
    "\n",
    "        input_str = string.ascii_lowercase[1: len(inp.shape)]\n",
    "        if isinstance(inp, tf.Tensor):\n",
    "            sums = tf.einsum('a{}->a'.format(input_str), inp).numpy()\n",
    "        elif isinstance(inp, np.ndarray):\n",
    "            sums = np.einsum('a{}->a'.format(input_str), inp)\n",
    "        else:\n",
    "            raise NotImplementedError('input must be a tensorflow tensor or a numpy array')\n",
    "        return sums\n",
    "\n",
    "    start_out = _run_forward(model, start_point, target, forward_kwargs=forward_kwargs)\n",
    "    end_out = _run_forward(model, end_point, target, forward_kwargs=forward_kwargs)\n",
    "\n",
    "    if (len(model.output_shape) == 1 or model.output_shape[-1] == 1) and target is not None:\n",
    "        target_tensor = tf.cast(target, dtype=start_out.dtype)\n",
    "        target_tensor = tf.reshape(1 - target_tensor, [len(target), 1])\n",
    "        sign = 2 * target_tensor - 1\n",
    "\n",
    "        start_out = target_tensor + sign * start_out\n",
    "        end_out = target_tensor + sign * end_out\n",
    "\n",
    "    start_out_sum = _sum_rows(start_out)\n",
    "    end_out_sum = _sum_rows(end_out)\n",
    "\n",
    "    attr_sum = np.zeros(start_out_sum.shape)\n",
    "    for j in range(len(attributions)):\n",
    "        attrs_sum_j = _sum_rows(attributions[j])\n",
    "        attr_sum += attrs_sum_j\n",
    "\n",
    "    _deltas = attr_sum - (end_out_sum - start_out_sum)\n",
    "\n",
    "    return _deltas\n",
    "\n",
    "\n",
    "def _select_target(preds: tf.Tensor,\n",
    "                   targets: Union[None, tf.Tensor, np.ndarray, list]) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Select the predictions corresponding to the targets if targets is not None.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds\n",
    "        Predictions before selection.\n",
    "    targets\n",
    "        Targets to select.\n",
    "    Returns\n",
    "    -------\n",
    "        Selected predictions\n",
    "\n",
    "    \"\"\"\n",
    "    if targets is not None:\n",
    "        if isinstance(preds, tf.Tensor):\n",
    "            preds = tf.linalg.diag_part(tf.gather(preds, targets, axis=1))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise ValueError(\"target cannot be `None` if `model` output dimensions > 1\")\n",
    "    return preds\n",
    "\n",
    "\n",
    "def _run_forward(model: Union[tf.keras.models.Model],\n",
    "                 x: Union[List[tf.Tensor], List[np.ndarray]],\n",
    "                 target: Union[None, tf.Tensor, np.ndarray, list],\n",
    "                 forward_kwargs: Optional[dict] = None) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Returns the output of the model. If the target is not `None`, only the output for the selected target is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated for classification models.\n",
    "    forward_kwargs\n",
    "        Input keyword args.\n",
    "    Returns\n",
    "    -------\n",
    "        Model output or model output after target selection for classification models.\n",
    "\n",
    "    \"\"\"\n",
    "    if forward_kwargs is None:\n",
    "        preds = model(x)\n",
    "    else:\n",
    "        preds = model(x, **forward_kwargs)\n",
    "\n",
    "    if len(model.output_shape) > 1 and model.output_shape[-1] > 1:\n",
    "        preds = _select_target(preds, target)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def _run_forward_from_layer(model: tf.keras.models.Model,\n",
    "                            layer: tf.keras.layers.Layer,\n",
    "                            orig_call: Callable,\n",
    "                            orig_dummy_input: Union[list, np.ndarray],\n",
    "                            x: tf.Tensor,\n",
    "                            target: Union[None, tf.Tensor, np.ndarray, list],\n",
    "                            forward_kwargs: Optional[dict] = None,\n",
    "                            run_from_layer_inputs: bool = False,\n",
    "                            select_target: bool = True) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Function currently unused.\n",
    "    Executes a forward call from an internal layer of the model to the model output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    layer\n",
    "        Starting layer for the forward call.\n",
    "    orig_call\n",
    "        Original `call` method of the layer.\n",
    "    orig_dummy_input\n",
    "        Dummy input needed to initiate the model forward call. The number of instances in the dummy input must\n",
    "        be the same as the number of instances in x. The dummy input values play no role in the evaluation\n",
    "        as the  layer's status is overwritten during the forward call.\n",
    "    x\n",
    "        Layer's inputs. The layer's status is overwritten with `x` during the forward call.\n",
    "    target\n",
    "        Target for the output position to be returned.\n",
    "    forward_kwargs\n",
    "        Input keyword args. It must be a dict with numpy arrays as values. If it's not None,\n",
    "        the first dimension of the arrays must correspond to the number of instances in x and orig_dummy_input.\n",
    "    run_from_layer_inputs\n",
    "        If True, the forward pass starts from the layer's inputs, if False it starts from the layer's outputs.\n",
    "    select_target\n",
    "        Whether to return predictions for selected targets or return predictions for all targets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Model's predictions for the given target.\n",
    "\n",
    "    \"\"\"\n",
    "    def feed_layer(layer):\n",
    "        \"\"\"\n",
    "        Overwrites the intermediate layer status with the precomputed values `x`.\n",
    "\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Store the result and inputs of `layer.call` internally.\n",
    "                if run_from_layer_inputs:\n",
    "                    layer.inp = x\n",
    "                    layer.result = func(*x, **kwargs)\n",
    "                else:\n",
    "                    layer.inp = args\n",
    "                    layer.result = x\n",
    "                # Return the result to continue with the forward pass.\n",
    "                return layer.result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        layer.call = decorator(layer.call)\n",
    "\n",
    "    feed_layer(layer)\n",
    "    if forward_kwargs is None:\n",
    "        preds = model(orig_dummy_input)\n",
    "    else:\n",
    "        preds = model(orig_dummy_input, **forward_kwargs)\n",
    "\n",
    "    delattr(layer, 'inp')\n",
    "    delattr(layer, 'result')\n",
    "    layer.call = orig_call\n",
    "\n",
    "    if select_target and len(model.output_shape) > 1 and model.output_shape[-1] > 1:\n",
    "        preds = _select_target(preds, target)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def _run_forward_to_layer(model: tf.keras.models.Model,\n",
    "                          layer: tf.keras.layers.Layer,\n",
    "                          orig_call: Callable,\n",
    "                          x: Union[List[np.ndarray], np.ndarray],\n",
    "                          forward_kwargs: Optional[dict] = None,\n",
    "                          run_to_layer_inputs: bool = False) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Executes a forward call from the model input to an internal layer output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    layer\n",
    "        Starting layer for the forward call.\n",
    "    orig_call\n",
    "        Original `call` method of the layer.\n",
    "    x\n",
    "        Model's inputs.\n",
    "    forward_kwargs\n",
    "        Input keyword args.\n",
    "    run_to_layer_inputs\n",
    "        If True, the layer's inputs are returned. If False, the layer's output's are returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Output of the given layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def take_layer(layer):\n",
    "        \"\"\"\n",
    "        Stores the layer's outputs internally to the layer's object.\n",
    "\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Store the result of `layer.call` internally.\n",
    "                layer.inp = args\n",
    "                layer.result = func(*args, **kwargs)\n",
    "                # Return the result to continue with the forward pass.\n",
    "                return layer.result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        layer.call = decorator(layer.call)\n",
    "\n",
    "    # inp = tf.zeros((x.shape[0], ) + model.input_shape[1:])\n",
    "    take_layer(layer)\n",
    "    if forward_kwargs is None:\n",
    "        _ = model(x)\n",
    "    else:\n",
    "        _ = model(x, **forward_kwargs)\n",
    "    layer_inp = layer.inp\n",
    "    layer_out = layer.result\n",
    "\n",
    "    delattr(layer, 'inp')\n",
    "    delattr(layer, 'result')\n",
    "    layer.call = orig_call\n",
    "\n",
    "    if run_to_layer_inputs:\n",
    "        return layer_inp\n",
    "    else:\n",
    "        return layer_out\n",
    "\n",
    "from tensorflow.python.training.tracking.data_structures import ListWrapper\n",
    "def _forward_input_baseline(X: Union[List[np.ndarray], np.ndarray],\n",
    "                            bls: Union[List[np.ndarray], np.ndarray],\n",
    "                            model: tf.keras.Model,\n",
    "                            layer: tf.keras.layers.Layer,\n",
    "                            orig_call: Callable,\n",
    "                            forward_kwargs: Optional[dict] = None,\n",
    "                            forward_to_inputs: bool = False) -> Tuple[Union[list, tf.Tensor], Union[list, tf.Tensor]]:\n",
    "    \"\"\"\n",
    "    Forwards inputs and baselines to the layer's inputs or outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Input data points.\n",
    "    bls\n",
    "        Baselines.\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    layer\n",
    "        Desired layer output.\n",
    "    orig_call\n",
    "        Original `call` method of layer.\n",
    "    forward_kwargs\n",
    "        Input keyword args.\n",
    "    forward_to_inputs\n",
    "        If True, X and bls are forwarded to the layer's input. If False, they are forwarded to the layer's outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Forwarded inputs and  baselines as a numpy arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    #print(\"layer: \", layer)\n",
    "    if layer is not None:\n",
    "        X_layer = _run_forward_to_layer(model,\n",
    "                                        layer,\n",
    "                                        orig_call,\n",
    "                                        X,\n",
    "                                        forward_kwargs=forward_kwargs,\n",
    "                                        run_to_layer_inputs=forward_to_inputs)\n",
    "        bls_layer = _run_forward_to_layer(model,\n",
    "                                          layer,\n",
    "                                          orig_call,\n",
    "                                          bls,\n",
    "                                          forward_kwargs=forward_kwargs,\n",
    "                                          run_to_layer_inputs=forward_to_inputs)\n",
    "\n",
    "        #print(\"bls_layer type: \", type(bls_layer))\n",
    "        #print(\"X_layer type: \", type(X_layer))\n",
    "        if isinstance(X_layer, tuple):\n",
    "            X_layer = list(X_layer)\n",
    "\n",
    "        if isinstance(bls_layer, tuple):\n",
    "            bls_layer = list(bls_layer)\n",
    "\n",
    "        return X_layer, bls_layer\n",
    "\n",
    "    else:\n",
    "        return X, bls\n",
    "\n",
    "\n",
    "def _gradients_input(model: Union[tf.keras.models.Model],\n",
    "                     x: List[tf.Tensor],\n",
    "                     target: Union[None, tf.Tensor],\n",
    "                     forward_kwargs: Optional[dict] = None) -> List[tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the target class output (or the output if the output dimension is equal to 1)\n",
    "    with respect to each input feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated if the output dimension is higher than 1.\n",
    "    forward_kwargs\n",
    "        Input keyword args.\n",
    "    Returns\n",
    "    -------\n",
    "        Gradients for each input feature.\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        preds = _run_forward(model, x, target, forward_kwargs=forward_kwargs)\n",
    "\n",
    "    grads = tape.gradient(preds, x)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def _gradients_layer(model: Union[tf.keras.models.Model],\n",
    "                     layer: Union[tf.keras.layers.Layer],\n",
    "                     orig_call: Callable,\n",
    "                     orig_dummy_input: Union[list, np.ndarray],\n",
    "                     x: Union[List[tf.Tensor], tf.Tensor],\n",
    "                     target: Union[None, tf.Tensor],\n",
    "                     forward_kwargs: Optional[dict] = None,\n",
    "                     compute_layer_inputs_gradients: bool = False) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the target class output (or the output if the output dimension is equal to 1)\n",
    "    with respect to each element of `layer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    layer\n",
    "        Layer of the model with respect to which the gradients are calculated.\n",
    "    orig_call\n",
    "        Original `call` method of the layer. This is necessary since the call method is modified by the function\n",
    "        in order to make the layer output visible to the GradientTape.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated if the output dimension is higher than 1.\n",
    "    forward_kwargs\n",
    "        Input keyword args.\n",
    "    compute_layer_inputs_gradients\n",
    "        If True, gradients are computed with respect to the layer's inputs.\n",
    "        If False, they are computed with respect to the layer's outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Gradients for each element of layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def watch_layer(layer, tape):\n",
    "        \"\"\"\n",
    "        Make an intermediate hidden `layer` watchable by the `tape`.\n",
    "        After calling this function, you can obtain the gradient with\n",
    "        respect to the output of the `layer` by calling:\n",
    "\n",
    "            grads = tape.gradient(..., layer.result)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Store the result and the input of `layer.call` internally.\n",
    "                if compute_layer_inputs_gradients:\n",
    "                    layer.inp = x\n",
    "                    layer.result = func(*x, **kwargs)\n",
    "                    # From this point onwards, watch this tensor.\n",
    "                    tape.watch(layer.inp)\n",
    "                else:\n",
    "                    layer.inp = args\n",
    "                    layer.result = x\n",
    "                    # From this point onwards, watch this tensor.\n",
    "                    tape.watch(layer.result)\n",
    "                # Return the result to continue with the forward pass.\n",
    "                return layer.result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        layer.call = decorator(layer.call)\n",
    "\n",
    "    #  Repeating the dummy input needed to initiate the model's forward call in order to ensure that\n",
    "    #  the number of dummy instances is the same as the number of real instances.\n",
    "    #  This is necessary in case `forward_kwargs` is not None. In that case, the model forward call  would crash\n",
    "    #  if the number of instances in `orig_dummy_input` is different from the number of instances in `forward_kwargs`.\n",
    "    #  The number of instances in `forward_kwargs` is the same as the number of instances in `x` by construction.\n",
    "    if isinstance(orig_dummy_input, list):\n",
    "        if isinstance(x, list):\n",
    "            orig_dummy_input = [np.repeat(inp, x[0].shape[0], axis=0) for inp in orig_dummy_input]\n",
    "        else:\n",
    "            orig_dummy_input = [np.repeat(inp, x.shape[0], axis=0) for inp in orig_dummy_input]\n",
    "    else:\n",
    "        if isinstance(x, list):\n",
    "            orig_dummy_input = np.repeat(orig_dummy_input, x[0].shape[0], axis=0)\n",
    "        else:\n",
    "            orig_dummy_input = np.repeat(orig_dummy_input, x.shape[0], axis=0)\n",
    "\n",
    "    #  Calculating the gradients with respect to the layer.\n",
    "    with tf.GradientTape() as tape:\n",
    "        watch_layer(layer, tape)\n",
    "        preds = _run_forward(model, orig_dummy_input, target, forward_kwargs=forward_kwargs)\n",
    "\n",
    "    if compute_layer_inputs_gradients:\n",
    "        grads = tape.gradient(preds, layer.inp)\n",
    "    else:\n",
    "        grads = tape.gradient(preds, layer.result)\n",
    "\n",
    "    delattr(layer, 'inp')\n",
    "    delattr(layer, 'result')\n",
    "    layer.call = orig_call\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def _format_baseline(X: np.ndarray,\n",
    "                     baselines: Union[None, int, float, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Formats baselines to return a numpy array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Input data points.\n",
    "    baselines\n",
    "        Baselines.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Formatted inputs and  baselines as a numpy arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    #print(\"X.shape: \", X.shape)\n",
    "    if baselines is None:\n",
    "        bls = np.zeros(X.shape).astype(X.dtype)\n",
    "    elif isinstance(baselines, int) or isinstance(baselines, float):\n",
    "        bls = np.full(X.shape, baselines).astype(X.dtype)\n",
    "    elif isinstance(baselines, np.ndarray):\n",
    "        bls = baselines.astype(X.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"baselines must be `int`, `float`, `np.ndarray` or `None`. Found {type(baselines)}\")\n",
    "    #print(\"bls.shape: \", bls.shape)\n",
    "    return bls\n",
    "\n",
    "\n",
    "def _format_target(target: Union[None, int, list, np.ndarray],\n",
    "                   nb_samples: int) -> Union[None, List[int]]:\n",
    "    \"\"\"\n",
    "    Formats target to return a list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target\n",
    "        Original target.\n",
    "    nb_samples\n",
    "        Number of samples in the batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Formatted target as a list.\n",
    "\n",
    "    \"\"\"\n",
    "    if target is not None:\n",
    "        if isinstance(target, int):\n",
    "            target = [target for _ in range(nb_samples)]\n",
    "        elif isinstance(target, list) or isinstance(target, np.ndarray):\n",
    "            target = [t.astype(int) for t in target]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def _sum_integral_terms(step_sizes: list,\n",
    "                        grads: Union[tf.Tensor, np.ndarray]) -> Union[tf.Tensor, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Sums the terms in the path integral with weights `step_sizes`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    step_sizes\n",
    "        Weights in the path integral sum.\n",
    "    grads\n",
    "        Gradients to sum for each feature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Sums of the gradients along the chosen path.\n",
    "\n",
    "    \"\"\"\n",
    "    input_str = string.ascii_lowercase[1: len(grads.shape)]\n",
    "    if isinstance(grads, tf.Tensor):\n",
    "        step_sizes = tf.convert_to_tensor(step_sizes)\n",
    "        einstr = 'a,a{}->{}'.format(input_str, input_str)\n",
    "        sums = tf.einsum(einstr, step_sizes, grads).numpy()\n",
    "    elif isinstance(grads, np.ndarray):\n",
    "        einstr = 'a,a{}->{}'.format(input_str, input_str)\n",
    "        sums = np.einsum(einstr, step_sizes, grads)\n",
    "    else:\n",
    "        raise NotImplementedError('input must be a tensorflow tensor or a numpy array')\n",
    "    return sums\n",
    "\n",
    "\n",
    "def _calculate_sum_int(batches: List[List[tf.Tensor]],\n",
    "                       model: Union[tf.keras.Model],\n",
    "                       target: Union[None, List[int]],\n",
    "                       target_paths: np.ndarray,\n",
    "                       n_steps: int,\n",
    "                       nb_samples: int,\n",
    "                       step_sizes: List[float],\n",
    "                       j: int) -> Union[tf.Tensor, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the sum of all the terms in the integral from a list of batch gradients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batches\n",
    "        List of batch gradients.\n",
    "    model\n",
    "        tf.keras or keras model.\n",
    "    target\n",
    "        List of targets.\n",
    "    target_paths\n",
    "        Targets for each path in the integral.\n",
    "    n_steps\n",
    "        Number of steps in the integral.\n",
    "    nb_samples\n",
    "        Total number of samples.\n",
    "    step_sizes\n",
    "        Step sizes used to calculate the integral.\n",
    "    j\n",
    "        Iterates through list of inputs or list of layers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    grads = tf.concat(batches[j], 0)\n",
    "    shape = grads.shape[1:]\n",
    "    if isinstance(shape, tf.TensorShape):\n",
    "        shape = tuple(shape.as_list())\n",
    "\n",
    "    # invert sign of gradients for target 0 examples if classifier returns only positive class probability\n",
    "    if (len(model.output_shape) == 1 or model.output_shape[-1] == 1) and target is not None:\n",
    "        sign = 2 * target_paths - 1\n",
    "        grads = np.asarray([s * g for s, g in zip(sign, grads)])\n",
    "\n",
    "    grads = tf.reshape(grads, (n_steps, nb_samples) + shape)\n",
    "    # sum integral terms and scale attributions\n",
    "    sum_int = _sum_integral_terms(step_sizes, grads.numpy())\n",
    "\n",
    "    return sum_int\n",
    "\n",
    "\n",
    "def _validate_output(model: tf.keras.Model,\n",
    "                     target: Optional[List[int]]) -> None:\n",
    "    \"\"\"\n",
    "    Validates the model's output type and raises an error if the output type is not supported.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Keras model for which the output is validated.\n",
    "    target\n",
    "        Targets for which gradients are calculated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    if not model.output_shape or not any(isinstance(model.output_shape, t) for t in _valid_output_shape_type):\n",
    "        raise NotImplementedError(f\"The model output_shape attribute must be in {_valid_output_shape_type}. \"\n",
    "                                  f\"Found model.output_shape: {model.output_shape}\")\n",
    "\n",
    "    if (len(model.output_shape) == 1\n",
    "        or model.output_shape[-1] == 1) \\\n",
    "            and target is None:\n",
    "        logger.warning(\"It looks like you are passing a model with a scalar output and target is set to `None`.\"\n",
    "                       \"If your model is a regression model this will produce correct attributions. If your model \"\n",
    "                       \"is a classification model, targets for each datapoint must be defined. \"\n",
    "                       \"Not defining the target may lead to incorrect values for the attributions.\"\n",
    "                       \"Targets can be either the true classes or the classes predicted by the model.\")\n",
    "\n",
    "\n",
    "class IntegratedGradients1(Explainer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: tf.keras.Model,\n",
    "                 layer: Optional[tf.keras.layers.Layer] = None,\n",
    "                 method: str = \"gausslegendre\",\n",
    "                 n_steps: int = 10,\n",
    "                 internal_batch_size: int = 100\n",
    "                 ) -> None:\n",
    "        \"\"\"\n",
    "        An implementation of the integrated gradients method for Tensorflow and Keras models.\n",
    "\n",
    "        For details of the method see the original paper:\n",
    "        https://arxiv.org/abs/1703.01365 .\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model\n",
    "            Tensorflow or Keras model.\n",
    "        layer\n",
    "            Layer with respect to which the gradients are calculated.\n",
    "            If not provided, the gradients are calculated with respect to the input.\n",
    "        method\n",
    "            Method for the integral approximation. Methods available:\n",
    "            \"riemann_left\", \"riemann_right\", \"riemann_middle\", \"riemann_trapezoid\", \"gausslegendre\".\n",
    "        n_steps\n",
    "            Number of step in the path integral approximation from the baseline to the input instance.\n",
    "        internal_batch_size\n",
    "            Batch size for the internal batching.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(meta=copy.deepcopy(DEFAULT_META_INTGRAD))\n",
    "        params = locals()\n",
    "        remove = ['self', 'model', '__class__', 'layer']\n",
    "        params = {k: v for k, v in params.items() if k not in remove}\n",
    "        self.model = model\n",
    "\n",
    "        if self.model.inputs is None:\n",
    "            self._has_inputs = False\n",
    "        else:\n",
    "            self._has_inputs = True\n",
    "\n",
    "        if layer is None:\n",
    "            self.orig_call = None\n",
    "            layer_num = 0\n",
    "        else:\n",
    "            self.orig_call = layer.call\n",
    "            try:\n",
    "                layer_num = model.layers.index(layer)\n",
    "            except ValueError:\n",
    "                logger.info(\"Layer not in the list of model.layers\")\n",
    "                layer_num = None\n",
    "\n",
    "        params['layer'] = layer_num\n",
    "        self.meta['params'].update(params)\n",
    "        self.layer = layer\n",
    "        self.n_steps = n_steps\n",
    "        self.method = method\n",
    "        self.internal_batch_size = internal_batch_size\n",
    "\n",
    "        self._is_list: Optional[bool] = None\n",
    "        self._is_np: Optional[bool] = None\n",
    "        self.orig_dummy_input: Optional[Union[list, np.ndarray]] = None\n",
    "\n",
    "\n",
    "    def explain(self,\n",
    "                X: Union[np.ndarray, List[np.ndarray]],\n",
    "                forward_kwargs: Optional[dict] = None,\n",
    "                baselines: Union[int, float, np.ndarray, List[int], List[float], List[np.ndarray]] = None,\n",
    "                target: Union[int, list, np.ndarray] = None,\n",
    "                attribute_to_layer_inputs: bool = False) -> Explanation:\n",
    "        \"\"\"Calculates the attributions for each input feature or element of layer and\n",
    "        returns an Explanation object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Instance for which integrated gradients attribution are computed.\n",
    "        forward_kwargs\n",
    "            Input keyword args. If it's not None, it must be a dict with numpy arrays as values.\n",
    "            The first dimension of the arrays must correspond to the number of examples.\n",
    "            It will be repeated for each of n_steps along the integrated path.\n",
    "            The attributions are not computed with respect to these arguments.\n",
    "        baselines\n",
    "            Baselines (starting point of the path integral) for each instance.\n",
    "            If the passed value is an `np.ndarray` must have the same shape as X.\n",
    "            If not provided, all features values for the baselines are set to 0.\n",
    "        target\n",
    "            Defines which element of the model output is considered to compute the gradients.\n",
    "            It can be a list of integers or a numeric value. If a numeric value is passed, the gradients are calculated\n",
    "            for the same element of the output for all data points.\n",
    "            It must be provided if the model output dimension is higher than 1.\n",
    "            For regression models whose output is a scalar, target should not be provided.\n",
    "            For classification models `target` can be either the true classes or the classes predicted by the model.\n",
    "        attribute_to_layer_inputs\n",
    "            In case of layers gradients, controls whether the gradients are computed for the layer's inputs or\n",
    "            outputs. If True, gradients are computed for the layer's inputs, if False for the layer's outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            `Explanation` object including `meta` and `data` attributes with integrated gradients attributions\n",
    "            for each feature.\n",
    "\n",
    "        \"\"\"\n",
    "        self._is_list = isinstance(X, list)\n",
    "        self._is_np = isinstance(X, np.ndarray)\n",
    "        #print('self._is_list: ', self._is_list)\n",
    "        #print('self._is_np: ', self._is_np)\n",
    "        if self._is_list:\n",
    "            self.orig_dummy_input = [np.zeros((1,) + xx.shape[1:], dtype=xx.dtype) for xx in X]  # type: ignore\n",
    "            nb_samples = len(X[0])\n",
    "            input_dtypes = [xx.dtype for xx in X]\n",
    "            # Formatting baselines in case of models with multiple inputs\n",
    "            if baselines is None:\n",
    "                baselines = [None for _ in range(len(X))]\n",
    "            else:\n",
    "                if not isinstance(baselines, list):\n",
    "                    raise ValueError(f\"If the input X is a list, baseline can only be `None` or \"\n",
    "                                     f\"a list of the same length of X. Found baselines type {type(baselines)}\")\n",
    "                else:\n",
    "                    if len(X) != len(baselines):\n",
    "                        raise ValueError(f\"Length of 'X' must match length of 'baselines'. \"\n",
    "                                         f\"Found len(X): {len(X)}, len(baselines): {len(baselines)}\")\n",
    "\n",
    "            if max([len(x) for x in X]) != min([len(x) for x in X]):\n",
    "                raise ValueError(\"First dimension must be egual for all inputs\")\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                x, baseline = X[i], baselines[i]  # type: ignore\n",
    "                # format and check baselines\n",
    "                baseline = _format_baseline(x, baseline)\n",
    "                baselines[i] = baseline  # type: ignore\n",
    "\n",
    "        elif self._is_np:\n",
    "            #X = cast(np.ndarray, X)  # help mypy out\n",
    "            #print('in is np : X shape:', X.shape)\n",
    "            self.orig_dummy_input = np.zeros((1,) + X.shape[1:], dtype=X.dtype)  # type: ignore\n",
    "            nb_samples = len(X)\n",
    "            input_dtypes = [X.dtype]  # type: ignore\n",
    "            # Formatting baselines for models with a single input\n",
    "            baselines = _format_baseline(X, baselines)\n",
    "            #print(\"baselines.shape1: \", baselines.shape)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a np.ndarray or a list of np.ndarray\")\n",
    "\n",
    "        # defining integral method\n",
    "        step_sizes_func, alphas_func = approximation_parameters(self.method)\n",
    "        step_sizes, alphas = step_sizes_func(self.n_steps), alphas_func(self.n_steps)\n",
    "        #print('alphas: ', alphas)\n",
    "        target = _format_target(target, nb_samples)\n",
    "        #print('target:', target)\n",
    "        #print('target type:', type(target))\n",
    "        if self._is_list:\n",
    "            # Attributions calculation in case of multiple inputs\n",
    "            if not self._has_inputs:\n",
    "                # Inferring model's inputs from data points for models with no explicit inputs\n",
    "                # (typically subclassed models)\n",
    "                inputs = [tf.keras.Input(shape=xx.shape[1:], dtype=xx.dtype) for xx in X]\n",
    "                self.model(inputs)\n",
    "\n",
    "            _validate_output(self.model, target)\n",
    "\n",
    "            if self.layer is None:\n",
    "                # No layer passed, attributions computed with respect to the inputs\n",
    "                attributions = self._compute_attributions_list_input(X,\n",
    "                                                                     baselines,\n",
    "                                                                     target,\n",
    "                                                                     step_sizes,\n",
    "                                                                     alphas,\n",
    "                                                                     nb_samples,\n",
    "                                                                     forward_kwargs,\n",
    "                                                                     attribute_to_layer_inputs)\n",
    "\n",
    "            else:\n",
    "                # forwad inputs and  baselines\n",
    "                X_layer, baselines_layer = _forward_input_baseline(X,\n",
    "                                                                   baselines,\n",
    "                                                                   self.model,\n",
    "                                                                   self.layer,\n",
    "                                                                   self.orig_call,\n",
    "                                                                   forward_kwargs=forward_kwargs,\n",
    "                                                                   forward_to_inputs=attribute_to_layer_inputs)\n",
    "\n",
    "                if isinstance(X_layer, list) and isinstance(baselines_layer, list):\n",
    "                    attributions = self._compute_attributions_list_input(X_layer,\n",
    "                                                                         baselines_layer,\n",
    "                                                                         target,\n",
    "                                                                         step_sizes,\n",
    "                                                                         alphas,\n",
    "                                                                         nb_samples,\n",
    "                                                                         forward_kwargs,\n",
    "                                                                         attribute_to_layer_inputs)\n",
    "                else:\n",
    "                    attributions = self._compute_attributions_tensor_input(X_layer,\n",
    "                                                                           baselines_layer,\n",
    "                                                                           target,\n",
    "                                                                           step_sizes,\n",
    "                                                                           alphas,\n",
    "                                                                           nb_samples,\n",
    "                                                                           forward_kwargs,\n",
    "                                                                           attribute_to_layer_inputs)\n",
    "\n",
    "        else:\n",
    "            # Attributions calculation in case of single input\n",
    "            if not self._has_inputs:\n",
    "                inputs = tf.keras.Input(shape=X.shape[1:], dtype=X.dtype)  # type: ignore\n",
    "                self.model(inputs)\n",
    "\n",
    "            _validate_output(self.model, target)\n",
    "\n",
    "            if self.layer is None:\n",
    "                attributions = self._compute_attributions_tensor_input(X,\n",
    "                                                                       baselines,\n",
    "                                                                       target,\n",
    "                                                                       step_sizes,\n",
    "                                                                       alphas,\n",
    "                                                                       nb_samples,\n",
    "                                                                       forward_kwargs,\n",
    "                                                                       attribute_to_layer_inputs)\n",
    "\n",
    "            else:\n",
    "                #print(\"--------else self.layer is None:\")\n",
    "                # forwad inputs and  baselines\n",
    "                X_layer, baselines_layer = _forward_input_baseline(X,\n",
    "                                            baselines,\n",
    "                                            self.model,\n",
    "                                            self.layer,\n",
    "                                            self.orig_call,\n",
    "                                            forward_kwargs=forward_kwargs,\n",
    "                                            forward_to_inputs=attribute_to_layer_inputs)\n",
    "\n",
    "                #print(\"-----baselines_layer[0].shape: \", baselines_layer[0].shape)\n",
    "                if isinstance(X_layer, list) and isinstance(baselines_layer, list):\n",
    "                    attributions = self._compute_attributions_list_input(X_layer,\n",
    "                                                baselines_layer,\n",
    "                                                target,\n",
    "                                                step_sizes,\n",
    "                                                alphas,\n",
    "                                                nb_samples,\n",
    "                                                forward_kwargs,\n",
    "                                                attribute_to_layer_inputs)\n",
    "                    #print(\"------baselines.shape\", baselines.shape)\n",
    "                else:\n",
    "                    attributions = self._compute_attributions_tensor_input(X_layer,\n",
    "                                                                           baselines_layer,\n",
    "                                                                           target,\n",
    "                                                                           step_sizes,\n",
    "                                                                           alphas,\n",
    "                                                                           nb_samples,\n",
    "                                                                           forward_kwargs,\n",
    "                                                                           attribute_to_layer_inputs)\n",
    "        # calculate convergence deltas\n",
    "        deltas = _compute_convergence_delta(self.model,\n",
    "                                            input_dtypes,\n",
    "                                            attributions,\n",
    "                                            baselines,\n",
    "                                            X,\n",
    "                                            forward_kwargs,\n",
    "                                            target,\n",
    "                                            self._is_list)\n",
    "\n",
    "        return self.build_explanation(\n",
    "            X=X,\n",
    "            forward_kwargs=forward_kwargs,\n",
    "            baselines=baselines,\n",
    "            target=target,\n",
    "            attributions=attributions,\n",
    "            deltas=deltas\n",
    "        )\n",
    "\n",
    "\n",
    "    def build_explanation(self,\n",
    "                          X: List[np.ndarray],\n",
    "                          forward_kwargs: Optional[dict],\n",
    "                          baselines: List[np.ndarray],\n",
    "                          target: Optional[List[int]],\n",
    "                          attributions: Union[List[np.ndarray], List[tf.Tensor]],\n",
    "                          deltas: np.ndarray) -> Explanation:\n",
    "\n",
    "        data = copy.deepcopy(DEFAULT_DATA_INTGRAD)\n",
    "        predictions = self.model(X).numpy()\n",
    "        if isinstance(attributions[0], tf.Tensor):\n",
    "            attributions = [attr.numpy() for attr in attributions]\n",
    "        data.update(X=X,\n",
    "                    forward_kwargs=forward_kwargs,\n",
    "                    baselines=baselines,\n",
    "                    target=target,\n",
    "                    attributions=attributions,\n",
    "                    deltas=deltas,\n",
    "                    predictions=predictions)\n",
    "\n",
    "        return Explanation(meta=copy.deepcopy(self.meta), data=data)\n",
    "\n",
    "\n",
    "    def reset_predictor(self, predictor: Union[tf.keras.Model]) -> None:\n",
    "        # TODO: check what else should be done (e.g. validate dtypes again?)\n",
    "        self.model = predictor\n",
    "\n",
    "\n",
    "    def _compute_attributions_list_input(self,\n",
    "                       X: List[np.ndarray],\n",
    "                       baselines: Union[List[int], List[float], List[np.ndarray]],\n",
    "                       target: Optional[List[int]],\n",
    "                       step_sizes: List[float],\n",
    "                       alphas: List[float],\n",
    "                       nb_samples: int,\n",
    "                       forward_kwargs: Optional[dict],\n",
    "                       compute_layer_inputs_gradients: bool) -> List:\n",
    "        \"\"\"For each tensor in a list of input tensors,\n",
    "        calculates the attributions for each feature or element of layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "        Instance for which integrated gradients attribution are computed.\n",
    "        baselines\n",
    "            Baselines (starting point of the path integral) for each instance.\n",
    "        target\n",
    "            Defines which element of the model output is considered to compute the gradients.\n",
    "        step_sizes\n",
    "            Weights in the path integral sum.\n",
    "        alphas\n",
    "            Interpolation parameter defining the points of the interal path.\n",
    "        nb_samples\n",
    "            Total number of samples.\n",
    "        forward_kwargs\n",
    "            Input keywords args.\n",
    "        compute_layer_inputs_gradients\n",
    "            In case of layers gradients, controls whether the gradients are computed for the layer's inputs or\n",
    "            outputs. If True, gradients are computed for the layer's inputs, if False for the layer's outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tuple with integrated gradients attributions, deltas and predictions\n",
    "\n",
    "        \"\"\"\n",
    "        #print(\"X type: \", type(X))\n",
    "        #print(\"X len: \", len(X))\n",
    "        #print('X[0] shape: ', X[0].shape)\n",
    "        #print('X[1] shape: ', X[1].shape)\n",
    "        attrs_dtypes = [ [xx.dtype for xx in X][0] ] # [# of outputs]\n",
    "        #print(\"attrs_dtypes type: \", type(attrs_dtypes))\n",
    "        #print(\"attrs_dtypes len: \", len(attrs_dtypes))\n",
    "        #print(\"attrs_dtypes[0] type: \", type(attrs_dtypes[0]))\n",
    "\n",
    "        # define paths in features' space\n",
    "        paths = []\n",
    "        #print(\"type(baselines): \", type(baselines))\n",
    "        #print(\"baselines[0].shape: \", baselines[0].shape)\n",
    "        for i in range(len(X)): #ListWrapper of length 2\n",
    "            x, baseline = X[i], baselines[i]  # type: ignore\n",
    "            # construct paths (279 X 802 X 32)\n",
    "            blist =[]\n",
    "            for j in range(self.n_steps): \n",
    "              tmp = baseline + alphas[j] * (x-baseline) \n",
    "              #print('alphas[j] shape: ', alphas[j].shape)\n",
    "              #print('alphas[j]: ', alphas[j])\n",
    "              #print(\"tmp shape:\", tmp.shape)            \n",
    "              blist.append(tmp)\n",
    "            path = np.concatenate(blist, axis=0)\n",
    "            #print(\"path shape\", path.shape)\n",
    "            paths.append(path)\n",
    "\n",
    "        if forward_kwargs is not None:\n",
    "            paths_kwargs = {k: np.concatenate([forward_kwargs[k] for _ in range(self.n_steps)], axis=0)\n",
    "                            for k in forward_kwargs.keys()}\n",
    "        else:\n",
    "            paths_kwargs = None\n",
    "\n",
    "        # define target paths\n",
    "        if target is not None:\n",
    "            target_paths = np.concatenate([target \n",
    "                    for _ in range(self.n_steps)],\n",
    "                    axis=0)\n",
    "        else:\n",
    "            target_paths = None\n",
    "        if forward_kwargs is not None:\n",
    "            if target_paths is not None:\n",
    "                #print(\"PATH1....\")\n",
    "                ds_args = tuple(p for p in paths) + (paths_kwargs, target_paths)\n",
    "            else:\n",
    "                #print(\"PATH2....\")\n",
    "                ds_args = tuple(p for p in paths) + (paths_kwargs,)\n",
    "\n",
    "        else:\n",
    "            if target_paths is not None:\n",
    "                #print(\"PATH3....\")\n",
    "                #print(\"len(target_paths): \", len(target_paths))\n",
    "                #print(\"target_paths[0].shape: \", target_paths[0].shape)\n",
    "                #print(\"target_paths[0]: \", target_paths[0])\n",
    "                #print(\"len(paths): \", len(paths))\n",
    "                #print(\"type(paths): \", type(paths))\n",
    "                #print(\"paths[0].shape: \", paths[0].shape)\n",
    "                #print(\"paths[1].shape: \", paths[1].shape)\n",
    "                #print(\"paths: \", paths)\n",
    "                #ds_args = tuple(p for p in paths) + (target_paths,)\n",
    "                ds_args = tuple([paths[0]]) + (target_paths,)\n",
    "            else:\n",
    "                #print(\"PATH4....\")\n",
    "                ds_args = tuple(p for p in paths)\n",
    "        #print('len(ds_args): ', len(ds_args))\n",
    "        #print('ds_args[0].shape: ', ds_args[0].shape)\n",
    "        #print('ds_args[1].shape: ', ds_args[1].shape)\n",
    "        paths_ds = tf.data.Dataset.from_tensor_slices(ds_args).batch(self.internal_batch_size)\n",
    "        paths_ds.as_numpy_iterator()\n",
    "        paths_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        #print(\"paths_ds len: \", len(paths_ds))\n",
    "\n",
    "        # calculate gradients for batches\n",
    "        batches = []\n",
    "        for path in paths_ds:\n",
    "            #print(\"path type: \", type(path))\n",
    "            #print(\"path len: \", len(path))\n",
    "            if forward_kwargs is not None:\n",
    "                if target is not None:\n",
    "                    paths_b, kwargs_b, target_b = path[:-2], path[-2], path[-1]\n",
    "                else:\n",
    "                    paths_b, kwargs_b = path\n",
    "                    target_b = None\n",
    "            else:\n",
    "                if target is not None:\n",
    "                    paths_b, target_b = path[:-1], path[-1]\n",
    "                    kwargs_b = None\n",
    "                else:\n",
    "                    paths_b, kwargs_b, target_b = path, None, None\n",
    "\n",
    "            paths_b = [tf.dtypes.cast(paths_b[i], attrs_dtypes[i]) for i in range(len(paths_b))]\n",
    "\n",
    "            #print(\"self.layer: \", self.layer)\n",
    "            if self.layer is None:\n",
    "                grads_b = _gradients_input(self.model, paths_b, target_b, forward_kwargs=kwargs_b)\n",
    "            else:\n",
    "                grads_b = _gradients_layer(self.model,\n",
    "                                           self.layer,\n",
    "                                           self.orig_call,\n",
    "                                           self.orig_dummy_input,\n",
    "                                           paths_b,\n",
    "                                           target_b,\n",
    "                                           forward_kwargs=kwargs_b,\n",
    "                                           compute_layer_inputs_gradients=compute_layer_inputs_gradients)\n",
    "            #print(\"grads_b type: \", type(grads_b))\n",
    "            #print(\"grads_b shape: \", grads_b.shape)\n",
    "\n",
    "            batches.append(grads_b)\n",
    "\n",
    "        # multi-input\n",
    "        #print(\"batches type: \", type(batches))\n",
    "        #print(\"len(batches): \", len(batches))\n",
    "        #print(\"batches[0] len\",len(batches[0]) )\n",
    "        #print(\"attrs_dtypes type: \", type(attrs_dtypes))\n",
    "        #print(\"len(attrs_dtypes): \", len(attrs_dtypes))\n",
    "        #print(\"batches[0]: \", batches[0])\n",
    "\n",
    "        batches = [[batches[i][j] for i in range(len(batches))] for j in range(len(attrs_dtypes))]\n",
    "\n",
    "        # calculate attributions from gradients batches\n",
    "        attributions = []\n",
    "        for j in range(len(attrs_dtypes)):\n",
    "            sum_int = _calculate_sum_int(batches, self.model,\n",
    "                                         target, target_paths,\n",
    "                                         self.n_steps, nb_samples,\n",
    "                                         step_sizes, j)\n",
    "            norm = X[j] - baselines[j]  # type: ignore\n",
    "            attribution = norm * sum_int\n",
    "            attributions.append(attribution)\n",
    "        return attributions\n",
    "\n",
    "    def _compute_attributions_tensor_input(self,\n",
    "                                           X: Union[np.ndarray, tf.Tensor],\n",
    "                                           baselines: Union[np.ndarray, tf.Tensor],\n",
    "                                           target: Optional[List[int]],\n",
    "                                           step_sizes: List[float],\n",
    "                                           alphas: List[float],\n",
    "                                           nb_samples: int,\n",
    "                                           forward_kwargs: Optional[dict],\n",
    "                                           compute_layer_inputs_gradients: bool) -> List:\n",
    "        \"\"\"For a single input tensor, calculates the attributions for each input feature or element of layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Instance for which integrated gradients attribution are computed.\n",
    "        baselines\n",
    "            Baselines (starting point of the path integral) for each instance.\n",
    "        target\n",
    "            Defines which element of the model output is considered to compute the gradients.\n",
    "        step_sizes\n",
    "            Weights in the path integral sum.\n",
    "        alphas\n",
    "            Interpolation parameter defining the points of the interal path.\n",
    "        nb_samples\n",
    "            Total number of samples.\n",
    "        forward_kwargs\n",
    "            Inputs keywords args.\n",
    "        compute_layer_inputs_gradients\n",
    "            In case of layers gradients, controls whether the gradients are computed for the layer's inputs or\n",
    "            outputs. If True, gradients are computed for the layer's inputs, if False for the layer's outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tuple with integrated gradients attributions, deltas and predictions\n",
    "        \"\"\"\n",
    "        # define paths in features's or layers' space\n",
    "        paths = np.concatenate([baselines + alphas[i] * (X - baselines) for i in range(self.n_steps)], axis=0)\n",
    "\n",
    "        if forward_kwargs is not None:\n",
    "            paths_kwargs = {k: np.concatenate([forward_kwargs[k] for _ in range(self.n_steps)], axis=0)\n",
    "                            for k in forward_kwargs.keys()}\n",
    "        else:\n",
    "            paths_kwargs = None\n",
    "\n",
    "        # define target paths\n",
    "        if target is not None:\n",
    "            target_paths = np.concatenate([target for _ in range(self.n_steps)], axis=0)\n",
    "        else:\n",
    "            target_paths = None\n",
    "\n",
    "        if forward_kwargs is not None:\n",
    "            if target_paths is not None:\n",
    "                ds_args = (paths, paths_kwargs, target_paths)\n",
    "            else:\n",
    "                ds_args = (paths, paths_kwargs)  # type: ignore\n",
    "        else:\n",
    "            if target_paths is not None:\n",
    "                ds_args = (paths, target_paths)  # type: ignore\n",
    "            else:\n",
    "                ds_args = paths\n",
    "\n",
    "        paths_ds = tf.data.Dataset.from_tensor_slices(ds_args).batch(self.internal_batch_size)\n",
    "        paths_ds.as_numpy_iterator()\n",
    "        paths_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        # calculate gradients for batches\n",
    "        batches = []\n",
    "        for path in paths_ds:\n",
    "            if forward_kwargs is not None:\n",
    "                if target is not None:\n",
    "                    paths_b, kwargs_b, target_b = path\n",
    "                else:\n",
    "                    paths_b, kwargs_b = path\n",
    "                    target_b = None\n",
    "            else:\n",
    "                kwargs_b = None\n",
    "                if target is not None:\n",
    "                    paths_b, target_b = path\n",
    "                else:\n",
    "                    paths_b, target_b = path, None\n",
    "\n",
    "            if self.layer is None:\n",
    "                grads_b = _gradients_input(self.model, paths_b, target_b, forward_kwargs=kwargs_b)\n",
    "\n",
    "            else:\n",
    "                grads_b = _gradients_layer(self.model,\n",
    "                                           self.layer,\n",
    "                                           self.orig_call,\n",
    "                                           self.orig_dummy_input,\n",
    "                                           paths_b,\n",
    "                                           target_b,\n",
    "                                           forward_kwargs=kwargs_b,\n",
    "                                           compute_layer_inputs_gradients=compute_layer_inputs_gradients)\n",
    "\n",
    "            batches.append(grads_b)\n",
    "\n",
    "        # calculate attributions from gradients batches\n",
    "        attributions = []\n",
    "        sum_int = _calculate_sum_int([batches], self.model,\n",
    "                                     target, target_paths,\n",
    "                                     self.n_steps, nb_samples,\n",
    "                                     step_sizes, 0)\n",
    "        #print('sum_int shape: ', sum_int.shape)\n",
    "        norm = X - baselines\n",
    "\n",
    "        attribution = norm * sum_int\n",
    "        attributions.append(attribution)\n",
    "\n",
    "        return attributions\n",
    "    def _mk(self) -> List:\n",
    "        step_sizes_func, mk_func = approximation_parameters(self.method)\n",
    "        step_sizes, mk = step_sizes_func(self.n_steps), mk_func(self.n_steps)\n",
    "        return mk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1J8DFouqmVQ"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Attribution_all/x_newtrain_attribution_code.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m xtrain \u001b[38;5;241m=\u001b[39m loadTestTrainData(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Max-len-1000/x_train[0]_0.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m xtrain_attr \u001b[38;5;241m=\u001b[39m \u001b[43mloadTestTrainData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Attribution_all/x_newtrain_attribution_code.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ytrain \u001b[38;5;241m=\u001b[39m loadTestTrainData(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Max-len-1000/y_train[0]_0.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m xvalidation \u001b[38;5;241m=\u001b[39m loadTestTrainData(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Max-len-1000/x_validation[0]_0.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mloadTestTrainData\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadTestTrainData\u001b[39m(filename): \u001b[38;5;66;03m# e.g., 'test.npy'    \u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     21\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(f, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Attribution_all/x_newtrain_attribution_code.npy'"
     ]
    }
   ],
   "source": [
    "xtrain = loadTestTrainData('D:/Code/Project-AI-JAVA-ANNOTATION-2021/Side-Project/0000/Max-len-1000/x_train[0]_0.npy')\n",
    "xtrain_attr = loadTestTrainData('C:/Users/YOLOHsu/Desktop/10366046/Attribution_all/x_newtrain_attribution_code.npy')\n",
    "ytrain = loadTestTrainData('C:/Users/YOLOHsu/Desktop/10366046/Max-len-1000/y_train[0]_0.npy')\n",
    "xvalidation = loadTestTrainData('C:/Users/YOLOHsu/Desktop/10366046/Max-len-1000/x_validation[0]_0.npy')\n",
    "xvalidation_attr = loadTestTrainData('C:/Users/YOLOHsu/Desktop/10366046/Attribution_all/x_newvalidation_attribution_code.npy')\n",
    "yvalidation = loadTestTrainData('C:/Users/YOLOHsu/Desktop/10366046/Max-len-1000/y_validation[0]_0.npy')\n",
    "xtrain = model.layers[1](xtrain)[0]\n",
    "xvalidation = model.layers[1](xvalidation)[0]\n",
    "print('xtrain shape: ', xtrain.shape)\n",
    "print('xtrain_attr shape: ', xtrain_attr.shape)\n",
    "print('ytrain shape: ', ytrain.shape)\n",
    "print('xvalidation shape: ', xvalidation.shape)\n",
    "print('xvalidation_attr shape: ', xvalidation_attr.shape)\n",
    "print('yvalidation shape: ', yvalidation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        s1 = 35\n",
    "        s2 = 35\n",
    "        s3 = 35\n",
    "        new_model = newmodel(model)\n",
    "        # method = 'riemann_left', 'riemann_right', 'riemann_middle', 'riemann_trapezoid', 'gausslegendre'\n",
    "        n_steps = 6\n",
    "        ig  = IntegratedGradients1(new_model,\n",
    "                    layer=new_model.layers[1],\n",
    "                    n_steps=n_steps,\n",
    "                    method='riemann_trapezoid',\n",
    "                    internal_batch_size=32)\n",
    "        type_weight = 1\n",
    "        learning_rate_value = 0.0001\n",
    "        batch_size_num = 32\n",
    "        epochs_value = 10\n",
    "        losses = {\"classifier_model\": \"binary_crossentropy\"}\n",
    "        lossWeights = {\"classifier_model\": type_weight}\n",
    "        metrics = {\"classifier_model\": \"binary_accuracy\"}\n",
    "        num_train = 215 #215 len(xtrain)\n",
    "        num_validation = 215 # 215 len(xvalidation)\n",
    "        newtrain = [[[None for _ in range(32)]for _ in range(1001)]  for _ in range(num_train)]\n",
    "        newtrain = np.asarray(newtrain)\n",
    "        print('newtrain shape: ', newtrain.shape)\n",
    "        newvalidation = [[[None for _ in range(32)]for _ in range(1001)]  for _ in range(num_validation)]\n",
    "        newvalidation = np.asarray(newvalidation)\n",
    "        print('newvalidation shape: ', newvalidation.shape)\n",
    "        attr_train = [[[None for _ in range(32)]for _ in range(1001)]  for _ in range(num_train*2)]\n",
    "        attr_train = np.asarray(attr_train).astype('float32')\n",
    "        print('attr_train shape: ', attr_train.shape)\n",
    "        attr_validation = [[[None for _ in range(32)]for _ in range(1001)]  for _ in range(num_validation*2)]\n",
    "        attr_validation = np.asarray(attr_validation).astype('float32')\n",
    "        print('attr_validation shape: ', attr_validation.shape)\n",
    "        binary_acc_train = []\n",
    "        binary_acc_validation = []\n",
    "        attr_mse_train = []\n",
    "        attr_mse_validation = []\n",
    "        alpha = 0.5\n",
    "        eta = 0.5\n",
    "        for i in range(10):\n",
    "            print('----- ', i, ' -----')\n",
    "            start = time.time()\n",
    "        #---產生資料---\n",
    "            # --- train data---\n",
    "            print('########## train data ##########')\n",
    "            start_train = time.time()\n",
    "            start_delta_l = time.time()\n",
    "            for b in range(0,num_train,s1):\n",
    "                e = b + s1\n",
    "                if e>num_train: e = num_train\n",
    "                #print('-----', b, '-----', e, '-----')\n",
    "                #print('xtrain[b:e] shape: ', xtrain[b:e].shape) #(s,1001,32)\n",
    "                #print('ytrain[b:e] shape: ', ytrain[b:e].shape) #(s,36)\n",
    "\n",
    "                # delta_l = -alpha * grad_loss\n",
    "                with tf.GradientTape() as g:\n",
    "                    x = xtrain[b:e]\n",
    "                    y = ytrain[b:e]\n",
    "                    g.watch(x)\n",
    "                    y_true = y\n",
    "                    y_pred = new_model(x)\n",
    "                    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "                    #print('loss: ', loss)\n",
    "                #print('67: ', get_gpu_memory())\n",
    "                gt = g.gradient(loss, x)\n",
    "                #print('68: ', get_gpu_memory())\n",
    "                #print('gt: ', gt)\n",
    "                delta_l = -alpha * gt\n",
    "                #print('delta_l shape: ', delta_l.shape) #(s,1001,32)\n",
    "                newtrain[b:e] = xtrain[b:e] + delta_l\n",
    "            end_delta_l = time.time()\n",
    "            print('train data delta_l time: ', end_delta_l-start_delta_l)\n",
    "\n",
    "            start_delta_a = time.time()\n",
    "            for b in range(0,num_train,s2):\n",
    "                e = b + s2\n",
    "                if e>num_train: e = num_train\n",
    "                #print('-----', b, '-----', e, '-----')\n",
    "                #print('xtrain[b:e] shape: ', xtrain[b:e].shape) #(s,1001,32)\n",
    "                #print('ytrain[b:e] shape: ', ytrain[b:e].shape) #(s,36)\n",
    "\n",
    "                # delta_a = -eta * ( attrx - attrx_ ) * attrx_gradient\n",
    "                x = np.asarray(xtrain).astype('float32')\n",
    "                predictions = new_model(xtrain[b:e]).numpy()\n",
    "                #print('86: ', get_gpu_memory())\n",
    "                explanation = ig.explain(x[b:e], baselines=None, target=predictions, attribute_to_layer_inputs=False).attributions[0]\n",
    "                #print('88: ', get_gpu_memory())\n",
    "                attrx = explanation\n",
    "                #print('attrx shape: ', attrx.shape) #(s,1001,32)\n",
    "                attrx_ = xtrain_attr[b:e]\n",
    "                #print('attrx_ shape: ', attrx_.shape) #(s,1001,32)\n",
    "                mk = ig._mk()\n",
    "                #attrx_gradient = attrx_grad(xtrain[b:e], attrx, mk, new_model, n_steps)\n",
    "                zk_grad_x = attrx / xtrain[b:e]\n",
    "                zk_grad2_zk = 0\n",
    "                for j in range(n_steps):\n",
    "                    zk = xtrain[b:e] * mk[j]\n",
    "                    with tf.GradientTape() as g:\n",
    "                        x = xtrain[b:e]\n",
    "                        g.watch(x)\n",
    "                        with tf.GradientTape() as gg:\n",
    "                            gg.watch(x)\n",
    "                            loss = new_model(x)\n",
    "                        ggt = gg.gradient(loss, x)\n",
    "                    gt = g.gradient(ggt, x)\n",
    "                    ans = gt * (mk[j] * mk[j])\n",
    "                    zk_grad2_zk += ans\n",
    "                attrx_gradient = zk_grad_x + (xtrain[b:e]/n_steps) + zk_grad2_zk\n",
    "                #print('attrx_gradient shape: ', attrx_gradient.shape) #(s,1001,32)\n",
    "                delta_a = -eta * ( attrx - attrx_ ) * attrx_gradient\n",
    "                #print('delta_a shape: ', delta_a.shape) #(s,1001,32)\n",
    "                newtrain[b:e] = newtrain[b:e] + delta_a\n",
    "            end_delta_a = time.time()\n",
    "            print('train data delta_a time: ', end_delta_a-start_delta_a)\n",
    "            #saveTestTrainData('D:/00/train.npy', newx0)\n",
    "            #print(newx)\n",
    "            end_train = time.time()\n",
    "            print('train data time: ', end_train-start_train)\n",
    "            '''\n",
    "            new_model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss=losses, loss_weights=lossWeights, metrics=metrics)\n",
    "            x = np.asarray(newtrain).astype('float32')\n",
    "            y = np.asarray(ytrain[0:num_train]).astype('float32')\n",
    "            new_model.evaluate(x, y)\n",
    "            '''\n",
    "\n",
    "            # --- validation data ---\n",
    "            print('########## validation data ##########')\n",
    "            start_validation = time.time()\n",
    "            start_delta_l = time.time()\n",
    "            for b in range(0,num_validation,s1):\n",
    "                e = b + s1\n",
    "                if e>num_validation: e = num_validation\n",
    "                #print('-----', b, '-----', e, '-----')\n",
    "                #print('xvalidation[b:e] shape: ', xvalidation[b:e].shape) #(s,1001,32)\n",
    "                #print('yvalidation[b:e] shape: ', yvalidation[b:e].shape) #(s,36)\n",
    "\n",
    "                # delta_l = -alpha * grad_loss\n",
    "                with tf.GradientTape() as g:\n",
    "                    x = xvalidation[b:e]\n",
    "                    y = yvalidation[b:e]\n",
    "                    g.watch(x)\n",
    "                    y_true = y\n",
    "                    y_pred = new_model(x)\n",
    "                    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "                    #print('loss: ', loss)\n",
    "                #print('149: ', get_gpu_memory())\n",
    "                gt = g.gradient(loss, x)\n",
    "                #print('151: ', get_gpu_memory())\n",
    "                #print('gt: ', gt)\n",
    "                delta_l = -alpha * gt\n",
    "                #print('delta_l shape: ', delta_l.shape) #(s,1001,32)\n",
    "                newvalidation[b:e] = xvalidation[b:e] + delta_l\n",
    "            end_delta_l = time.time()\n",
    "            print('validation data delta_l time: ', end_delta_l-start_delta_l)\n",
    "\n",
    "            start_delta_a = time.time()\n",
    "            for b in range(0,num_validation,s2):\n",
    "                e = b + s2\n",
    "                if e>num_validation: e = num_validation\n",
    "                #print('-----', b, '-----', e, '-----')\n",
    "                #print('xvalidation[b:e] shape: ', xvalidation[b:e].shape) #(s,1001,32)\n",
    "                #print('yvalidation[b:e] shape: ', yvalidation[b:e].shape) #(s,36)\n",
    "\n",
    "                # delta_a = -eta * ( attrx - attrx_ ) * attrx_gradient\n",
    "                x = np.asarray(xvalidation).astype('float32')\n",
    "                predictions = new_model(xvalidation[b:e]).numpy()\n",
    "                #print('166: ', get_gpu_memory())\n",
    "                explanation = ig.explain(x[b:e], baselines=None, target=predictions, attribute_to_layer_inputs=False).attributions[0]\n",
    "                #print('168: ', get_gpu_memory())\n",
    "                attrx = explanation\n",
    "                #print('attrx shape: ', attrx.shape) #(s,1001,32)\n",
    "                attrx_ = xvalidation_attr[b:e]\n",
    "                #print('attrx_ shape: ', attrx_.shape) #(s,1001,32)\n",
    "                mk = ig._mk()\n",
    "                #attrx_gradient = attrx_grad(xvalidation[b:e], attrx, mk, new_model, n_steps)\n",
    "                zk_grad_x = attrx / xvalidation[b:e]\n",
    "                zk_grad2_zk = 0\n",
    "                for j in range(n_steps):\n",
    "                    zk = xvalidation[b:e] * mk[j]\n",
    "                    with tf.GradientTape() as g:\n",
    "                        x = xvalidation[b:e]\n",
    "                        g.watch(x)\n",
    "                        with tf.GradientTape() as gg:\n",
    "                            gg.watch(x)\n",
    "                            loss = new_model(x)\n",
    "                        ggt = gg.gradient(loss, x)\n",
    "                    gt = g.gradient(ggt, x)\n",
    "                    ans = gt * (mk[j] * mk[j])\n",
    "                    zk_grad2_zk += ans\n",
    "                attrx_gradient = zk_grad_x + (xvalidation[b:e]/n_steps) + zk_grad2_zk\n",
    "                #print('attrx_gradient shape: ', attrx_gradient.shape) #(s,1001,32)\n",
    "                delta_a = -eta * ( attrx - attrx_ ) * attrx_gradient\n",
    "                #print('delta_a shape: ', delta_a.shape) #(s,1001,32)\n",
    "                newvalidation[b:e] = newvalidation[b:e] + delta_a\n",
    "            end_delta_a = time.time()\n",
    "            print('validation data delta_a time: ', end_delta_a-start_delta_a)\n",
    "            #saveTestvalidationData('D:/00/validation.npy', newx0)\n",
    "            #print(newx)\n",
    "            end_validation = time.time()\n",
    "            print('validation data time: ', end_validation-start_validation)\n",
    "            '''\n",
    "            new_model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss=losses, loss_weights=lossWeights, metrics=metrics)\n",
    "            x = np.asarray(newvalidation).astype('float32')\n",
    "            y = np.asarray(yvalidation[0:num_validation]).astype('float32')\n",
    "            new_model.evaluate(x, y)\n",
    "            '''\n",
    "\n",
    "        #----- training new model -----\n",
    "            print('########## training new model ##########')\n",
    "            xtrain0 = xtrain[0:num_train]\n",
    "            ytrain0 = ytrain[0:num_train]\n",
    "            x_train = np.concatenate((xtrain0, newtrain), axis=0)\n",
    "            x_train = np.asarray(x_train).astype('float32')\n",
    "            y_train = np.concatenate((ytrain0, ytrain0), axis=0)\n",
    "            xvalidation0 = xvalidation[0:num_validation]\n",
    "            yvalidation0 = yvalidation[0:num_validation]\n",
    "            x_validation = np.concatenate((xvalidation0, newvalidation), axis=0)\n",
    "            x_validation = np.asarray(x_validation).astype('float32')\n",
    "            y_validation = np.concatenate((yvalidation0, yvalidation0), axis=0)\n",
    "            #print('x_train shape: ', x_train.shape) #(num_train*2,1001,32)\n",
    "            #print('y_train shape: ', y_train.shape) #(num_train*2,36)\n",
    "            #print('x_validation shape: ', x_validation.shape) #(num_validation*2,1001,32)\n",
    "            #print('y_validation shape: ', y_validation.shape) #(num_validation*2,36)\n",
    "            new_model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss=losses, loss_weights=lossWeights, metrics=metrics)\n",
    "\n",
    "            history = new_model.fit(\n",
    "                          x = x_train,\n",
    "                          y = y_train,\n",
    "                          epochs = epochs_value, #100 200 500 3000\n",
    "                          verbose = 2, #set visibility\n",
    "                          #callbacks = [model_checkpoint_callback],\n",
    "                          validation_data = (x_validation, y_validation), #-> has issue \n",
    "                          batch_size = batch_size_num\n",
    "                          )\n",
    "\n",
    "        #----- binary acc & mse -----\n",
    "            print('########## binary acc & mse ##########')\n",
    "\n",
    "            binary_acc_train .append(history.history['binary_accuracy'][epochs_value-1])\n",
    "            binary_acc_validation .append(history.history['val_binary_accuracy'][epochs_value-1])\n",
    "            print('binary_accuracy train: ', history.history['binary_accuracy'][epochs_value-1])\n",
    "            print('binary_accuracy validation: ', history.history['val_binary_accuracy'][epochs_value-1])\n",
    "\n",
    "            # ---- attribution train -----\n",
    "            print('---- attribution train -----')\n",
    "            start_att = time.time()\n",
    "            for b in range(0,num_train*2,s3):\n",
    "                e = b + s3\n",
    "                if e>num_train*2: e = num_train*2\n",
    "                #print('-----', b, '-----', e, '-----')\n",
    "                #print('x_train[b:e] shape: ', x_train[b:e].shape)\n",
    "                #print('y_train[b:e] shape: ', y_train[b:e].shape)\n",
    "\n",
    "                predictions = y_train[b:e]\n",
    "                #print('254: ', get_gpu_memory())\n",
    "                explanation = ig.explain(x_train[b:e], baselines=None, target=predictions, attribute_to_layer_inputs=False)\n",
    "                #print('256: ', get_gpu_memory())\n",
    "                attr_train[b:e] = explanation.attributions[0]\n",
    "\n",
    "            attr_train_pred = tf.abs(attr_train)\n",
    "            attr_train_pred = tf.reduce_sum(attr_train_pred, axis=2).numpy()/32\n",
    "            #print('attr_train_pred shape: ', attr_train_pred.shape) #(num_train*2,1001)\n",
    "            attr_train_true = xtrain_attr[0:num_train]\n",
    "            attr_train_true = tf.reduce_sum(attr_train_true, axis=2).numpy()/32\n",
    "            attr_train_true = tf.concat([attr_train_true, attr_train_true], 0)\n",
    "            #print('attr_train_true shape: ', attr_train_true.shape) #(num_train*2,1001)\n",
    "            #average = tf.reduce_sum(attrx_pred, axis=1).numpy()/1001\n",
    "            #print('average: ', average)\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            attr_mse = mse(attr_train_true, attr_train_pred).numpy()\n",
    "            attr_mse_train.append(attr_mse)\n",
    "            print('attr_mse train: ', attr_mse)\n",
    "            end_att = time.time()\n",
    "            print('train attribution time: ', end_att-start_att)\n",
    "            # ---- attributoin validation -----\n",
    "            print('---- attribution validation -----')\n",
    "            start_att = time.time()\n",
    "            for b in range(0,num_validation*2,s3):\n",
    "                e = b + s3\n",
    "                if e>num_validation*2: e = num_validation*2\n",
    "                #print('-----', b, '-----', e, '-----')\n",
    "                #print('x_validation[b:e] shape: ', x_validation[b:e].shape)\n",
    "                #print('y_validation[b:e] shape: ', y_validation[b:e].shape)\n",
    "\n",
    "                predictions = y_validation[b:e]\n",
    "                #print('285: ', get_gpu_memory())\n",
    "                explanation = ig.explain(x_validation[b:e], baselines=None, target=predictions, attribute_to_layer_inputs=False)\n",
    "                #print('287: ', get_gpu_memory())\n",
    "                attr_validation[b:e] = explanation.attributions[0]\n",
    "            attr_validation_pred = tf.abs(attr_validation)\n",
    "            attr_validation_pred = tf.reduce_sum(attr_validation_pred, axis=2).numpy()/32\n",
    "            #print('attr_validation_pred shape: ', attr_validation_pred.shape) #(num_validation*2,1001)\n",
    "            attr_validation_true = xvalidation_attr[0:num_validation]\n",
    "            attr_validation_true = tf.reduce_sum(attr_validation_true, axis=2).numpy()/32\n",
    "            attr_validation_true = tf.concat([attr_validation_true, attr_validation_true], 0)\n",
    "            #print('attr_validation_true shape: ', attr_validation_true.shape) #(num_validation*2,1001)\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            attr_mse = mse(attr_validation_true, attr_validation_pred).numpy()\n",
    "            attr_mse_validation.append(attr_mse)\n",
    "            print('attr_mse validation: ', attr_mse)\n",
    "            end_att = time.time()\n",
    "            print('validation attribution time: ', end_att-start_att)\n",
    "            end = time.time()\n",
    "            print('time: ', end-start)\n",
    "        plotAttributionAcc(attr_mse_train)\n",
    "        print('----- train acc -----')\n",
    "        print('binary_acc_train: ', binary_acc_train)\n",
    "        print('attr_mse_train: ', attr_mse_train)\n",
    "        print('----- validation acc -----')\n",
    "        print('binary_acc_validation: ', binary_acc_validation)\n",
    "        print('attr_mse_validation: ', attr_mse_validation)\n",
    "            #print(\"Model training completed...\")\n",
    "            #save history\n",
    "            #print(\"Saving history...\")\n",
    "            #saveDictionary(history.history, Trained_model_Path + \"/\" + \"model_history\")\n",
    "            #print(\"History saving completed...\")\n",
    "\n",
    "            #save model\n",
    "            #print(\"Saving model...\")\n",
    "            #model.save(Trained_model_Path + \"/\" + \"test_model1.h5\")\n",
    "            #print(\"Model saving completed...\")\n",
    "\n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "achZsSjjc6hw",
    "U4NG1PGqXQkm"
   ],
   "name": "attribution_train_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
