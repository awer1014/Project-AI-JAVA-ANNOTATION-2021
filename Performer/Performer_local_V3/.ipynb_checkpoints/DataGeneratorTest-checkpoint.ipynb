{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import DataGenerator2_2 as DG2\n",
    "import math\n",
    "import os\n",
    "import DataBuffer as db\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'\n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateInputBLKData(datafilename, destDir1, filename1 = \"blk_\", block_size = 1000, fext = \".npy\", offset = 0):\n",
    "    if not os.path.exists(destDir1):\n",
    "        os.mkdir(destDir1)  #create patchbackfiledir case dir\n",
    "        \n",
    "    data = list(loadTestTrainData(datafilename))\n",
    "    print(\"type(train):\", type(data))\n",
    "    print(\"train[0].shape:\", data[0].shape) #encoder inputs\n",
    "    lines = data[0].shape[0] # 279 lines\n",
    "    \n",
    "    block_numer = math.ceil(lines/block_size)\n",
    "    \n",
    "    for blk_id in range(block_numer):\n",
    "        print(\"blk_idx: \", blk_id)\n",
    "        blk_offset = blk_id*block_size\n",
    "        blk_end = min((blk_id+1)*block_size, lines)\n",
    "        saveTestTrainData(destDir1 + filename1 + str(offset+blk_id) + \".npy\", data[0][blk_offset:blk_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateOutputBLKData(datafilename, destDir1, filename=\"blk_\", block_size=1000, fext=\".npy\"):\n",
    "    if not os.path.exists(destDir1):\n",
    "        os.mkdir(destDir1)  #create patchbackfiledir case dir\n",
    "    \n",
    "    data = list(loadTestTrainData(datafilename))\n",
    "    print(\"len(data):\", len(data))\n",
    "    print(\"type(data):\", type(data))\n",
    "    print(\"data.shape:\", data[0].shape) #encoder inputs\n",
    "    lines = len(data) # 279 lines\n",
    "    print(\"lines:\", lines) #encoder inputs\n",
    "    \n",
    "    block_numer = math.ceil( lines / block_size )\n",
    "    \n",
    "    for blk_id in range(block_numer):\n",
    "        print(\"blk_idx: \", blk_id)\n",
    "        blk_offset = blk_id*block_size\n",
    "        blk_end = min((blk_id+1)*block_size, lines)\n",
    "        saveTestTrainData(destDir1 + \"blk_\"+str(blk_id) + \".npy\", data[blk_offset:blk_end])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#### Split data to blocks  ##########\n",
    "    \"\"\"\n",
    "    destDir1 = \"model\\\\\"\n",
    "    for i in range(6):\n",
    "        generateInputBLKData(i, \"x_train_\"+ str(i)+ \".npy\", destDir1, destDir2, block_size = 48048, filename1=\"x_train[0]_\", filename2=\"x_train[1]_\")\n",
    "    \n",
    "    destDir3 = \"Data\\\\DecoderOutput1\\\\\"\n",
    "    destDir4 = \"Data\\\\DecoderOutput2\\\\\"\n",
    "    generateOutputBLKData(\"y_train[0].npy\", destDir3, block_size = 50)\n",
    "    generateOutputBLKData(\"y_train[1].npy\", destDir4, block_size = 50)\n",
    "    \"\"\"\n",
    "######################################################\n",
    "    #for output\n",
    "\n",
    "    input_buffer_params = {\"data_path\": [\"Performer_local_V3\\Trianing\\InputTxt\"],\n",
    "                           \"data_number\": 302400,\n",
    "                           \"data_type\": [int],\n",
    "                           \"block_size\": 5400\n",
    "                           }\n",
    "        \n",
    "    # for input\n",
    "    output_buffer_params = {\n",
    "        \"data_path\": [\"Performer_local_V3\\\\Model-for-training\",\n",
    "                      \"Performer_local_V3\\\\Model-for-training\"],\n",
    "        \"data_number\": [302400, 3024000], #[270670, 270670] \n",
    "        \"data_type\": [int, int], #[int, int]\n",
    "        \"block_size\": [5400, 5400] #[48048, 48048]\n",
    "        }\n",
    "    \n",
    "    input_data_path = input_buffer_params[\"data_path\"]\n",
    "    input_data_number = input_buffer_params[\"data_number\"]\n",
    "    input_data_type = input_buffer_params[\"data_type\"]\n",
    "    input_block_size = input_buffer_params[\"block_size\"]\n",
    "    \n",
    "    output_data_path = output_buffer_params[\"data_path\"]\n",
    "    output_data_number = output_buffer_params[\"data_number\"]\n",
    "    output_data_type = output_buffer_params[\"data_type\"]\n",
    "    output_block_size = output_buffer_params[\"block_size\"]\n",
    "    \n",
    "    input_db1 = db.DataBuffer(input_data_path, input_data_number, input_data_type, input_block_size, file_name=\"x_train[0]_\" )\n",
    "\n",
    "    output_db1 = db.DataBuffer(output_data_path[0], output_data_number[0], output_data_type[0], output_block_size[0], file_name=\"y_train[0]_\" )\n",
    "    \t\n",
    "    output_db2 = db.DataBuffer(output_data_path[1], output_data_number[1], output_data_type[1], output_block_size[1], file_name=\"y_train[1]_\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(\"i: \", i)\n",
    "        data_id  = randrange(302400)\n",
    "        data1 = input_db1.get_data(data_id)\n",
    "        data3 = output_db1.get_data(data_id)\n",
    "        data4 = output_db2.get_data(data_id)\n",
    "        print(\"data1 :\", (data1))\n",
    "        print(\"data2 :\", (data2))\n",
    "        print(\"data3 :\", (data3))\n",
    "        print(\"data4 :\", (data4))\n",
    "\n",
    "        \"\"\"\n",
    "        data = db1.get_data(200)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        #print(\"data :\", (data))\n",
    "        \n",
    "        data = db1.get_data(10)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        #print(\"data :\", (data))\n",
    "\n",
    "        data = db1.get_data(110)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        #print(\"data :\", (data))\n",
    "        \n",
    "        data = db1.get_data(113)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        \"\"\"\n",
    "    #print(\"data :\", (data))\n",
    "    \n",
    "    #x = DG2.DataGenerator1(buffer_params, [list(range(279)),list(range(279))] , None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
