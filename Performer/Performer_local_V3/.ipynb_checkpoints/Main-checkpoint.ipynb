{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d94ffe56",
    "outputId": "1249cd6b-bff9-44bc-f32e-212835586c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\W.R_Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performer as tfr\n",
    "import nltk\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "9b54d338",
    "outputId": "805aecc7-247e-4761-a0f1-f2ac4bb1e054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef solve_cudnn_error():\\n    import tensorflow as tf\\n    gpus = tf.config.experimental.list_physical_devices(\\'GPU\\')\\n    if gpus:\\n        try:\\n            # Currently, memory growth needs to be the same across GPUs\\n            for gpu in gpus:\\n                tf.config.experimental.set_memory_growth(gpu, True)\\n            logical_gpus = tf.config.experimental.list_logical_devices(\\'GPU\\')\\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n        except RuntimeError as e:\\n            # Memory growth must be set before GPUs have been initialized\\n            print(e)\\n            \\nsolve_cudnn_error()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def solve_cudnn_error():\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "            \n",
    "solve_cudnn_error()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "22361e8f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(file_name):\n",
    "    errlist=[]\n",
    "    LBlist=[]\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "    #讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows: \n",
    "            RL=list(row.values())\n",
    "            #print(\"RL[0]: \", type(RL[0]), \"RL[1]: \", type(RL[1]))\n",
    "            RL[1:] = list(map(int, RL[1:]))\n",
    "            errs=RL[1:37]\n",
    "            LB=RL[37:]\n",
    "            errlist.append(errs)\n",
    "            LBlist.append(LB)\n",
    "    return errlist,LBlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b92cf71a"
   },
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1b88909b"
   },
   "outputs": [],
   "source": [
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "abb41dde"
   },
   "outputs": [],
   "source": [
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'NUM_INT', '>'], [\"<NUM_INT>\"])\n",
    "    replace_sublist(x, ['<', 'NUM_FLOAT', '>'], [\"<NUM_FLOAT>\"])\n",
    "    replace_sublist(x, ['<', 'STRING', '>'], [\"<STRING>\"])\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0db5e0b1"
   },
   "outputs": [],
   "source": [
    "def parseSentence(x):\t\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        #print(ord(x[i]))\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"ASCII\":\n",
    "            if inp==\"E\":\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  # nltk.word_tokenize(chrs) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e88de533"
   },
   "outputs": [],
   "source": [
    "def readcode(fname):\n",
    "    with open(fname,encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f1JYt9ELGe5Z"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plotTrainingLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingErrorLineAcc(history):\n",
    "    \n",
    "    #======================accuracy======================\n",
    "    acc = []\n",
    "    #get all sample name\n",
    "    names = []\n",
    "    for sample in range(360):\n",
    "        #get NN name\n",
    "        name = \"LNout\" + str(sample) + \"_categorical_accuracy\"\n",
    "        names.append(name)\n",
    "    #get each epoch avg value\n",
    "    for epoch in range(len(history.history[names[0]])): #get sample length\n",
    "        one_epoch_sum = 0.0\n",
    "        #print(\"epoch loop: \", epoch)\n",
    "        for name in (names):\n",
    "            one_epoch_sum += history.history[name][epoch]\n",
    "        one_epoch_avg = one_epoch_sum / len(names) #get one epoch avg\n",
    "        acc.append(one_epoch_avg) #return to acc\n",
    "    plt.plot(acc)\n",
    "    \n",
    "    #======================validation accuary======================\n",
    "    val_acc = []\n",
    "    #get all sample name\n",
    "    val_names = []\n",
    "    for val_sample in range(360):\n",
    "        #get NN name\n",
    "        val_name = \"val_LNout\" + str(val_sample) +\"_categorical_accuracy\"\n",
    "        val_names.append(val_name)\n",
    "        \n",
    "    #get each epoch avg value\n",
    "    for epoch in range(len(history.history[names[0]])): #get sample length\n",
    "        val_one_epoch_sum = 0.0\n",
    "        for name in (val_names):\n",
    "            val_one_epoch_sum += history.history[name][epoch]\n",
    "        val_one_epoch_avg = val_one_epoch_sum / len(val_names) #get one epoch avg\n",
    "        val_acc.append(val_one_epoch_avg) #return to acc\n",
    "    plt.plot(val_acc)\n",
    "    \n",
    "    plt.ylabel('acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingErrorTypeAcc(history):\n",
    "    plt.plot(history.history['error_feed_forward_output1_binary_accuracy'])\n",
    "    plt.plot(history.history['val_error_feed_forward_output1_binary_accuracy'])\n",
    "    plt.title('model error_feed_forward_output1_binary_accuracy')\n",
    "    plt.ylabel('error_feed_forward_output1_binary_accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def listdir_fullpath(d):\n",
    "    return [f for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ec120d95"
   },
   "outputs": [],
   "source": [
    "#save model for training\n",
    "class TestTranslate(unittest.TestCase):\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<CR>': 5,\n",
    "            '<NUM_INT>': 6,\n",
    "            '<NUM_FLOAT>': 7,\n",
    "            '<STRING>': 8,\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "    \n",
    "    def test_translate(self):\n",
    "        #print(\"i am here: \" )\n",
    "        #source_file=[]\n",
    "        #Set Para\n",
    "        max_javaline_length = 160 #Max number of lines\n",
    "        #set path\n",
    "        Output_Path = \"Trianing\\InputCSV\\Split-500-reduce-binary-accuracy\"\n",
    "        Input_Path = \"Trianing\\InputTxt\\Split-500-reduce-binary-accuracy\"\n",
    "        model_for_training_org_path = \"Model-for-training-org\\Split-500-reduce-binary-accuracy\"\n",
    "        model_for_training_path = \"Model-for-training\\Split-500-reduce-binary-accuracy\"\n",
    "        Trained_model_Path = \"Trained_models\\Split-500-reduce-binary-accuracy\"\n",
    "        #get all txt file in input path\n",
    "        in_path = (glob.glob(Input_Path + \"/**/*.txt\"))\n",
    "        #print(\"in_path: \", in_path)\n",
    "        #cases = listdir_fullpath(Input_Path)\n",
    "        source_max_len = 0\n",
    "        target_max_len = 0\n",
    "        token_num = 0\n",
    "        all_sample_num = 16644 #all sample number\n",
    "        block_num = 16644 #sample num e.g 10000 sample have 10*1000\n",
    "        #Set model para\n",
    "        training_source_max_len = 300 #for replace source_max_len\n",
    "        self.sl = 0\n",
    "        \n",
    "        import math\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version \n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"First loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            #print(\"dirs: \", dirs)\n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])                    \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "                    #print(\"source_tokens length: \", len(source_tokens))\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            #get csv file     \n",
    "            out_path = Output_Path + \"/\" + \"test\" + str(loop)+\".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            \n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err,lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            dd = np.asarray(target_errors)\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]     \n",
    "            \n",
    "            #change source token length\n",
    "            source_tokens2 = []\n",
    "            target_errors2 = []\n",
    "            target_LB2 = []\n",
    "\n",
    "            THRESHOLD_FILE_LEN = training_source_max_len\n",
    "            for i in range(len(source_tokens)):\n",
    "                src = source_tokens[i]\n",
    "                target_error = target_errors[i]\n",
    "                target_LBs = target_LB[i]\n",
    "                if (len(src)<=THRESHOLD_FILE_LEN):# and  len(target)<=THRESHOLD_FILE_LEN):\n",
    "                    source_tokens2.append(src)\n",
    "                    target_errors2.append(target_error)\n",
    "                    target_LB2.append(target_LBs)\n",
    "            source_tokens = source_tokens2\n",
    "            target_errors = target_errors2 #list of intgers, error types\n",
    "            target_LB = target_LB2\n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            \n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            \n",
    "            #output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens] \n",
    "            \n",
    "            self.sl = max(list(map(len, encode_tokens))+[self.sl])\n",
    "            source_max_len = self.sl\n",
    "\n",
    "        #padding here\n",
    "        print(\"source_max_len:\", source_max_len)\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version\n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"Second loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])\n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "\n",
    "            out_path = Output_Path + \"/\" + \"test\"+ str(loop)+ \".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err, lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "            #if len(source_tokens)>max_files: break\n",
    "            dd = np.asarray(target_errors)\n",
    "            #print(\"AAAA: \", dd.shape)\n",
    "            #print(\"aaaa: \" , type(target_errors[0][0]))\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]\n",
    "            #print(\"source_token legth: \" , len(source_tokens))\n",
    "            #print(\"YYYY: \" , type(target_errors[0][0]))\n",
    "            #print(\"ZZZZ: \" , len(target_LB))\n",
    "            #print(\"ZZZZ len(target_LB[0]): \" , len(target_LB[0]))\n",
    "            #print(\"XXXX2: \" , len(source_tokens))\n",
    "            \n",
    "            #change source token length\n",
    "            source_tokens2 = []\n",
    "            target_errors2 = []\n",
    "            target_LB2 = []\n",
    "\n",
    "            THRESHOLD_FILE_LEN = training_source_max_len\n",
    "            for i in range(len(source_tokens)):\n",
    "                src = source_tokens[i]\n",
    "                target_error = target_errors[i]\n",
    "                target_LBs = target_LB[i]\n",
    "                if (len(src)<=THRESHOLD_FILE_LEN):# and  len(target)<=THRESHOLD_FILE_LEN):\n",
    "                    source_tokens2.append(src)\n",
    "                    target_errors2.append(target_error)\n",
    "                    target_LB2.append(target_LBs)\n",
    "            source_tokens = source_tokens2\n",
    "            target_errors = target_errors2 #list of intgers, error types\n",
    "            target_LB = target_LB2\n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "\n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            #print(\"encode_tokens1: \", encode_tokens)\n",
    "            encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_tokens2: \", encode_tokens)\n",
    "            encode_input = [list(map(lambda x: self.source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_input1: \", encode_input)\n",
    "            token_num = len(self.source_token_dict)\n",
    "            #print(\"token num: \", token_num)\n",
    "            #print(token_num)\n",
    "            #print(type(token_num))\n",
    "            #define save path and save dict  \n",
    "            dict_name = \"source_token_dict.pickle\"\n",
    "            #save for org\n",
    "            saveDictionary(self.source_token_dict, model_for_training_org_path + \"/\" + dict_name)\n",
    "            #save for training\n",
    "            saveDictionary(self.source_token_dict, model_for_training_path + \"/\" + dict_name)\n",
    "            #print(\"x.shape\", np.asarray(encode_input).shape)  #x.shape (2,  9)\n",
    "            \n",
    "            #x=[np.array(encode_input * 1)]\n",
    "            #y=[np.array(target_errors * 1),np.array(target_LB * 1)]\n",
    "\n",
    "            #print(\"x.shape\", np.asarray(x).shape)  #x.shape (2, 2048, 9)\n",
    "\n",
    "            ####  Split the data set into train and test_model\n",
    "            x = np.asarray(encode_input)\n",
    "            y = list(zip(np.asarray(target_errors), np.asarray(target_LB)))\n",
    "\n",
    "            #print(\"x.shape: \", x.shape)\n",
    "            #print(\"y length: \", len(y))\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=42)\n",
    "            \n",
    "            y_test = list(zip(*y_test))\n",
    "            y_test[0] = np.asarray(y_test[0])\n",
    "            y_test[1] = np.asarray(y_test[1])\n",
    "            y_test[1] = to_categorical(y_test[1], num_classes = max_javaline_length) \n",
    "            #y_test = list(zip(y_test[0], y_test[1])) \n",
    "            y_test[1] = np.split(y_test[1], indices_or_sections = len(target_LB[0]), axis = 1) \n",
    "            y_test[1] = [np.squeeze(elm, axis = 1) for elm in y_test[1]]           \n",
    "            \n",
    "            #=============================#\n",
    "            x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.1, random_state=42)\n",
    "            print(\"Split x_train shape: \", (x_train).shape)\n",
    "            print(\"Split x_validation shape: \", (x_validation).shape)\n",
    "            \n",
    "            y_train = list(zip(*y_train))  \n",
    "            y_train[0] = np.asarray(y_train[0])\n",
    "            y_train[1] = np.asarray(y_train[1])\n",
    "\n",
    "            y_train[1] = to_categorical(y_train[1], num_classes=max_javaline_length) #make error type vector to binary matrix\n",
    "            #y_train = list(zip(y_train[0], y_train[1]))\n",
    "\n",
    "            #print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:\" , y_train[0].shape)\n",
    "            #print(\"y_train[1].shape\", y_train[1].shape)\n",
    "            #print(\"???????????: y_train.length \", len(y_train))\n",
    "            #print(\"???????????: y_train.[1] type \", type(y_train[1]))\n",
    "            y_train[1] = np.split(y_train[1], indices_or_sections=len(target_LB[0]), axis=1)\n",
    "            y_train[1] = [np.squeeze(elm, axis = 1) for elm in y_train[1]]\n",
    "            #print(\"after change->len(y_train[1].shape)\", len(y_train[1]) )\n",
    "\n",
    "            y_validation = list(zip(*y_validation))\n",
    "            y_validation[0] = np.asarray(y_validation[0])\n",
    "            y_validation[1] = np.asarray(y_validation[1])\n",
    "            y_validation[1] = to_categorical(y_validation[1], num_classes = max_javaline_length) \n",
    "            #y_test = list(zip(y_test[0], y_test[1])) \n",
    "            y_validation[1] = np.split(y_validation[1], indices_or_sections=len(target_LB[0]), axis=1) \n",
    "            y_validation[1] = [np.squeeze(elm, axis = 1) for elm in y_validation[1]]           \n",
    "            \n",
    "            #''' <----save switch, if you need to create npy model for traing open this\n",
    "            #save org model for training \n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            \n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "            \n",
    "            #=========================================\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_validation_\" + str(loop) + \".npy\", x_validation)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_validation[0]_\" + str(loop) + \".npy\", y_validation[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_validation[1]_\" + str(loop) + \".npy\", y_validation[1])\n",
    "            \n",
    "            #transform x_train\n",
    "            \n",
    "            print(\"org x_train shape: \", (x_train).shape)\n",
    "            print(\"org x_test shape: \", (x_test).shape)\n",
    "            \n",
    "            \n",
    "            #save split model for training\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            \n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "            \n",
    "            #=====================================\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_validation_\" + str(loop) + \".npy\", x_validation)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_validation[0]_\" + str(loop) + \".npy\", y_validation[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_validation[1]_\" + str(loop) + \".npy\", y_validation[1])\n",
    "        print(\"Training model save successful...\")    \n",
    "        #'''\n",
    "        \n",
    "        \n",
    "        #start training\n",
    "        import DataGeneratorTrain as DGTrain\n",
    "        import DataGeneratorValidation as DGValidation\n",
    "        import DataBuffer as db\n",
    "        from random import randrange\n",
    "        #Set driver path\n",
    "        #source_max_len = 1200 #set max len  #default : 2889\n",
    "        line_block_num = 360 #lbNum\n",
    "        source_token_dict_name = \"source_token_dict.pickle\"\n",
    "        #load source_token_dict\n",
    "        source_token_dict = loadDictionary(model_for_training_path + \"/\" + source_token_dict_name)\n",
    "        #Set model para    \n",
    "        model = tfr.get_model(max_input_len=(source_max_len),\n",
    "                              max_javaline_length=160,\n",
    "                              errNum=36,\n",
    "                              lbNum=line_block_num, #lbNum=len(target_LB[0]), #160\n",
    "                              token_num=len(source_token_dict),\n",
    "                              embed_dim=256, #32, try 32 or 64\n",
    "                              encoder_num=6, #2 max = 6\n",
    "                              head_num=4,#4\n",
    "                              hidden_dim=128, #128\n",
    "                              dropout_rate=0.05 #0.05\n",
    "                             )\n",
    "        #Set losses\n",
    "        losses = {\"error_feed_forward_output1\": \"binary_crossentropy\"}\n",
    "        #error type weight\n",
    "        lossWeights = {\"error_feed_forward_output1\": 1.0}\n",
    "        metrics = {\"error_feed_forward_output1\": \"binary_accuracy\"}\n",
    "        #metrics = {\"error_feed_forward_output1\": tf.keras.metrics.Accuracy()}\n",
    "        \n",
    "        #error line weight\n",
    "        for i in range(line_block_num):\n",
    "            name = \"LNout\" + str(i)\n",
    "            losses[name] = \"categorical_crossentropy\"\n",
    "            lossWeights[name] = 1 #error_feed_forward_output2[] weight # 100\n",
    "            metrics[name] = tf.keras.metrics.CategoricalAccuracy()\n",
    "        \n",
    "        \n",
    "        #set complie para\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=losses, loss_weights=lossWeights, metrics=metrics)\n",
    "        \n",
    "        #for output\n",
    "        #for x\n",
    "        input_buffer_params = { \n",
    "            \"data_path\": model_for_training_path,\n",
    "            \"data_number\": 5037,\n",
    "            \"data_type\": int,\n",
    "            \"block_size\": 5037 \n",
    "            }\n",
    "        \n",
    "        #for input\n",
    "        #for y\n",
    "        output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path, model_for_training_path],\n",
    "            \"data_number\": [5037, 5037],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [5037, 5037] \n",
    "            }\n",
    "        \n",
    "        #===========================================\n",
    "        #for output\n",
    "        #for x\n",
    "        validation_input_buffer_params = { \n",
    "            \"data_path\": model_for_training_path,\n",
    "            \"data_number\": 560,\n",
    "            \"data_type\": int,\n",
    "            \"block_size\": 560 \n",
    "            }\n",
    "        \n",
    "        #for input\n",
    "        #for y\n",
    "        validation_output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path, model_for_training_path],\n",
    "            \"data_number\": [560, 560],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [560, 560] \n",
    "            }\n",
    "        \n",
    "        \n",
    "        #Create Generators\n",
    "        print(\"Creating training generator...\")\n",
    "        training_generator = DGTrain.DataGeneratorTrain(input_buffer_params,\n",
    "                                                  output_buffer_params,\n",
    "                                                  [list(range(5037)), list(range(5037))] \n",
    "                                                )\n",
    "        #Create Generators\n",
    "        print(\"Creating validation generator...\")\n",
    "        validation_generator = DGValidation.DataGeneratorValidation(validation_input_buffer_params,\n",
    "                                                  validation_output_buffer_params,\n",
    "                                                  [list(range(560)), list(range(560))] \n",
    "                                                )\n",
    "        \n",
    "        \n",
    "        #''' <-----traing switch\n",
    "        #Start training\n",
    "        print(\"Strat training...\")\n",
    "        \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = Trained_model_Path + \"/\" + \"checkpoint_model.h5\",\n",
    "                                                                       monitor = \"val_loss\",\n",
    "                                                                       mode = \"min\",\n",
    "                                                                       save_best_only = True\n",
    "                                                                      )\n",
    "        \n",
    "        history = model.fit_generator(generator = training_generator,\n",
    "                                      epochs = 1750, #100 200 500 3000\n",
    "                                      verbose = 2, #set visibility\n",
    "                                      validation_data = validation_generator,\n",
    "                                      callbacks = [model_checkpoint_callback],\n",
    "                                     )\n",
    "        \n",
    "        \n",
    "        print(\"Model training completed...\")\n",
    "        #save history\n",
    "        print(\"Saving history...\")\n",
    "        saveDictionary(history.history, Trained_model_Path + \"/\" + \"model_history\")\n",
    "        print(\"History saving completed...\")\n",
    "        \n",
    "        #save model\n",
    "        print(\"Saving model...\")\n",
    "        model.save(Trained_model_Path + \"/\" + \"test_model1.h5\")\n",
    "        print(\"Model saving completed...\")\n",
    "        \n",
    "        #print(\"history.history.keys: \", history.history.keys())\n",
    "        #show loss grapgh\n",
    "        plotTrainingLoss(history)\n",
    "        plotTrainingErrorTypeAcc(history)\n",
    "        plotTrainingErrorLineAcc(history)\n",
    "        #'''\n",
    "                \n",
    "    def getsource_max_lan(self):\n",
    "        return self.sl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7aa3c824"
   },
   "outputs": [],
   "source": [
    "def saveDictionary(dt, file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"wb\")\n",
    "        pickle.dump(dt, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "a6ddfb54"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"rb\")\n",
    "        dt = pickle.load(a_file)\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "20063beb"
   },
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "34dcfe7d"
   },
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ba969ac3"
   },
   "outputs": [],
   "source": [
    "def load(model_name):\n",
    "        import sys\n",
    "        #sys.path.append('/content/drive/MyDrive/Final_Edition_include_model')\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_layer_normalization\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_performer\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_position_wise_feed_forward\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\tensorflow_fast_attention\")\n",
    "\n",
    "        from keras_performer import performer\n",
    "        from tensorflow import keras\n",
    "        from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "        from keras_pos_embd import TrigPosEmbedding\n",
    "        from tensorflow_fast_attention.fast_attention import  Attention, SelfAttention\n",
    "        from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "        co = performer.get_custom_objects()\n",
    "\n",
    "        model = keras.models.load_model(\"Trained_models\\Split-500-reduce-softmax\"+ \"/\" + model_name, custom_objects= co)\n",
    "        source_token_dict = loadDictionary(\"Model-for-training\\Split-500-reduce-softmax\\source_token_dict.pickle\")\n",
    "       # t = loadDictionary(target_token_dict, 'target_token_dict.pickle')\n",
    "       # t_inv = loadDictionary(target_token_dict_inv, 'target_token_dict_inv.pickle')\n",
    "        return model, source_token_dict,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First loop:  0\n",
      "source_max_len: 302\n",
      "Second loop:  0\n",
      "Split x_train shape:  (5037, 302)\n",
      "Split x_validation shape:  (560, 302)\n",
      "org x_train shape:  (5037, 302)\n",
      "org x_test shape:  (622, 302)\n",
      "Training model save successful...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Creating training generator...\n",
      "Creating validation generator...\n",
      "Strat training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "39/39 - 380s - loss: 1703.2985 - error_feed_forward_output1_loss: 0.2562 - LNout0_loss: 5.1265 - LNout1_loss: 4.9944 - LNout2_loss: 5.4124 - LNout3_loss: 5.0219 - LNout4_loss: 5.5902 - LNout5_loss: 3.6979 - LNout6_loss: 4.4847 - LNout7_loss: 5.2868 - LNout8_loss: 5.1555 - LNout9_loss: 4.2459 - LNout10_loss: 4.4625 - LNout11_loss: 4.5963 - LNout12_loss: 5.7034 - LNout13_loss: 5.6002 - LNout14_loss: 5.0254 - LNout15_loss: 3.4322 - LNout16_loss: 6.4902 - LNout17_loss: 4.5244 - LNout18_loss: 4.6952 - LNout19_loss: 4.3528 - LNout20_loss: 4.2829 - LNout21_loss: 4.7741 - LNout22_loss: 5.7311 - LNout23_loss: 5.5407 - LNout24_loss: 3.4877 - LNout25_loss: 4.4595 - LNout26_loss: 4.8008 - LNout27_loss: 3.8290 - LNout28_loss: 4.3058 - LNout29_loss: 3.7877 - LNout30_loss: 5.1772 - LNout31_loss: 4.0677 - LNout32_loss: 5.5369 - LNout33_loss: 3.4857 - LNout34_loss: 4.7781 - LNout35_loss: 6.4025 - LNout36_loss: 5.3723 - LNout37_loss: 4.6402 - LNout38_loss: 5.8743 - LNout39_loss: 5.0460 - LNout40_loss: 3.8866 - LNout41_loss: 4.6959 - LNout42_loss: 5.4200 - LNout43_loss: 4.0437 - LNout44_loss: 4.2058 - LNout45_loss: 5.5902 - LNout46_loss: 4.4495 - LNout47_loss: 6.0127 - LNout48_loss: 5.0066 - LNout49_loss: 3.7470 - LNout50_loss: 4.9898 - LNout51_loss: 4.7791 - LNout52_loss: 4.5357 - LNout53_loss: 4.5623 - LNout54_loss: 4.5467 - LNout55_loss: 5.6391 - LNout56_loss: 4.2333 - LNout57_loss: 4.6577 - LNout58_loss: 5.4502 - LNout59_loss: 4.8139 - LNout60_loss: 4.3378 - LNout61_loss: 5.0246 - LNout62_loss: 5.2234 - LNout63_loss: 4.0547 - LNout64_loss: 5.9897 - LNout65_loss: 4.9928 - LNout66_loss: 6.8137 - LNout67_loss: 5.1019 - LNout68_loss: 4.2269 - LNout69_loss: 3.0807 - LNout70_loss: 4.0426 - LNout71_loss: 5.0765 - LNout72_loss: 5.7363 - LNout73_loss: 4.1127 - LNout74_loss: 5.0914 - LNout75_loss: 5.6433 - LNout76_loss: 6.0331 - LNout77_loss: 3.8233 - LNout78_loss: 4.9839 - LNout79_loss: 4.5551 - LNout80_loss: 3.6989 - LNout81_loss: 5.3284 - LNout82_loss: 4.4027 - LNout83_loss: 4.4265 - LNout84_loss: 4.1084 - LNout85_loss: 5.8095 - LNout86_loss: 4.5385 - LNout87_loss: 3.7125 - LNout88_loss: 3.3935 - LNout89_loss: 4.7677 - LNout90_loss: 3.4596 - LNout91_loss: 4.3801 - LNout92_loss: 3.7728 - LNout93_loss: 4.5213 - LNout94_loss: 4.0377 - LNout95_loss: 3.2228 - LNout96_loss: 5.3867 - LNout97_loss: 4.5465 - LNout98_loss: 4.4897 - LNout99_loss: 4.6348 - LNout100_loss: 4.7773 - LNout101_loss: 4.1200 - LNout102_loss: 5.6662 - LNout103_loss: 5.7077 - LNout104_loss: 5.8020 - LNout105_loss: 5.3895 - LNout106_loss: 4.9162 - LNout107_loss: 4.2414 - LNout108_loss: 3.8261 - LNout109_loss: 3.3276 - LNout110_loss: 4.7928 - LNout111_loss: 4.3434 - LNout112_loss: 4.3666 - LNout113_loss: 4.2671 - LNout114_loss: 4.7910 - LNout115_loss: 4.1318 - LNout116_loss: 4.6810 - LNout117_loss: 3.4749 - LNout118_loss: 4.7106 - LNout119_loss: 3.9904 - LNout120_loss: 3.8818 - LNout121_loss: 4.8108 - LNout122_loss: 4.4522 - LNout123_loss: 5.2953 - LNout124_loss: 5.7564 - LNout125_loss: 4.8386 - LNout126_loss: 4.3055 - LNout127_loss: 3.7945 - LNout128_loss: 3.8338 - LNout129_loss: 4.5783 - LNout130_loss: 6.0251 - LNout131_loss: 5.8763 - LNout132_loss: 3.6998 - LNout133_loss: 4.3444 - LNout134_loss: 4.9362 - LNout135_loss: 4.9042 - LNout136_loss: 4.8779 - LNout137_loss: 3.8052 - LNout138_loss: 5.5918 - LNout139_loss: 4.0245 - LNout140_loss: 3.6740 - LNout141_loss: 5.1157 - LNout142_loss: 4.5994 - LNout143_loss: 5.2639 - LNout144_loss: 4.0448 - LNout145_loss: 4.9831 - LNout146_loss: 5.3898 - LNout147_loss: 5.0575 - LNout148_loss: 3.9365 - LNout149_loss: 5.1994 - LNout150_loss: 4.1093 - LNout151_loss: 5.6913 - LNout152_loss: 5.4829 - LNout153_loss: 4.6937 - LNout154_loss: 3.8836 - LNout155_loss: 4.5649 - LNout156_loss: 5.5667 - LNout157_loss: 3.7827 - LNout158_loss: 5.9783 - LNout159_loss: 3.4022 - LNout160_loss: 3.9724 - LNout161_loss: 6.1363 - LNout162_loss: 5.8248 - LNout163_loss: 5.6183 - LNout164_loss: 5.1623 - LNout165_loss: 4.7090 - LNout166_loss: 4.7542 - LNout167_loss: 4.7130 - LNout168_loss: 5.7285 - LNout169_loss: 4.4303 - LNout170_loss: 4.4566 - LNout171_loss: 4.7905 - LNout172_loss: 6.1117 - LNout173_loss: 3.5760 - LNout174_loss: 5.0519 - LNout175_loss: 5.2712 - LNout176_loss: 3.9379 - LNout177_loss: 3.9084 - LNout178_loss: 4.6522 - LNout179_loss: 4.4150 - LNout180_loss: 5.3493 - LNout181_loss: 5.0547 - LNout182_loss: 5.1629 - LNout183_loss: 5.8373 - LNout184_loss: 3.9994 - LNout185_loss: 4.7463 - LNout186_loss: 5.0224 - LNout187_loss: 5.0230 - LNout188_loss: 4.6374 - LNout189_loss: 4.3108 - LNout190_loss: 3.5086 - LNout191_loss: 4.1843 - LNout192_loss: 4.0766 - LNout193_loss: 5.2421 - LNout194_loss: 5.4322 - LNout195_loss: 6.7372 - LNout196_loss: 5.7781 - LNout197_loss: 4.9745 - LNout198_loss: 4.5258 - LNout199_loss: 3.9168 - LNout200_loss: 5.4267 - LNout201_loss: 5.9595 - LNout202_loss: 5.0331 - LNout203_loss: 5.0154 - LNout204_loss: 4.7821 - LNout205_loss: 3.6220 - LNout206_loss: 5.3238 - LNout207_loss: 5.3065 - LNout208_loss: 4.8076 - LNout209_loss: 5.3093 - LNout210_loss: 5.5566 - LNout211_loss: 4.6798 - LNout212_loss: 5.7764 - LNout213_loss: 4.6922 - LNout214_loss: 5.0446 - LNout215_loss: 4.6391 - LNout216_loss: 4.5545 - LNout217_loss: 6.9404 - LNout218_loss: 5.2070 - LNout219_loss: 5.1019 - LNout220_loss: 3.8819 - LNout221_loss: 5.5306 - LNout222_loss: 3.4912 - LNout223_loss: 4.1760 - LNout224_loss: 5.4204 - LNout225_loss: 3.1100 - LNout226_loss: 5.1058 - LNout227_loss: 5.4676 - LNout228_loss: 4.5382 - LNout229_loss: 3.5430 - LNout230_loss: 4.5056 - LNout231_loss: 4.3180 - LNout232_loss: 4.4976 - LNout233_loss: 4.3595 - LNout234_loss: 5.5215 - LNout235_loss: 5.9820 - LNout236_loss: 3.8745 - LNout237_loss: 5.5315 - LNout238_loss: 3.1362 - LNout239_loss: 5.3823 - LNout240_loss: 3.8770 - LNout241_loss: 4.3515 - LNout242_loss: 5.5723 - LNout243_loss: 4.7034 - LNout244_loss: 4.0758 - LNout245_loss: 4.6630 - LNout246_loss: 5.3609 - LNout247_loss: 6.7307 - LNout248_loss: 4.8879 - LNout249_loss: 5.8301 - LNout250_loss: 5.9499 - LNout251_loss: 4.4764 - LNout252_loss: 3.6728 - LNout253_loss: 4.6799 - LNout254_loss: 4.7723 - LNout255_loss: 5.9154 - LNout256_loss: 4.4648 - LNout257_loss: 4.7154 - LNout258_loss: 4.1015 - LNout259_loss: 4.4728 - LNout260_loss: 3.7073 - LNout261_loss: 4.6992 - LNout262_loss: 4.7017 - LNout263_loss: 3.3489 - LNout264_loss: 4.0959 - LNout265_loss: 5.5549 - LNout266_loss: 4.3570 - LNout267_loss: 3.7618 - LNout268_loss: 4.2947 - LNout269_loss: 4.5296 - LNout270_loss: 4.1725 - LNout271_loss: 5.9402 - LNout272_loss: 4.7154 - LNout273_loss: 5.0038 - LNout274_loss: 4.2771 - LNout275_loss: 3.6660 - LNout276_loss: 5.1411 - LNout277_loss: 4.2377 - LNout278_loss: 4.1339 - LNout279_loss: 4.3768 - LNout280_loss: 4.5117 - LNout281_loss: 5.0155 - LNout282_loss: 4.8118 - LNout283_loss: 4.8658 - LNout284_loss: 3.1115 - LNout285_loss: 5.6934 - LNout286_loss: 4.3440 - LNout287_loss: 3.7999 - LNout288_loss: 5.8131 - LNout289_loss: 5.8080 - LNout290_loss: 5.5677 - LNout291_loss: 3.7470 - LNout292_loss: 5.4199 - LNout293_loss: 4.1367 - LNout294_loss: 5.8398 - LNout295_loss: 4.4010 - LNout296_loss: 4.2791 - LNout297_loss: 5.2003 - LNout298_loss: 3.1467 - LNout299_loss: 4.9453 - LNout300_loss: 3.8699 - LNout301_loss: 4.8012 - LNout302_loss: 4.9353 - LNout303_loss: 5.0918 - LNout304_loss: 5.1767 - LNout305_loss: 5.1017 - LNout306_loss: 4.3957 - LNout307_loss: 4.0263 - LNout308_loss: 4.3562 - LNout309_loss: 5.2577 - LNout310_loss: 6.2404 - LNout311_loss: 5.1482 - LNout312_loss: 3.9883 - LNout313_loss: 2.0728 - LNout314_loss: 5.2823 - LNout315_loss: 3.5384 - LNout316_loss: 4.2569 - LNout317_loss: 5.0698 - LNout318_loss: 4.9607 - LNout319_loss: 3.2825 - LNout320_loss: 3.8617 - LNout321_loss: 3.9916 - LNout322_loss: 4.9425 - LNout323_loss: 5.1195 - LNout324_loss: 3.9011 - LNout325_loss: 5.9266 - LNout326_loss: 5.2179 - LNout327_loss: 5.5710 - LNout328_loss: 4.8568 - LNout329_loss: 5.4958 - LNout330_loss: 3.3976 - LNout331_loss: 4.4999 - LNout332_loss: 3.9260 - LNout333_loss: 4.8151 - LNout334_loss: 4.6726 - LNout335_loss: 5.1987 - LNout336_loss: 4.2889 - LNout337_loss: 4.2770 - LNout338_loss: 4.2973 - LNout339_loss: 3.2966 - LNout340_loss: 4.4911 - LNout341_loss: 6.6112 - LNout342_loss: 4.3155 - LNout343_loss: 6.2739 - LNout344_loss: 5.8087 - LNout345_loss: 3.8250 - LNout346_loss: 3.7704 - LNout347_loss: 4.7637 - LNout348_loss: 4.7857 - LNout349_loss: 5.7792 - LNout350_loss: 6.8115 - LNout351_loss: 4.4278 - LNout352_loss: 4.7255 - LNout353_loss: 4.9849 - LNout354_loss: 3.9344 - LNout355_loss: 4.8885 - LNout356_loss: 5.1925 - LNout357_loss: 4.9742 - LNout358_loss: 4.4908 - LNout359_loss: 3.9371 - error_feed_forward_output1_binary_accuracy: 0.9295 - LNout0_categorical_accuracy: 0.0072 - LNout1_categorical_accuracy: 0.0034 - LNout2_categorical_accuracy: 0.0012 - LNout3_categorical_accuracy: 0.0032 - LNout4_categorical_accuracy: 2.0032e-04 - LNout5_categorical_accuracy: 0.4603 - LNout6_categorical_accuracy: 6.0096e-04 - LNout7_categorical_accuracy: 2.0032e-04 - LNout8_categorical_accuracy: 2.0032e-04 - LNout9_categorical_accuracy: 4.0064e-04 - LNout10_categorical_accuracy: 4.0064e-04 - LNout11_categorical_accuracy: 2.0032e-04 - LNout12_categorical_accuracy: 2.0032e-04 - LNout13_categorical_accuracy: 2.0032e-04 - LNout14_categorical_accuracy: 2.0032e-04 - LNout15_categorical_accuracy: 0.4918 - LNout16_categorical_accuracy: 2.0032e-04 - LNout17_categorical_accuracy: 2.0032e-04 - LNout18_categorical_accuracy: 2.0032e-04 - LNout19_categorical_accuracy: 0.0174 - LNout20_categorical_accuracy: 2.0032e-04 - LNout21_categorical_accuracy: 2.0032e-04 - LNout22_categorical_accuracy: 2.0032e-04 - LNout23_categorical_accuracy: 2.0032e-04 - LNout24_categorical_accuracy: 0.1196 - LNout25_categorical_accuracy: 2.0032e-04 - LNout26_categorical_accuracy: 2.0032e-04 - LNout27_categorical_accuracy: 0.2286 - LNout28_categorical_accuracy: 0.0459 - LNout29_categorical_accuracy: 0.1030 - LNout30_categorical_accuracy: 2.0032e-04 - LNout31_categorical_accuracy: 0.2933 - LNout32_categorical_accuracy: 2.0032e-04 - LNout33_categorical_accuracy: 0.6573 - LNout34_categorical_accuracy: 2.0032e-04 - LNout35_categorical_accuracy: 2.0032e-04 - LNout36_categorical_accuracy: 2.0032e-04 - LNout37_categorical_accuracy: 2.0032e-04 - LNout38_categorical_accuracy: 2.0032e-04 - LNout39_categorical_accuracy: 2.0032e-04 - LNout40_categorical_accuracy: 0.0739 - LNout41_categorical_accuracy: 2.0032e-04 - LNout42_categorical_accuracy: 2.0032e-04 - LNout43_categorical_accuracy: 0.1538 - LNout44_categorical_accuracy: 0.0146 - LNout45_categorical_accuracy: 2.0032e-04 - LNout46_categorical_accuracy: 2.0032e-04 - LNout47_categorical_accuracy: 2.0032e-04 - LNout48_categorical_accuracy: 2.0032e-04 - LNout49_categorical_accuracy: 0.0082 - LNout50_categorical_accuracy: 2.0032e-04 - LNout51_categorical_accuracy: 2.0032e-04 - LNout52_categorical_accuracy: 2.0032e-04 - LNout53_categorical_accuracy: 2.0032e-04 - LNout54_categorical_accuracy: 2.0032e-04 - LNout55_categorical_accuracy: 2.0032e-04 - LNout56_categorical_accuracy: 2.0032e-04 - LNout57_categorical_accuracy: 2.0032e-04 - LNout58_categorical_accuracy: 2.0032e-04 - LNout59_categorical_accuracy: 2.0032e-04 - LNout60_categorical_accuracy: 0.0190 - LNout61_categorical_accuracy: 2.0032e-04 - LNout62_categorical_accuracy: 0.0012 - LNout63_categorical_accuracy: 0.2063 - LNout64_categorical_accuracy: 2.0032e-04 - LNout65_categorical_accuracy: 0.0028 - LNout66_categorical_accuracy: 2.0032e-04 - LNout67_categorical_accuracy: 0.0032 - LNout68_categorical_accuracy: 0.0012 - LNout69_categorical_accuracy: 0.7849 - LNout70_categorical_accuracy: 6.0096e-04 - LNout71_categorical_accuracy: 2.0032e-04 - LNout72_categorical_accuracy: 2.0032e-04 - LNout73_categorical_accuracy: 0.1238 - LNout74_categorical_accuracy: 4.0064e-04 - LNout75_categorical_accuracy: 4.0064e-04 - LNout76_categorical_accuracy: 2.0032e-04 - LNout77_categorical_accuracy: 0.1354 - LNout78_categorical_accuracy: 2.0032e-04 - LNout79_categorical_accuracy: 2.0032e-04 - LNout80_categorical_accuracy: 0.1807 - LNout81_categorical_accuracy: 2.0032e-04 - LNout82_categorical_accuracy: 2.0032e-04 - LNout83_categorical_accuracy: 2.0032e-04 - LNout84_categorical_accuracy: 2.0032e-04 - LNout85_categorical_accuracy: 2.0032e-04 - LNout86_categorical_accuracy: 2.0032e-04 - LNout87_categorical_accuracy: 0.2909 - LNout88_categorical_accuracy: 0.5034 - LNout89_categorical_accuracy: 2.0032e-04 - LNout90_categorical_accuracy: 0.0996 - LNout91_categorical_accuracy: 2.0032e-04 - LNout92_categorical_accuracy: 4.0064e-04 - LNout93_categorical_accuracy: 2.0032e-04 - LNout94_categorical_accuracy: 2.0032e-04 - LNout95_categorical_accuracy: 0.7226 - LNout96_categorical_accuracy: 2.0032e-04 - LNout97_categorical_accuracy: 2.0032e-04 - LNout98_categorical_accuracy: 2.0032e-04 - LNout99_categorical_accuracy: 2.0032e-04 - LNout100_categorical_accuracy: 2.0032e-04 - LNout101_categorical_accuracy: 0.0012 - LNout102_categorical_accuracy: 2.0032e-04 - LNout103_categorical_accuracy: 2.0032e-04 - LNout104_categorical_accuracy: 2.0032e-04 - LNout105_categorical_accuracy: 2.0032e-04 - LNout106_categorical_accuracy: 2.0032e-04 - LNout107_categorical_accuracy: 2.0032e-04 - LNout108_categorical_accuracy: 0.2370 - LNout109_categorical_accuracy: 0.4247 - LNout110_categorical_accuracy: 2.0032e-04 - LNout111_categorical_accuracy: 2.0032e-04 - LNout112_categorical_accuracy: 2.0032e-04 - LNout113_categorical_accuracy: 0.0010 - LNout114_categorical_accuracy: 2.0032e-04 - LNout115_categorical_accuracy: 2.0032e-04 - LNout116_categorical_accuracy: 2.0032e-04 - LNout117_categorical_accuracy: 0.1324 - LNout118_categorical_accuracy: 2.0032e-04 - LNout119_categorical_accuracy: 2.0032e-04 - LNout120_categorical_accuracy: 0.3397 - LNout121_categorical_accuracy: 2.0032e-04 - LNout122_categorical_accuracy: 2.0032e-04 - LNout123_categorical_accuracy: 4.0064e-04 - LNout124_categorical_accuracy: 0.0000e+00 - LNout125_categorical_accuracy: 2.0032e-04 - LNout126_categorical_accuracy: 2.0032e-04 - LNout127_categorical_accuracy: 0.0252 - LNout128_categorical_accuracy: 2.0032e-04 - LNout129_categorical_accuracy: 8.0128e-04 - LNout130_categorical_accuracy: 2.0032e-04 - LNout131_categorical_accuracy: 2.0032e-04 - LNout132_categorical_accuracy: 0.2794 - LNout133_categorical_accuracy: 0.0048 - LNout134_categorical_accuracy: 2.0032e-04 - LNout135_categorical_accuracy: 2.0032e-04 - LNout136_categorical_accuracy: 2.0032e-04 - LNout137_categorical_accuracy: 0.0068 - LNout138_categorical_accuracy: 0.0072 - LNout139_categorical_accuracy: 2.0032e-04 - LNout140_categorical_accuracy: 0.4179 - LNout141_categorical_accuracy: 2.0032e-04 - LNout142_categorical_accuracy: 2.0032e-04 - LNout143_categorical_accuracy: 2.0032e-04 - LNout144_categorical_accuracy: 0.0533 - LNout145_categorical_accuracy: 2.0032e-04 - LNout146_categorical_accuracy: 2.0032e-04 - LNout147_categorical_accuracy: 2.0032e-04 - LNout148_categorical_accuracy: 0.1923 - LNout149_categorical_accuracy: 2.0032e-04 - LNout150_categorical_accuracy: 2.0032e-04 - LNout151_categorical_accuracy: 2.0032e-04 - LNout152_categorical_accuracy: 2.0032e-04 - LNout153_categorical_accuracy: 2.0032e-04 - LNout154_categorical_accuracy: 0.0441 - LNout155_categorical_accuracy: 2.0032e-04 - LNout156_categorical_accuracy: 2.0032e-04 - LNout157_categorical_accuracy: 0.0709 - LNout158_categorical_accuracy: 2.0032e-04 - LNout159_categorical_accuracy: 0.6366 - LNout160_categorical_accuracy: 0.2694 - LNout161_categorical_accuracy: 2.0032e-04 - LNout162_categorical_accuracy: 0.0000e+00 - LNout163_categorical_accuracy: 2.0032e-04 - LNout164_categorical_accuracy: 2.0032e-04 - LNout165_categorical_accuracy: 2.0032e-04 - LNout166_categorical_accuracy: 2.0032e-04 - LNout167_categorical_accuracy: 2.0032e-04 - LNout168_categorical_accuracy: 2.0032e-04 - LNout169_categorical_accuracy: 2.0032e-04 - LNout170_categorical_accuracy: 0.0010 - LNout171_categorical_accuracy: 4.0064e-04 - LNout172_categorical_accuracy: 2.0032e-04 - LNout173_categorical_accuracy: 0.4163 - LNout174_categorical_accuracy: 2.0032e-04 - LNout175_categorical_accuracy: 2.0032e-04 - LNout176_categorical_accuracy: 0.2129 - LNout177_categorical_accuracy: 0.0096 - LNout178_categorical_accuracy: 2.0032e-04 - LNout179_categorical_accuracy: 2.0032e-04 - LNout180_categorical_accuracy: 0.0000e+00 - LNout181_categorical_accuracy: 0.0030 - LNout182_categorical_accuracy: 0.0000e+00 - LNout183_categorical_accuracy: 0.0016 - LNout184_categorical_accuracy: 0.0024 - LNout185_categorical_accuracy: 2.0032e-04 - LNout186_categorical_accuracy: 2.0032e-04 - LNout187_categorical_accuracy: 2.0032e-04 - LNout188_categorical_accuracy: 2.0032e-04 - LNout189_categorical_accuracy: 4.0064e-04 - LNout190_categorical_accuracy: 0.3167 - LNout191_categorical_accuracy: 2.0032e-04 - LNout192_categorical_accuracy: 2.0032e-04 - LNout193_categorical_accuracy: 2.0032e-04 - LNout194_categorical_accuracy: 2.0032e-04 - LNout195_categorical_accuracy: 2.0032e-04 - LNout196_categorical_accuracy: 2.0032e-04 - LNout197_categorical_accuracy: 2.0032e-04 - LNout198_categorical_accuracy: 2.0032e-04 - LNout199_categorical_accuracy: 0.2752 - LNout200_categorical_accuracy: 2.0032e-04 - LNout201_categorical_accuracy: 2.0032e-04 - LNout202_categorical_accuracy: 2.0032e-04 - LNout203_categorical_accuracy: 2.0032e-04 - LNout204_categorical_accuracy: 2.0032e-04 - LNout205_categorical_accuracy: 0.1554 - LNout206_categorical_accuracy: 2.0032e-04 - LNout207_categorical_accuracy: 2.0032e-04 - LNout208_categorical_accuracy: 2.0032e-04 - LNout209_categorical_accuracy: 2.0032e-04 - LNout210_categorical_accuracy: 2.0032e-04 - LNout211_categorical_accuracy: 2.0032e-04 - LNout212_categorical_accuracy: 2.0032e-04 - LNout213_categorical_accuracy: 2.0032e-04 - LNout214_categorical_accuracy: 2.0032e-04 - LNout215_categorical_accuracy: 2.0032e-04 - LNout216_categorical_accuracy: 0.0010 - LNout217_categorical_accuracy: 0.0000e+00 - LNout218_categorical_accuracy: 2.0032e-04 - LNout219_categorical_accuracy: 2.0032e-04 - LNout220_categorical_accuracy: 0.0030 - LNout221_categorical_accuracy: 2.0032e-04 - LNout222_categorical_accuracy: 0.2354 - LNout223_categorical_accuracy: 2.0032e-04 - LNout224_categorical_accuracy: 2.0032e-04 - LNout225_categorical_accuracy: 0.6466 - LNout226_categorical_accuracy: 2.0032e-04 - LNout227_categorical_accuracy: 2.0032e-04 - LNout228_categorical_accuracy: 2.0032e-04 - LNout229_categorical_accuracy: 0.0968 - LNout230_categorical_accuracy: 2.0032e-04 - LNout231_categorical_accuracy: 2.0032e-04 - LNout232_categorical_accuracy: 2.0032e-04 - LNout233_categorical_accuracy: 2.0032e-04 - LNout234_categorical_accuracy: 2.0032e-04 - LNout235_categorical_accuracy: 2.0032e-04 - LNout236_categorical_accuracy: 0.1270 - LNout237_categorical_accuracy: 2.0032e-04 - LNout238_categorical_accuracy: 0.6298 - LNout239_categorical_accuracy: 2.0032e-04 - LNout240_categorical_accuracy: 0.0010 - LNout241_categorical_accuracy: 6.0096e-04 - LNout242_categorical_accuracy: 2.0032e-04 - LNout243_categorical_accuracy: 4.0064e-04 - LNout244_categorical_accuracy: 0.2246 - LNout245_categorical_accuracy: 2.0032e-04 - LNout246_categorical_accuracy: 2.0032e-04 - LNout247_categorical_accuracy: 2.0032e-04 - LNout248_categorical_accuracy: 2.0032e-04 - LNout249_categorical_accuracy: 2.0032e-04 - LNout250_categorical_accuracy: 2.0032e-04 - LNout251_categorical_accuracy: 2.0032e-04 - LNout252_categorical_accuracy: 0.4171 - LNout253_categorical_accuracy: 2.0032e-04 - LNout254_categorical_accuracy: 2.0032e-04 - LNout255_categorical_accuracy: 2.0032e-04 - LNout256_categorical_accuracy: 0.0014 - LNout257_categorical_accuracy: 2.0032e-04 - LNout258_categorical_accuracy: 2.0032e-04 - LNout259_categorical_accuracy: 2.0032e-04 - LNout260_categorical_accuracy: 0.1631 - LNout261_categorical_accuracy: 2.0032e-04 - LNout262_categorical_accuracy: 2.0032e-04 - LNout263_categorical_accuracy: 0.0815 - LNout264_categorical_accuracy: 2.0032e-04 - LNout265_categorical_accuracy: 2.0032e-04 - LNout266_categorical_accuracy: 2.0032e-04 - LNout267_categorical_accuracy: 0.2570 - LNout268_categorical_accuracy: 2.0032e-04 - LNout269_categorical_accuracy: 2.0032e-04 - LNout270_categorical_accuracy: 0.1418 - LNout271_categorical_accuracy: 2.0032e-04 - LNout272_categorical_accuracy: 2.0032e-04 - LNout273_categorical_accuracy: 2.0032e-04 - LNout274_categorical_accuracy: 0.0974 - LNout275_categorical_accuracy: 0.0196 - LNout276_categorical_accuracy: 2.0032e-04 - LNout277_categorical_accuracy: 2.0032e-04 - LNout278_categorical_accuracy: 2.0032e-04 - LNout279_categorical_accuracy: 2.0032e-04 - LNout280_categorical_accuracy: 2.0032e-04 - LNout281_categorical_accuracy: 2.0032e-04 - LNout282_categorical_accuracy: 2.0032e-04 - LNout283_categorical_accuracy: 2.0032e-04 - LNout284_categorical_accuracy: 0.7837 - LNout285_categorical_accuracy: 2.0032e-04 - LNout286_categorical_accuracy: 2.0032e-04 - LNout287_categorical_accuracy: 0.4469 - LNout288_categorical_accuracy: 2.0032e-04 - LNout289_categorical_accuracy: 2.0032e-04 - LNout290_categorical_accuracy: 2.0032e-04 - LNout291_categorical_accuracy: 0.3728 - LNout292_categorical_accuracy: 2.0032e-04 - LNout293_categorical_accuracy: 2.0032e-04 - LNout294_categorical_accuracy: 2.0032e-04 - LNout295_categorical_accuracy: 2.0032e-04 - LNout296_categorical_accuracy: 2.0032e-04 - LNout297_categorical_accuracy: 2.0032e-04 - LNout298_categorical_accuracy: 0.3666 - LNout299_categorical_accuracy: 2.0032e-04 - LNout300_categorical_accuracy: 2.0032e-04 - LNout301_categorical_accuracy: 4.0064e-04 - LNout302_categorical_accuracy: 2.0032e-04 - LNout303_categorical_accuracy: 2.0032e-04 - LNout304_categorical_accuracy: 2.0032e-04 - LNout305_categorical_accuracy: 2.0032e-04 - LNout306_categorical_accuracy: 2.0032e-04 - LNout307_categorical_accuracy: 0.0070 - LNout308_categorical_accuracy: 2.0032e-04 - LNout309_categorical_accuracy: 2.0032e-04 - LNout310_categorical_accuracy: 0.0000e+00 - LNout311_categorical_accuracy: 2.0032e-04 - LNout312_categorical_accuracy: 4.0064e-04 - LNout313_categorical_accuracy: 0.9231 - LNout314_categorical_accuracy: 2.0032e-04 - LNout315_categorical_accuracy: 0.2117 - LNout316_categorical_accuracy: 0.0355 - LNout317_categorical_accuracy: 0.0012 - LNout318_categorical_accuracy: 2.0032e-04 - LNout319_categorical_accuracy: 0.3552 - LNout320_categorical_accuracy: 2.0032e-04 - LNout321_categorical_accuracy: 2.0032e-04 - LNout322_categorical_accuracy: 2.0032e-04 - LNout323_categorical_accuracy: 2.0032e-04 - LNout324_categorical_accuracy: 2.0032e-04 - LNout325_categorical_accuracy: 2.0032e-04 - LNout326_categorical_accuracy: 2.0032e-04 - LNout327_categorical_accuracy: 2.0032e-04 - LNout328_categorical_accuracy: 2.0032e-04 - LNout329_categorical_accuracy: 2.0032e-04 - LNout330_categorical_accuracy: 0.2035 - LNout331_categorical_accuracy: 2.0032e-04 - LNout332_categorical_accuracy: 0.0144 - LNout333_categorical_accuracy: 2.0032e-04 - LNout334_categorical_accuracy: 2.0032e-04 - LNout335_categorical_accuracy: 2.0032e-04 - LNout336_categorical_accuracy: 2.0032e-04 - LNout337_categorical_accuracy: 2.0032e-04 - LNout338_categorical_accuracy: 0.0198 - LNout339_categorical_accuracy: 0.5863 - LNout340_categorical_accuracy: 2.0032e-04 - LNout341_categorical_accuracy: 2.0032e-04 - LNout342_categorical_accuracy: 2.0032e-04 - LNout343_categorical_accuracy: 2.0032e-04 - LNout344_categorical_accuracy: 2.0032e-04 - LNout345_categorical_accuracy: 0.1989 - LNout346_categorical_accuracy: 0.2049 - LNout347_categorical_accuracy: 0.0010 - LNout348_categorical_accuracy: 2.0032e-04 - LNout349_categorical_accuracy: 2.0032e-04 - LNout350_categorical_accuracy: 2.0032e-04 - LNout351_categorical_accuracy: 2.0032e-04 - LNout352_categorical_accuracy: 2.0032e-04 - LNout353_categorical_accuracy: 0.0000e+00 - LNout354_categorical_accuracy: 6.0096e-04 - LNout355_categorical_accuracy: 0.0158 - LNout356_categorical_accuracy: 2.0032e-04 - LNout357_categorical_accuracy: 2.0032e-04 - LNout358_categorical_accuracy: 2.0032e-04 - LNout359_categorical_accuracy: 0.1358 - val_loss: 1372.5872 - val_error_feed_forward_output1_loss: 0.1782 - val_LNout0_loss: 4.8584 - val_LNout1_loss: 5.1617 - val_LNout2_loss: 5.3791 - val_LNout3_loss: 4.8664 - val_LNout4_loss: 5.4883 - val_LNout5_loss: 1.4064 - val_LNout6_loss: 3.3475 - val_LNout7_loss: 5.0535 - val_LNout8_loss: 4.8396 - val_LNout9_loss: 2.2544 - val_LNout10_loss: 2.9779 - val_LNout11_loss: 3.7027 - val_LNout12_loss: 6.0143 - val_LNout13_loss: 6.0747 - val_LNout14_loss: 4.3706 - val_LNout15_loss: 1.1039 - val_LNout16_loss: 7.5636 - val_LNout17_loss: 3.7069 - val_LNout18_loss: 3.6611 - val_LNout19_loss: 3.4748 - val_LNout20_loss: 2.7028 - val_LNout21_loss: 3.4488 - val_LNout22_loss: 5.8391 - val_LNout23_loss: 5.5198 - val_LNout24_loss: 1.4855 - val_LNout25_loss: 2.9660 - val_LNout26_loss: 3.5259 - val_LNout27_loss: 1.8398 - val_LNout28_loss: 2.3525 - val_LNout29_loss: 1.7113 - val_LNout30_loss: 5.1585 - val_LNout31_loss: 2.2090 - val_LNout32_loss: 5.4929 - val_LNout33_loss: 1.2293 - val_LNout34_loss: 3.5673 - val_LNout35_loss: 7.6370 - val_LNout36_loss: 5.2022 - val_LNout37_loss: 3.6144 - val_LNout38_loss: 6.7308 - val_LNout39_loss: 3.9698 - val_LNout40_loss: 1.7395 - val_LNout41_loss: 3.9089 - val_LNout42_loss: 5.1583 - val_LNout43_loss: 2.0819 - val_LNout44_loss: 2.3657 - val_LNout45_loss: 5.5891 - val_LNout46_loss: 3.2424 - val_LNout47_loss: 6.5308 - val_LNout48_loss: 4.3613 - val_LNout49_loss: 1.8838 - val_LNout50_loss: 4.5089 - val_LNout51_loss: 3.9315 - val_LNout52_loss: 3.3664 - val_LNout53_loss: 3.6494 - val_LNout54_loss: 3.6403 - val_LNout55_loss: 5.9047 - val_LNout56_loss: 2.8822 - val_LNout57_loss: 3.5779 - val_LNout58_loss: 5.4509 - val_LNout59_loss: 4.0721 - val_LNout60_loss: 3.0643 - val_LNout61_loss: 4.4665 - val_LNout62_loss: 5.0551 - val_LNout63_loss: 2.2456 - val_LNout64_loss: 6.8702 - val_LNout65_loss: 4.2344 - val_LNout66_loss: 8.6791 - val_LNout67_loss: 4.7616 - val_LNout68_loss: 2.9160 - val_LNout69_loss: 0.8489 - val_LNout70_loss: 2.3273 - val_LNout71_loss: 4.2814 - val_LNout72_loss: 6.2042 - val_LNout73_loss: 2.3723 - val_LNout74_loss: 4.4198 - val_LNout75_loss: 5.6931 - val_LNout76_loss: 6.8631 - val_LNout77_loss: 1.7621 - val_LNout78_loss: 4.1370 - val_LNout79_loss: 3.5773 - val_LNout80_loss: 1.6485 - val_LNout81_loss: 5.6633 - val_LNout82_loss: 3.0131 - val_LNout83_loss: 3.1065 - val_LNout84_loss: 2.3907 - val_LNout85_loss: 5.9962 - val_LNout86_loss: 2.8496 - val_LNout87_loss: 1.5510 - val_LNout88_loss: 1.0057 - val_LNout89_loss: 3.9870 - val_LNout90_loss: 1.1530 - val_LNout91_loss: 3.2025 - val_LNout92_loss: 1.9266 - val_LNout93_loss: 3.1070 - val_LNout94_loss: 2.0704 - val_LNout95_loss: 0.7175 - val_LNout96_loss: 5.1258 - val_LNout97_loss: 3.5180 - val_LNout98_loss: 3.3059 - val_LNout99_loss: 4.0945 - val_LNout100_loss: 4.3228 - val_LNout101_loss: 2.4680 - val_LNout102_loss: 6.0660 - val_LNout103_loss: 5.9209 - val_LNout104_loss: 5.7539 - val_LNout105_loss: 5.3045 - val_LNout106_loss: 4.3922 - val_LNout107_loss: 2.7003 - val_LNout108_loss: 1.8075 - val_LNout109_loss: 1.0183 - val_LNout110_loss: 4.0268 - val_LNout111_loss: 2.5234 - val_LNout112_loss: 2.9904 - val_LNout113_loss: 2.6998 - val_LNout114_loss: 3.9762 - val_LNout115_loss: 2.7634 - val_LNout116_loss: 3.6613 - val_LNout117_loss: 1.5385 - val_LNout118_loss: 3.8375 - val_LNout119_loss: 2.2724 - val_LNout120_loss: 1.8746 - val_LNout121_loss: 4.0577 - val_LNout122_loss: 3.2236 - val_LNout123_loss: 5.0649 - val_LNout124_loss: 6.2209 - val_LNout125_loss: 4.0192 - val_LNout126_loss: 2.8553 - val_LNout127_loss: 1.8565 - val_LNout128_loss: 1.9753 - val_LNout129_loss: 3.5814 - val_LNout130_loss: 6.6615 - val_LNout131_loss: 6.5669 - val_LNout132_loss: 1.6941 - val_LNout133_loss: 2.7402 - val_LNout134_loss: 4.5923 - val_LNout135_loss: 3.8905 - val_LNout136_loss: 3.8263 - val_LNout137_loss: 1.7904 - val_LNout138_loss: 5.9022 - val_LNout139_loss: 2.2498 - val_LNout140_loss: 1.4959 - val_LNout141_loss: 4.6408 - val_LNout142_loss: 3.2728 - val_LNout143_loss: 4.5606 - val_LNout144_loss: 2.1502 - val_LNout145_loss: 4.0795 - val_LNout146_loss: 5.4295 - val_LNout147_loss: 4.3156 - val_LNout148_loss: 1.9699 - val_LNout149_loss: 4.6597 - val_LNout150_loss: 2.5959 - val_LNout151_loss: 6.1724 - val_LNout152_loss: 5.4138 - val_LNout153_loss: 3.4673 - val_LNout154_loss: 2.1285 - val_LNout155_loss: 3.1446 - val_LNout156_loss: 5.6488 - val_LNout157_loss: 1.7617 - val_LNout158_loss: 6.8248 - val_LNout159_loss: 1.1318 - val_LNout160_loss: 2.0160 - val_LNout161_loss: 6.7195 - val_LNout162_loss: 5.9368 - val_LNout163_loss: 5.7441 - val_LNout164_loss: 4.7773 - val_LNout165_loss: 3.5166 - val_LNout166_loss: 3.4199 - val_LNout167_loss: 3.6348 - val_LNout168_loss: 6.2777 - val_LNout169_loss: 2.8646 - val_LNout170_loss: 2.9903 - val_LNout171_loss: 4.4374 - val_LNout172_loss: 6.8781 - val_LNout173_loss: 1.2194 - val_LNout174_loss: 4.4371 - val_LNout175_loss: 5.1815 - val_LNout176_loss: 2.0629 - val_LNout177_loss: 2.0857 - val_LNout178_loss: 3.1767 - val_LNout179_loss: 2.9743 - val_LNout180_loss: 4.8528 - val_LNout181_loss: 4.6896 - val_LNout182_loss: 4.7436 - val_LNout183_loss: 6.2521 - val_LNout184_loss: 2.2735 - val_LNout185_loss: 3.9660 - val_LNout186_loss: 4.2050 - val_LNout187_loss: 4.6670 - val_LNout188_loss: 3.6707 - val_LNout189_loss: 2.9149 - val_LNout190_loss: 1.2765 - val_LNout191_loss: 2.8653 - val_LNout192_loss: 2.1190 - val_LNout193_loss: 4.8717 - val_LNout194_loss: 5.3007 - val_LNout195_loss: 8.1151 - val_LNout196_loss: 5.9690 - val_LNout197_loss: 4.6876 - val_LNout198_loss: 3.4327 - val_LNout199_loss: 1.6845 - val_LNout200_loss: 5.0713 - val_LNout201_loss: 6.4891 - val_LNout202_loss: 4.7607 - val_LNout203_loss: 4.4018 - val_LNout204_loss: 3.9242 - val_LNout205_loss: 1.4414 - val_LNout206_loss: 5.4318 - val_LNout207_loss: 5.1317 - val_LNout208_loss: 3.8079 - val_LNout209_loss: 4.9483 - val_LNout210_loss: 5.7129 - val_LNout211_loss: 3.4586 - val_LNout212_loss: 5.7484 - val_LNout213_loss: 3.5076 - val_LNout214_loss: 4.5719 - val_LNout215_loss: 3.6865 - val_LNout216_loss: 3.4053 - val_LNout217_loss: 8.6288 - val_LNout218_loss: 4.7076 - val_LNout219_loss: 4.2423 - val_LNout220_loss: 2.3110 - val_LNout221_loss: 5.6347 - val_LNout222_loss: 1.4885 - val_LNout223_loss: 2.3645 - val_LNout224_loss: 5.0662 - val_LNout225_loss: 0.6860 - val_LNout226_loss: 4.6126 - val_LNout227_loss: 5.4024 - val_LNout228_loss: 3.2522 - val_LNout229_loss: 1.6791 - val_LNout230_loss: 3.5248 - val_LNout231_loss: 2.9263 - val_LNout232_loss: 3.0453 - val_LNout233_loss: 2.7213 - val_LNout234_loss: 5.7307 - val_LNout235_loss: 6.4392 - val_LNout236_loss: 1.9738 - val_LNout237_loss: 5.4549 - val_LNout238_loss: 0.4790 - val_LNout239_loss: 5.3514 - val_LNout240_loss: 1.8828 - val_LNout241_loss: 2.8774 - val_LNout242_loss: 5.8470 - val_LNout243_loss: 3.6463 - val_LNout244_loss: 2.0126 - val_LNout245_loss: 3.6515 - val_LNout246_loss: 5.2124 - val_LNout247_loss: 8.2479 - val_LNout248_loss: 4.1655 - val_LNout249_loss: 6.2967 - val_LNout250_loss: 6.9298 - val_LNout251_loss: 3.3354 - val_LNout252_loss: 1.2843 - val_LNout253_loss: 3.7271 - val_LNout254_loss: 3.7904 - val_LNout255_loss: 6.7197 - val_LNout256_loss: 2.9668 - val_LNout257_loss: 3.5523 - val_LNout258_loss: 2.1378 - val_LNout259_loss: 3.1503 - val_LNout260_loss: 1.5758 - val_LNout261_loss: 3.6670 - val_LNout262_loss: 3.9362 - val_LNout263_loss: 1.1279 - val_LNout264_loss: 2.3406 - val_LNout265_loss: 5.4290 - val_LNout266_loss: 2.7937 - val_LNout267_loss: 1.7522 - val_LNout268_loss: 2.6695 - val_LNout269_loss: 3.2514 - val_LNout270_loss: 2.1634 - val_LNout271_loss: 6.5836 - val_LNout272_loss: 4.1320 - val_LNout273_loss: 4.3824 - val_LNout274_loss: 2.3144 - val_LNout275_loss: 1.5348 - val_LNout276_loss: 4.3325 - val_LNout277_loss: 2.7846 - val_LNout278_loss: 2.4655 - val_LNout279_loss: 2.9866 - val_LNout280_loss: 3.3377 - val_LNout281_loss: 4.4725 - val_LNout282_loss: 4.0397 - val_LNout283_loss: 4.2821 - val_LNout284_loss: 0.7044 - val_LNout285_loss: 5.7060 - val_LNout286_loss: 2.9583 - val_LNout287_loss: 1.6357 - val_LNout288_loss: 6.1522 - val_LNout289_loss: 6.2856 - val_LNout290_loss: 5.5177 - val_LNout291_loss: 1.6234 - val_LNout292_loss: 5.2641 - val_LNout293_loss: 2.4074 - val_LNout294_loss: 6.0593 - val_LNout295_loss: 2.9360 - val_LNout296_loss: 2.5499 - val_LNout297_loss: 4.7873 - val_LNout298_loss: 0.8125 - val_LNout299_loss: 3.9315 - val_LNout300_loss: 1.9175 - val_LNout301_loss: 3.9536 - val_LNout302_loss: 4.3175 - val_LNout303_loss: 4.7997 - val_LNout304_loss: 4.6937 - val_LNout305_loss: 4.3143 - val_LNout306_loss: 3.0455 - val_LNout307_loss: 2.5903 - val_LNout308_loss: 2.7968 - val_LNout309_loss: 4.9295 - val_LNout310_loss: 7.3190 - val_LNout311_loss: 4.7070 - val_LNout312_loss: 2.0367 - val_LNout313_loss: 0.0471 - val_LNout314_loss: 4.9109 - val_LNout315_loss: 1.1365 - val_LNout316_loss: 2.7155 - val_LNout317_loss: 4.4334 - val_LNout318_loss: 3.8703 - val_LNout319_loss: 0.9962 - val_LNout320_loss: 1.7609 - val_LNout321_loss: 2.2120 - val_LNout322_loss: 3.9442 - val_LNout323_loss: 4.6317 - val_LNout324_loss: 2.3687 - val_LNout325_loss: 6.4408 - val_LNout326_loss: 4.8127 - val_LNout327_loss: 5.5173 - val_LNout328_loss: 4.4027 - val_LNout329_loss: 5.3912 - val_LNout330_loss: 1.3403 - val_LNout331_loss: 3.1157 - val_LNout332_loss: 1.9594 - val_LNout333_loss: 3.8227 - val_LNout334_loss: 3.6063 - val_LNout335_loss: 4.6919 - val_LNout336_loss: 2.7212 - val_LNout337_loss: 2.7561 - val_LNout338_loss: 3.2811 - val_LNout339_loss: 1.0121 - val_LNout340_loss: 2.8080 - val_LNout341_loss: 8.0902 - val_LNout342_loss: 3.1323 - val_LNout343_loss: 7.2284 - val_LNout344_loss: 5.9034 - val_LNout345_loss: 1.7523 - val_LNout346_loss: 1.6128 - val_LNout347_loss: 3.6660 - val_LNout348_loss: 3.3479 - val_LNout349_loss: 6.3319 - val_LNout350_loss: 8.5002 - val_LNout351_loss: 3.3219 - val_LNout352_loss: 3.8809 - val_LNout353_loss: 4.4811 - val_LNout354_loss: 2.1346 - val_LNout355_loss: 4.0871 - val_LNout356_loss: 4.7607 - val_LNout357_loss: 4.4954 - val_LNout358_loss: 3.1861 - val_LNout359_loss: 1.9987 - val_error_feed_forward_output1_binary_accuracy: 0.9609 - val_LNout0_categorical_accuracy: 0.0078 - val_LNout1_categorical_accuracy: 0.0000e+00 - val_LNout2_categorical_accuracy: 0.0000e+00 - val_LNout3_categorical_accuracy: 0.0000e+00 - val_LNout4_categorical_accuracy: 0.0000e+00 - val_LNout5_categorical_accuracy: 0.9297 - val_LNout6_categorical_accuracy: 0.0000e+00 - val_LNout7_categorical_accuracy: 0.0000e+00 - val_LNout8_categorical_accuracy: 0.0000e+00 - val_LNout9_categorical_accuracy: 0.0000e+00 - val_LNout10_categorical_accuracy: 0.0000e+00 - val_LNout11_categorical_accuracy: 0.0000e+00 - val_LNout12_categorical_accuracy: 0.0000e+00 - val_LNout13_categorical_accuracy: 0.0000e+00 - val_LNout14_categorical_accuracy: 0.0000e+00 - val_LNout15_categorical_accuracy: 0.9902 - val_LNout16_categorical_accuracy: 0.0000e+00 - val_LNout17_categorical_accuracy: 0.0000e+00 - val_LNout18_categorical_accuracy: 0.0000e+00 - val_LNout19_categorical_accuracy: 0.0000e+00 - val_LNout20_categorical_accuracy: 0.0000e+00 - val_LNout21_categorical_accuracy: 0.0000e+00 - val_LNout22_categorical_accuracy: 0.0000e+00 - val_LNout23_categorical_accuracy: 0.0000e+00 - val_LNout24_categorical_accuracy: 0.9922 - val_LNout25_categorical_accuracy: 0.0000e+00 - val_LNout26_categorical_accuracy: 0.0000e+00 - val_LNout27_categorical_accuracy: 0.9922 - val_LNout28_categorical_accuracy: 0.9980 - val_LNout29_categorical_accuracy: 0.9980 - val_LNout30_categorical_accuracy: 0.0000e+00 - val_LNout31_categorical_accuracy: 0.9980 - val_LNout32_categorical_accuracy: 0.0000e+00 - val_LNout33_categorical_accuracy: 0.9980 - val_LNout34_categorical_accuracy: 0.0000e+00 - val_LNout35_categorical_accuracy: 0.0000e+00 - val_LNout36_categorical_accuracy: 0.0000e+00 - val_LNout37_categorical_accuracy: 0.0000e+00 - val_LNout38_categorical_accuracy: 0.0000e+00 - val_LNout39_categorical_accuracy: 0.0000e+00 - val_LNout40_categorical_accuracy: 1.0000 - val_LNout41_categorical_accuracy: 0.0000e+00 - val_LNout42_categorical_accuracy: 0.0000e+00 - val_LNout43_categorical_accuracy: 1.0000 - val_LNout44_categorical_accuracy: 1.0000 - val_LNout45_categorical_accuracy: 0.0000e+00 - val_LNout46_categorical_accuracy: 0.0000e+00 - val_LNout47_categorical_accuracy: 0.0000e+00 - val_LNout48_categorical_accuracy: 0.0000e+00 - val_LNout49_categorical_accuracy: 0.0000e+00 - val_LNout50_categorical_accuracy: 0.0000e+00 - val_LNout51_categorical_accuracy: 0.0000e+00 - val_LNout52_categorical_accuracy: 0.0000e+00 - val_LNout53_categorical_accuracy: 0.0000e+00 - val_LNout54_categorical_accuracy: 0.0000e+00 - val_LNout55_categorical_accuracy: 0.0000e+00 - val_LNout56_categorical_accuracy: 0.0000e+00 - val_LNout57_categorical_accuracy: 0.0000e+00 - val_LNout58_categorical_accuracy: 0.0000e+00 - val_LNout59_categorical_accuracy: 0.0000e+00 - val_LNout60_categorical_accuracy: 0.7949 - val_LNout61_categorical_accuracy: 0.0000e+00 - val_LNout62_categorical_accuracy: 0.0000e+00 - val_LNout63_categorical_accuracy: 0.9199 - val_LNout64_categorical_accuracy: 0.0000e+00 - val_LNout65_categorical_accuracy: 0.0039 - val_LNout66_categorical_accuracy: 0.0000e+00 - val_LNout67_categorical_accuracy: 0.0000e+00 - val_LNout68_categorical_accuracy: 0.0000e+00 - val_LNout69_categorical_accuracy: 0.9688 - val_LNout70_categorical_accuracy: 0.0000e+00 - val_LNout71_categorical_accuracy: 0.0000e+00 - val_LNout72_categorical_accuracy: 0.0000e+00 - val_LNout73_categorical_accuracy: 0.9844 - val_LNout74_categorical_accuracy: 0.0000e+00 - val_LNout75_categorical_accuracy: 0.0000e+00 - val_LNout76_categorical_accuracy: 0.0000e+00 - val_LNout77_categorical_accuracy: 0.9883 - val_LNout78_categorical_accuracy: 0.0000e+00 - val_LNout79_categorical_accuracy: 0.0000e+00 - val_LNout80_categorical_accuracy: 0.9980 - val_LNout81_categorical_accuracy: 0.0000e+00 - val_LNout82_categorical_accuracy: 0.0000e+00 - val_LNout83_categorical_accuracy: 0.0000e+00 - val_LNout84_categorical_accuracy: 0.0000e+00 - val_LNout85_categorical_accuracy: 0.0000e+00 - val_LNout86_categorical_accuracy: 0.0000e+00 - val_LNout87_categorical_accuracy: 1.0000 - val_LNout88_categorical_accuracy: 1.0000 - val_LNout89_categorical_accuracy: 0.0000e+00 - val_LNout90_categorical_accuracy: 1.0000 - val_LNout91_categorical_accuracy: 0.0000e+00 - val_LNout92_categorical_accuracy: 0.0000e+00 - val_LNout93_categorical_accuracy: 0.0000e+00 - val_LNout94_categorical_accuracy: 0.0000e+00 - val_LNout95_categorical_accuracy: 1.0000 - val_LNout96_categorical_accuracy: 0.0000e+00 - val_LNout97_categorical_accuracy: 0.0000e+00 - val_LNout98_categorical_accuracy: 0.0000e+00 - val_LNout99_categorical_accuracy: 0.0000e+00 - val_LNout100_categorical_accuracy: 0.0000e+00 - val_LNout101_categorical_accuracy: 0.0000e+00 - val_LNout102_categorical_accuracy: 0.0000e+00 - val_LNout103_categorical_accuracy: 0.0000e+00 - val_LNout104_categorical_accuracy: 0.0000e+00 - val_LNout105_categorical_accuracy: 0.0000e+00 - val_LNout106_categorical_accuracy: 0.0000e+00 - val_LNout107_categorical_accuracy: 0.0000e+00 - val_LNout108_categorical_accuracy: 1.0000 - val_LNout109_categorical_accuracy: 1.0000 - val_LNout110_categorical_accuracy: 0.0000e+00 - val_LNout111_categorical_accuracy: 0.0000e+00 - val_LNout112_categorical_accuracy: 0.0000e+00 - val_LNout113_categorical_accuracy: 0.0000e+00 - val_LNout114_categorical_accuracy: 0.0000e+00 - val_LNout115_categorical_accuracy: 0.0000e+00 - val_LNout116_categorical_accuracy: 0.0000e+00 - val_LNout117_categorical_accuracy: 1.0000 - val_LNout118_categorical_accuracy: 0.0000e+00 - val_LNout119_categorical_accuracy: 0.0000e+00 - val_LNout120_categorical_accuracy: 0.8984 - val_LNout121_categorical_accuracy: 0.0000e+00 - val_LNout122_categorical_accuracy: 0.0000e+00 - val_LNout123_categorical_accuracy: 0.0000e+00 - val_LNout124_categorical_accuracy: 0.0000e+00 - val_LNout125_categorical_accuracy: 0.0000e+00 - val_LNout126_categorical_accuracy: 0.0000e+00 - val_LNout127_categorical_accuracy: 1.0000 - val_LNout128_categorical_accuracy: 0.0000e+00 - val_LNout129_categorical_accuracy: 0.0000e+00 - val_LNout130_categorical_accuracy: 0.0000e+00 - val_LNout131_categorical_accuracy: 0.0000e+00 - val_LNout132_categorical_accuracy: 1.0000 - val_LNout133_categorical_accuracy: 0.0000e+00 - val_LNout134_categorical_accuracy: 0.0000e+00 - val_LNout135_categorical_accuracy: 0.0000e+00 - val_LNout136_categorical_accuracy: 0.0000e+00 - val_LNout137_categorical_accuracy: 1.0000 - val_LNout138_categorical_accuracy: 0.0000e+00 - val_LNout139_categorical_accuracy: 0.0000e+00 - val_LNout140_categorical_accuracy: 1.0000 - val_LNout141_categorical_accuracy: 0.0000e+00 - val_LNout142_categorical_accuracy: 0.0000e+00 - val_LNout143_categorical_accuracy: 0.0000e+00 - val_LNout144_categorical_accuracy: 1.0000 - val_LNout145_categorical_accuracy: 0.0000e+00 - val_LNout146_categorical_accuracy: 0.0000e+00 - val_LNout147_categorical_accuracy: 0.0000e+00 - val_LNout148_categorical_accuracy: 1.0000 - val_LNout149_categorical_accuracy: 0.0000e+00 - val_LNout150_categorical_accuracy: 0.0000e+00 - val_LNout151_categorical_accuracy: 0.0000e+00 - val_LNout152_categorical_accuracy: 0.0000e+00 - val_LNout153_categorical_accuracy: 0.0000e+00 - val_LNout154_categorical_accuracy: 0.0000e+00 - val_LNout155_categorical_accuracy: 0.0000e+00 - val_LNout156_categorical_accuracy: 0.0000e+00 - val_LNout157_categorical_accuracy: 1.0000 - val_LNout158_categorical_accuracy: 0.0000e+00 - val_LNout159_categorical_accuracy: 1.0000 - val_LNout160_categorical_accuracy: 1.0000 - val_LNout161_categorical_accuracy: 0.0000e+00 - val_LNout162_categorical_accuracy: 0.0000e+00 - val_LNout163_categorical_accuracy: 0.0000e+00 - val_LNout164_categorical_accuracy: 0.0000e+00 - val_LNout165_categorical_accuracy: 0.0000e+00 - val_LNout166_categorical_accuracy: 0.0000e+00 - val_LNout167_categorical_accuracy: 0.0000e+00 - val_LNout168_categorical_accuracy: 0.0000e+00 - val_LNout169_categorical_accuracy: 0.0000e+00 - val_LNout170_categorical_accuracy: 0.0000e+00 - val_LNout171_categorical_accuracy: 0.0000e+00 - val_LNout172_categorical_accuracy: 0.0000e+00 - val_LNout173_categorical_accuracy: 1.0000 - val_LNout174_categorical_accuracy: 0.0000e+00 - val_LNout175_categorical_accuracy: 0.0000e+00 - val_LNout176_categorical_accuracy: 1.0000 - val_LNout177_categorical_accuracy: 0.0000e+00 - val_LNout178_categorical_accuracy: 0.0000e+00 - val_LNout179_categorical_accuracy: 0.0000e+00 - val_LNout180_categorical_accuracy: 0.0000e+00 - val_LNout181_categorical_accuracy: 0.0059 - val_LNout182_categorical_accuracy: 0.0000e+00 - val_LNout183_categorical_accuracy: 0.0000e+00 - val_LNout184_categorical_accuracy: 0.0000e+00 - val_LNout185_categorical_accuracy: 0.0000e+00 - val_LNout186_categorical_accuracy: 0.0000e+00 - val_LNout187_categorical_accuracy: 0.0000e+00 - val_LNout188_categorical_accuracy: 0.0000e+00 - val_LNout189_categorical_accuracy: 0.0000e+00 - val_LNout190_categorical_accuracy: 0.9961 - val_LNout191_categorical_accuracy: 0.0000e+00 - val_LNout192_categorical_accuracy: 0.0000e+00 - val_LNout193_categorical_accuracy: 0.0000e+00 - val_LNout194_categorical_accuracy: 0.0000e+00 - val_LNout195_categorical_accuracy: 0.0000e+00 - val_LNout196_categorical_accuracy: 0.0000e+00 - val_LNout197_categorical_accuracy: 0.0000e+00 - val_LNout198_categorical_accuracy: 0.0000e+00 - val_LNout199_categorical_accuracy: 1.0000 - val_LNout200_categorical_accuracy: 0.0000e+00 - val_LNout201_categorical_accuracy: 0.0000e+00 - val_LNout202_categorical_accuracy: 0.0000e+00 - val_LNout203_categorical_accuracy: 0.0000e+00 - val_LNout204_categorical_accuracy: 0.0000e+00 - val_LNout205_categorical_accuracy: 1.0000 - val_LNout206_categorical_accuracy: 0.0000e+00 - val_LNout207_categorical_accuracy: 0.0000e+00 - val_LNout208_categorical_accuracy: 0.0000e+00 - val_LNout209_categorical_accuracy: 0.0000e+00 - val_LNout210_categorical_accuracy: 0.0000e+00 - val_LNout211_categorical_accuracy: 0.0000e+00 - val_LNout212_categorical_accuracy: 0.0000e+00 - val_LNout213_categorical_accuracy: 0.0000e+00 - val_LNout214_categorical_accuracy: 0.0000e+00 - val_LNout215_categorical_accuracy: 0.0000e+00 - val_LNout216_categorical_accuracy: 0.0000e+00 - val_LNout217_categorical_accuracy: 0.0000e+00 - val_LNout218_categorical_accuracy: 0.0000e+00 - val_LNout219_categorical_accuracy: 0.0000e+00 - val_LNout220_categorical_accuracy: 0.0000e+00 - val_LNout221_categorical_accuracy: 0.0000e+00 - val_LNout222_categorical_accuracy: 1.0000 - val_LNout223_categorical_accuracy: 0.0000e+00 - val_LNout224_categorical_accuracy: 0.0000e+00 - val_LNout225_categorical_accuracy: 1.0000 - val_LNout226_categorical_accuracy: 0.0000e+00 - val_LNout227_categorical_accuracy: 0.0000e+00 - val_LNout228_categorical_accuracy: 0.0000e+00 - val_LNout229_categorical_accuracy: 1.0000 - val_LNout230_categorical_accuracy: 0.0000e+00 - val_LNout231_categorical_accuracy: 0.0000e+00 - val_LNout232_categorical_accuracy: 0.0000e+00 - val_LNout233_categorical_accuracy: 0.0000e+00 - val_LNout234_categorical_accuracy: 0.0000e+00 - val_LNout235_categorical_accuracy: 0.0000e+00 - val_LNout236_categorical_accuracy: 1.0000 - val_LNout237_categorical_accuracy: 0.0000e+00 - val_LNout238_categorical_accuracy: 1.0000 - val_LNout239_categorical_accuracy: 0.0000e+00 - val_LNout240_categorical_accuracy: 0.7344 - val_LNout241_categorical_accuracy: 0.0000e+00 - val_LNout242_categorical_accuracy: 0.0000e+00 - val_LNout243_categorical_accuracy: 0.0000e+00 - val_LNout244_categorical_accuracy: 0.9922 - val_LNout245_categorical_accuracy: 0.0000e+00 - val_LNout246_categorical_accuracy: 0.0000e+00 - val_LNout247_categorical_accuracy: 0.0000e+00 - val_LNout248_categorical_accuracy: 0.0000e+00 - val_LNout249_categorical_accuracy: 0.0000e+00 - val_LNout250_categorical_accuracy: 0.0000e+00 - val_LNout251_categorical_accuracy: 0.0000e+00 - val_LNout252_categorical_accuracy: 1.0000 - val_LNout253_categorical_accuracy: 0.0000e+00 - val_LNout254_categorical_accuracy: 0.0000e+00 - val_LNout255_categorical_accuracy: 0.0000e+00 - val_LNout256_categorical_accuracy: 0.0000e+00 - val_LNout257_categorical_accuracy: 0.0000e+00 - val_LNout258_categorical_accuracy: 0.0000e+00 - val_LNout259_categorical_accuracy: 0.0000e+00 - val_LNout260_categorical_accuracy: 1.0000 - val_LNout261_categorical_accuracy: 0.0000e+00 - val_LNout262_categorical_accuracy: 0.0000e+00 - val_LNout263_categorical_accuracy: 1.0000 - val_LNout264_categorical_accuracy: 0.0000e+00 - val_LNout265_categorical_accuracy: 0.0000e+00 - val_LNout266_categorical_accuracy: 0.0000e+00 - val_LNout267_categorical_accuracy: 1.0000 - val_LNout268_categorical_accuracy: 0.0000e+00 - val_LNout269_categorical_accuracy: 0.0000e+00 - val_LNout270_categorical_accuracy: 1.0000 - val_LNout271_categorical_accuracy: 0.0000e+00 - val_LNout272_categorical_accuracy: 0.0000e+00 - val_LNout273_categorical_accuracy: 0.0000e+00 - val_LNout274_categorical_accuracy: 1.0000 - val_LNout275_categorical_accuracy: 1.0000 - val_LNout276_categorical_accuracy: 0.0000e+00 - val_LNout277_categorical_accuracy: 0.0000e+00 - val_LNout278_categorical_accuracy: 0.0000e+00 - val_LNout279_categorical_accuracy: 0.0000e+00 - val_LNout280_categorical_accuracy: 0.0000e+00 - val_LNout281_categorical_accuracy: 0.0000e+00 - val_LNout282_categorical_accuracy: 0.0000e+00 - val_LNout283_categorical_accuracy: 0.0000e+00 - val_LNout284_categorical_accuracy: 1.0000 - val_LNout285_categorical_accuracy: 0.0000e+00 - val_LNout286_categorical_accuracy: 0.0000e+00 - val_LNout287_categorical_accuracy: 1.0000 - val_LNout288_categorical_accuracy: 0.0000e+00 - val_LNout289_categorical_accuracy: 0.0000e+00 - val_LNout290_categorical_accuracy: 0.0000e+00 - val_LNout291_categorical_accuracy: 1.0000 - val_LNout292_categorical_accuracy: 0.0000e+00 - val_LNout293_categorical_accuracy: 0.0000e+00 - val_LNout294_categorical_accuracy: 0.0000e+00 - val_LNout295_categorical_accuracy: 0.0000e+00 - val_LNout296_categorical_accuracy: 0.0000e+00 - val_LNout297_categorical_accuracy: 0.0000e+00 - val_LNout298_categorical_accuracy: 1.0000 - val_LNout299_categorical_accuracy: 0.0000e+00 - val_LNout300_categorical_accuracy: 0.0000e+00 - val_LNout301_categorical_accuracy: 0.0000e+00 - val_LNout302_categorical_accuracy: 0.0000e+00 - val_LNout303_categorical_accuracy: 0.0000e+00 - val_LNout304_categorical_accuracy: 0.0000e+00 - val_LNout305_categorical_accuracy: 0.0000e+00 - val_LNout306_categorical_accuracy: 0.0000e+00 - val_LNout307_categorical_accuracy: 0.0000e+00 - val_LNout308_categorical_accuracy: 0.0000e+00 - val_LNout309_categorical_accuracy: 0.0000e+00 - val_LNout310_categorical_accuracy: 0.0000e+00 - val_LNout311_categorical_accuracy: 0.0000e+00 - val_LNout312_categorical_accuracy: 0.0000e+00 - val_LNout313_categorical_accuracy: 1.0000 - val_LNout314_categorical_accuracy: 0.0000e+00 - val_LNout315_categorical_accuracy: 1.0000 - val_LNout316_categorical_accuracy: 1.0000 - val_LNout317_categorical_accuracy: 0.0000e+00 - val_LNout318_categorical_accuracy: 0.0000e+00 - val_LNout319_categorical_accuracy: 1.0000 - val_LNout320_categorical_accuracy: 0.0000e+00 - val_LNout321_categorical_accuracy: 0.0000e+00 - val_LNout322_categorical_accuracy: 0.0000e+00 - val_LNout323_categorical_accuracy: 0.0000e+00 - val_LNout324_categorical_accuracy: 0.0000e+00 - val_LNout325_categorical_accuracy: 0.0000e+00 - val_LNout326_categorical_accuracy: 0.0000e+00 - val_LNout327_categorical_accuracy: 0.0000e+00 - val_LNout328_categorical_accuracy: 0.0000e+00 - val_LNout329_categorical_accuracy: 0.0000e+00 - val_LNout330_categorical_accuracy: 1.0000 - val_LNout331_categorical_accuracy: 0.0000e+00 - val_LNout332_categorical_accuracy: 1.0000 - val_LNout333_categorical_accuracy: 0.0000e+00 - val_LNout334_categorical_accuracy: 0.0000e+00 - val_LNout335_categorical_accuracy: 0.0000e+00 - val_LNout336_categorical_accuracy: 0.0000e+00 - val_LNout337_categorical_accuracy: 0.0000e+00 - val_LNout338_categorical_accuracy: 0.0000e+00 - val_LNout339_categorical_accuracy: 1.0000 - val_LNout340_categorical_accuracy: 0.0000e+00 - val_LNout341_categorical_accuracy: 0.0000e+00 - val_LNout342_categorical_accuracy: 0.0000e+00 - val_LNout343_categorical_accuracy: 0.0000e+00 - val_LNout344_categorical_accuracy: 0.0000e+00 - val_LNout345_categorical_accuracy: 1.0000 - val_LNout346_categorical_accuracy: 1.0000 - val_LNout347_categorical_accuracy: 0.0000e+00 - val_LNout348_categorical_accuracy: 0.0000e+00 - val_LNout349_categorical_accuracy: 0.0000e+00 - val_LNout350_categorical_accuracy: 0.0000e+00 - val_LNout351_categorical_accuracy: 0.0000e+00 - val_LNout352_categorical_accuracy: 0.0000e+00 - val_LNout353_categorical_accuracy: 0.0000e+00 - val_LNout354_categorical_accuracy: 0.0000e+00 - val_LNout355_categorical_accuracy: 0.0000e+00 - val_LNout356_categorical_accuracy: 0.0000e+00 - val_LNout357_categorical_accuracy: 0.0000e+00 - val_LNout358_categorical_accuracy: 0.0000e+00 - val_LNout359_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "39/39 - 147s - loss: 716.9869 - error_feed_forward_output1_loss: 0.1664 - LNout0_loss: 4.1725 - LNout1_loss: 4.9594 - LNout2_loss: 3.7720 - LNout3_loss: 3.2631 - LNout4_loss: 3.3404 - LNout5_loss: 0.8914 - LNout6_loss: 1.2769 - LNout7_loss: 2.5993 - LNout8_loss: 2.4865 - LNout9_loss: 0.6991 - LNout10_loss: 1.0270 - LNout11_loss: 1.4875 - LNout12_loss: 3.9642 - LNout13_loss: 5.5599 - LNout14_loss: 2.3756 - LNout15_loss: 0.2893 - LNout16_loss: 6.6328 - LNout17_loss: 1.7548 - LNout18_loss: 1.4149 - LNout19_loss: 1.5717 - LNout20_loss: 0.7560 - LNout21_loss: 1.0418 - LNout22_loss: 4.1564 - LNout23_loss: 3.8034 - LNout24_loss: 0.3457 - LNout25_loss: 0.8945 - LNout26_loss: 1.1847 - LNout27_loss: 0.4587 - LNout28_loss: 0.5557 - LNout29_loss: 0.3799 - LNout30_loss: 3.0705 - LNout31_loss: 0.5513 - LNout32_loss: 2.9077 - LNout33_loss: 0.2336 - LNout34_loss: 1.1949 - LNout35_loss: 5.9065 - LNout36_loss: 3.0602 - LNout37_loss: 1.3309 - LNout38_loss: 5.9459 - LNout39_loss: 1.3844 - LNout40_loss: 0.3754 - LNout41_loss: 1.7626 - LNout42_loss: 2.7503 - LNout43_loss: 0.5277 - LNout44_loss: 0.6389 - LNout45_loss: 3.9095 - LNout46_loss: 1.1257 - LNout47_loss: 5.0860 - LNout48_loss: 1.9843 - LNout49_loss: 0.4640 - LNout50_loss: 2.2973 - LNout51_loss: 2.1700 - LNout52_loss: 1.3025 - LNout53_loss: 1.5130 - LNout54_loss: 1.3711 - LNout55_loss: 4.0978 - LNout56_loss: 1.0068 - LNout57_loss: 1.4725 - LNout58_loss: 3.7243 - LNout59_loss: 1.7894 - LNout60_loss: 1.7894 - LNout61_loss: 2.6431 - LNout62_loss: 2.6987 - LNout63_loss: 1.0487 - LNout64_loss: 5.9022 - LNout65_loss: 2.0331 - LNout66_loss: 8.7339 - LNout67_loss: 2.6231 - LNout68_loss: 1.1416 - LNout69_loss: 0.4242 - LNout70_loss: 0.7377 - LNout71_loss: 2.0630 - LNout72_loss: 4.4215 - LNout73_loss: 0.7025 - LNout74_loss: 1.9947 - LNout75_loss: 3.4968 - LNout76_loss: 4.9912 - LNout77_loss: 0.4680 - LNout78_loss: 1.7128 - LNout79_loss: 1.4070 - LNout80_loss: 0.3860 - LNout81_loss: 4.1205 - LNout82_loss: 0.8888 - LNout83_loss: 1.0569 - LNout84_loss: 0.6650 - LNout85_loss: 3.6154 - LNout86_loss: 0.8181 - LNout87_loss: 0.3393 - LNout88_loss: 0.1809 - LNout89_loss: 1.6259 - LNout90_loss: 0.2271 - LNout91_loss: 1.1622 - LNout92_loss: 0.4994 - LNout93_loss: 1.0988 - LNout94_loss: 0.5220 - LNout95_loss: 0.1205 - LNout96_loss: 3.1899 - LNout97_loss: 1.3709 - LNout98_loss: 1.2318 - LNout99_loss: 2.3484 - LNout100_loss: 2.2039 - LNout101_loss: 0.6318 - LNout102_loss: 4.8693 - LNout103_loss: 3.7654 - LNout104_loss: 3.5978 - LNout105_loss: 3.3920 - LNout106_loss: 2.0710 - LNout107_loss: 0.8161 - LNout108_loss: 0.3758 - LNout109_loss: 0.1995 - LNout110_loss: 1.7127 - LNout111_loss: 0.6264 - LNout112_loss: 0.9217 - LNout113_loss: 0.8189 - LNout114_loss: 1.6224 - LNout115_loss: 0.9580 - LNout116_loss: 1.4230 - LNout117_loss: 0.3450 - LNout118_loss: 1.6501 - LNout119_loss: 0.6189 - LNout120_loss: 1.0582 - LNout121_loss: 2.0449 - LNout122_loss: 1.2411 - LNout123_loss: 3.0962 - LNout124_loss: 4.3141 - LNout125_loss: 1.6302 - LNout126_loss: 0.9642 - LNout127_loss: 0.4498 - LNout128_loss: 0.5223 - LNout129_loss: 1.3460 - LNout130_loss: 5.3564 - LNout131_loss: 5.5602 - LNout132_loss: 0.3569 - LNout133_loss: 0.8138 - LNout134_loss: 2.5196 - LNout135_loss: 1.4349 - LNout136_loss: 1.4410 - LNout137_loss: 0.4548 - LNout138_loss: 3.7198 - LNout139_loss: 0.6462 - LNout140_loss: 0.2934 - LNout141_loss: 2.6796 - LNout142_loss: 1.2476 - LNout143_loss: 2.1068 - LNout144_loss: 0.5052 - LNout145_loss: 1.7326 - LNout146_loss: 3.4105 - LNout147_loss: 1.8580 - LNout148_loss: 0.4477 - LNout149_loss: 2.3106 - LNout150_loss: 0.8481 - LNout151_loss: 4.0944 - LNout152_loss: 3.1867 - LNout153_loss: 1.3101 - LNout154_loss: 0.5182 - LNout155_loss: 1.0829 - LNout156_loss: 3.5654 - LNout157_loss: 0.3838 - LNout158_loss: 4.8719 - LNout159_loss: 0.2288 - LNout160_loss: 0.4794 - LNout161_loss: 5.4209 - LNout162_loss: 4.0330 - LNout163_loss: 3.5051 - LNout164_loss: 2.3867 - LNout165_loss: 1.2295 - LNout166_loss: 1.2774 - LNout167_loss: 1.3766 - LNout168_loss: 4.8403 - LNout169_loss: 0.8514 - LNout170_loss: 0.8421 - LNout171_loss: 2.6170 - LNout172_loss: 5.5770 - LNout173_loss: 0.2307 - LNout174_loss: 2.0956 - LNout175_loss: 3.0470 - LNout176_loss: 0.4566 - LNout177_loss: 0.5049 - LNout178_loss: 0.9964 - LNout179_loss: 0.9347 - LNout180_loss: 2.6394 - LNout181_loss: 2.7508 - LNout182_loss: 2.5243 - LNout183_loss: 4.1433 - LNout184_loss: 0.8212 - LNout185_loss: 1.8257 - LNout186_loss: 1.7040 - LNout187_loss: 2.5822 - LNout188_loss: 1.5050 - LNout189_loss: 0.9548 - LNout190_loss: 0.2749 - LNout191_loss: 0.9537 - LNout192_loss: 0.5772 - LNout193_loss: 2.5281 - LNout194_loss: 2.7728 - LNout195_loss: 7.1000 - LNout196_loss: 4.0711 - LNout197_loss: 2.5620 - LNout198_loss: 1.2670 - LNout199_loss: 0.3430 - LNout200_loss: 2.5930 - LNout201_loss: 4.6135 - LNout202_loss: 2.8161 - LNout203_loss: 2.1162 - LNout204_loss: 1.8525 - LNout205_loss: 0.3482 - LNout206_loss: 3.2403 - LNout207_loss: 2.6283 - LNout208_loss: 1.4922 - LNout209_loss: 2.5401 - LNout210_loss: 3.9758 - LNout211_loss: 1.2564 - LNout212_loss: 3.2701 - LNout213_loss: 1.2657 - LNout214_loss: 2.4181 - LNout215_loss: 1.4144 - LNout216_loss: 1.1559 - LNout217_loss: 8.4204 - LNout218_loss: 1.9753 - LNout219_loss: 1.7219 - LNout220_loss: 0.7664 - LNout221_loss: 3.7901 - LNout222_loss: 0.3469 - LNout223_loss: 0.5848 - LNout224_loss: 2.4906 - LNout225_loss: 0.1124 - LNout226_loss: 2.3441 - LNout227_loss: 3.2826 - LNout228_loss: 1.1156 - LNout229_loss: 0.4067 - LNout230_loss: 1.3182 - LNout231_loss: 0.9599 - LNout232_loss: 1.0239 - LNout233_loss: 0.7701 - LNout234_loss: 4.8159 - LNout235_loss: 4.7440 - LNout236_loss: 0.4794 - LNout237_loss: 3.1471 - LNout238_loss: 0.0663 - LNout239_loss: 3.2406 - LNout240_loss: 0.6417 - LNout241_loss: 1.0189 - LNout242_loss: 4.5253 - LNout243_loss: 1.3839 - LNout244_loss: 0.4687 - LNout245_loss: 1.4298 - LNout246_loss: 2.9899 - LNout247_loss: 8.2161 - LNout248_loss: 1.8576 - LNout249_loss: 4.1377 - LNout250_loss: 6.0301 - LNout251_loss: 1.3089 - LNout252_loss: 0.2740 - LNout253_loss: 1.4721 - LNout254_loss: 1.4010 - LNout255_loss: 5.0513 - LNout256_loss: 0.8187 - LNout257_loss: 1.1874 - LNout258_loss: 0.5143 - LNout259_loss: 1.1607 - LNout260_loss: 0.3641 - LNout261_loss: 1.6427 - LNout262_loss: 1.7309 - LNout263_loss: 0.2370 - LNout264_loss: 0.6351 - LNout265_loss: 3.0057 - LNout266_loss: 0.7402 - LNout267_loss: 0.3918 - LNout268_loss: 0.7217 - LNout269_loss: 1.1489 - LNout270_loss: 0.5047 - LNout271_loss: 6.1227 - LNout272_loss: 1.7463 - LNout273_loss: 2.0083 - LNout274_loss: 0.5878 - LNout275_loss: 0.3575 - LNout276_loss: 1.6658 - LNout277_loss: 0.9221 - LNout278_loss: 0.6614 - LNout279_loss: 0.9641 - LNout280_loss: 1.1554 - LNout281_loss: 2.0717 - LNout282_loss: 1.7755 - LNout283_loss: 2.0527 - LNout284_loss: 0.1261 - LNout285_loss: 3.3522 - LNout286_loss: 0.9300 - LNout287_loss: 0.3477 - LNout288_loss: 4.5160 - LNout289_loss: 5.0666 - LNout290_loss: 3.1866 - LNout291_loss: 0.3609 - LNout292_loss: 2.7106 - LNout293_loss: 0.6931 - LNout294_loss: 4.0577 - LNout295_loss: 0.8623 - LNout296_loss: 0.7523 - LNout297_loss: 2.3758 - LNout298_loss: 0.1585 - LNout299_loss: 1.4112 - LNout300_loss: 0.5181 - LNout301_loss: 1.6414 - LNout302_loss: 2.3402 - LNout303_loss: 2.6447 - LNout304_loss: 2.3998 - LNout305_loss: 1.7995 - LNout306_loss: 1.1029 - LNout307_loss: 0.9129 - LNout308_loss: 0.7943 - LNout309_loss: 2.6915 - LNout310_loss: 6.3635 - LNout311_loss: 2.7321 - LNout312_loss: 0.5215 - LNout313_loss: 0.0068 - LNout314_loss: 2.6526 - LNout315_loss: 0.1979 - LNout316_loss: 0.7256 - LNout317_loss: 1.7906 - LNout318_loss: 1.5225 - LNout319_loss: 0.1866 - LNout320_loss: 0.4135 - LNout321_loss: 0.6041 - LNout322_loss: 1.5745 - LNout323_loss: 2.3113 - LNout324_loss: 0.7158 - LNout325_loss: 4.9473 - LNout326_loss: 2.4473 - LNout327_loss: 3.3388 - LNout328_loss: 2.8227 - LNout329_loss: 3.3857 - LNout330_loss: 0.2915 - LNout331_loss: 1.0338 - LNout332_loss: 0.4749 - LNout333_loss: 1.3759 - LNout334_loss: 1.3435 - LNout335_loss: 2.0446 - LNout336_loss: 0.8085 - LNout337_loss: 0.8158 - LNout338_loss: 1.2570 - LNout339_loss: 0.1866 - LNout340_loss: 0.7789 - LNout341_loss: 7.5543 - LNout342_loss: 1.1294 - LNout343_loss: 6.3785 - LNout344_loss: 3.3305 - LNout345_loss: 0.4373 - LNout346_loss: 0.3342 - LNout347_loss: 1.5427 - LNout348_loss: 1.0235 - LNout349_loss: 4.8616 - LNout350_loss: 8.5237 - LNout351_loss: 1.2220 - LNout352_loss: 1.9305 - LNout353_loss: 2.8730 - LNout354_loss: 0.6798 - LNout355_loss: 1.6753 - LNout356_loss: 2.6642 - LNout357_loss: 2.2638 - LNout358_loss: 1.0327 - LNout359_loss: 0.4696 - error_feed_forward_output1_binary_accuracy: 0.9616 - LNout0_categorical_accuracy: 0.0723 - LNout1_categorical_accuracy: 0.0355 - LNout2_categorical_accuracy: 0.1783 - LNout3_categorical_accuracy: 0.2776 - LNout4_categorical_accuracy: 0.2368 - LNout5_categorical_accuracy: 0.9281 - LNout6_categorical_accuracy: 0.6250 - LNout7_categorical_accuracy: 0.4259 - LNout8_categorical_accuracy: 0.3023 - LNout9_categorical_accuracy: 0.9377 - LNout10_categorical_accuracy: 0.6320 - LNout11_categorical_accuracy: 0.5603 - LNout12_categorical_accuracy: 0.1743 - LNout13_categorical_accuracy: 0.0000e+00 - LNout14_categorical_accuracy: 0.2550 - LNout15_categorical_accuracy: 0.9926 - LNout16_categorical_accuracy: 6.0096e-04 - LNout17_categorical_accuracy: 0.3313 - LNout18_categorical_accuracy: 0.6408 - LNout19_categorical_accuracy: 0.3530 - LNout20_categorical_accuracy: 0.8622 - LNout21_categorical_accuracy: 0.8982 - LNout22_categorical_accuracy: 0.0000e+00 - LNout23_categorical_accuracy: 0.0000e+00 - LNout24_categorical_accuracy: 0.9960 - LNout25_categorical_accuracy: 0.9143 - LNout26_categorical_accuracy: 0.6548 - LNout27_categorical_accuracy: 0.9972 - LNout28_categorical_accuracy: 0.9988 - LNout29_categorical_accuracy: 0.9988 - LNout30_categorical_accuracy: 0.2051 - LNout31_categorical_accuracy: 0.9996 - LNout32_categorical_accuracy: 0.3081 - LNout33_categorical_accuracy: 0.9998 - LNout34_categorical_accuracy: 0.7446 - LNout35_categorical_accuracy: 0.0000e+00 - LNout36_categorical_accuracy: 0.2564 - LNout37_categorical_accuracy: 0.6056 - LNout38_categorical_accuracy: 0.0000e+00 - LNout39_categorical_accuracy: 0.6919 - LNout40_categorical_accuracy: 1.0000 - LNout41_categorical_accuracy: 0.3846 - LNout42_categorical_accuracy: 0.3351 - LNout43_categorical_accuracy: 1.0000 - LNout44_categorical_accuracy: 1.0000 - LNout45_categorical_accuracy: 0.0509 - LNout46_categorical_accuracy: 0.6154 - LNout47_categorical_accuracy: 0.0000e+00 - LNout48_categorical_accuracy: 0.4615 - LNout49_categorical_accuracy: 0.7943 - LNout50_categorical_accuracy: 0.3614 - LNout51_categorical_accuracy: 0.2049 - LNout52_categorical_accuracy: 0.6913 - LNout53_categorical_accuracy: 0.4677 - LNout54_categorical_accuracy: 0.6931 - LNout55_categorical_accuracy: 0.0831 - LNout56_categorical_accuracy: 0.7185 - LNout57_categorical_accuracy: 0.4870 - LNout58_categorical_accuracy: 0.0323 - LNout59_categorical_accuracy: 0.4944 - LNout60_categorical_accuracy: 0.8073 - LNout61_categorical_accuracy: 0.3742 - LNout62_categorical_accuracy: 0.3576 - LNout63_categorical_accuracy: 0.9263 - LNout64_categorical_accuracy: 0.0000e+00 - LNout65_categorical_accuracy: 0.4772 - LNout66_categorical_accuracy: 0.0000e+00 - LNout67_categorical_accuracy: 0.2722 - LNout68_categorical_accuracy: 0.8307 - LNout69_categorical_accuracy: 0.9700 - LNout70_categorical_accuracy: 0.9032 - LNout71_categorical_accuracy: 0.3562 - LNout72_categorical_accuracy: 0.0511 - LNout73_categorical_accuracy: 0.9876 - LNout74_categorical_accuracy: 0.3558 - LNout75_categorical_accuracy: 0.2029 - LNout76_categorical_accuracy: 0.0763 - LNout77_categorical_accuracy: 0.9912 - LNout78_categorical_accuracy: 0.5369 - LNout79_categorical_accuracy: 0.6404 - LNout80_categorical_accuracy: 0.9972 - LNout81_categorical_accuracy: 0.0341 - LNout82_categorical_accuracy: 0.7764 - LNout83_categorical_accuracy: 0.6068 - LNout84_categorical_accuracy: 0.9275 - LNout85_categorical_accuracy: 0.2051 - LNout86_categorical_accuracy: 0.9591 - LNout87_categorical_accuracy: 1.0000 - LNout88_categorical_accuracy: 1.0000 - LNout89_categorical_accuracy: 0.5389 - LNout90_categorical_accuracy: 1.0000 - LNout91_categorical_accuracy: 0.5397 - LNout92_categorical_accuracy: 0.9842 - LNout93_categorical_accuracy: 0.5593 - LNout94_categorical_accuracy: 0.9551 - LNout95_categorical_accuracy: 1.0000 - LNout96_categorical_accuracy: 0.1024 - LNout97_categorical_accuracy: 0.4904 - LNout98_categorical_accuracy: 0.5415 - LNout99_categorical_accuracy: 0.2061 - LNout100_categorical_accuracy: 0.3283 - LNout101_categorical_accuracy: 0.9521 - LNout102_categorical_accuracy: 0.0000e+00 - LNout103_categorical_accuracy: 0.2308 - LNout104_categorical_accuracy: 0.1026 - LNout105_categorical_accuracy: 0.1334 - LNout106_categorical_accuracy: 0.4591 - LNout107_categorical_accuracy: 0.7692 - LNout108_categorical_accuracy: 1.0000 - LNout109_categorical_accuracy: 1.0000 - LNout110_categorical_accuracy: 0.5681 - LNout111_categorical_accuracy: 0.9744 - LNout112_categorical_accuracy: 0.8786 - LNout113_categorical_accuracy: 0.7532 - LNout114_categorical_accuracy: 0.4223 - LNout115_categorical_accuracy: 0.7662 - LNout116_categorical_accuracy: 0.6054 - LNout117_categorical_accuracy: 1.0000 - LNout118_categorical_accuracy: 0.4591 - LNout119_categorical_accuracy: 0.8197 - LNout120_categorical_accuracy: 0.9071 - LNout121_categorical_accuracy: 0.5262 - LNout122_categorical_accuracy: 0.7240 - LNout123_categorical_accuracy: 0.1975 - LNout124_categorical_accuracy: 0.1496 - LNout125_categorical_accuracy: 0.6681 - LNout126_categorical_accuracy: 0.5897 - LNout127_categorical_accuracy: 0.9946 - LNout128_categorical_accuracy: 0.8440 - LNout129_categorical_accuracy: 0.6420 - LNout130_categorical_accuracy: 0.0000e+00 - LNout131_categorical_accuracy: 0.0000e+00 - LNout132_categorical_accuracy: 1.0000 - LNout133_categorical_accuracy: 0.6669 - LNout134_categorical_accuracy: 0.2810 - LNout135_categorical_accuracy: 0.6657 - LNout136_categorical_accuracy: 0.5425 - LNout137_categorical_accuracy: 0.9932 - LNout138_categorical_accuracy: 0.1036 - LNout139_categorical_accuracy: 0.8053 - LNout140_categorical_accuracy: 1.0000 - LNout141_categorical_accuracy: 0.2310 - LNout142_categorical_accuracy: 0.5625 - LNout143_categorical_accuracy: 0.4613 - LNout144_categorical_accuracy: 1.0000 - LNout145_categorical_accuracy: 0.5128 - LNout146_categorical_accuracy: 0.1542 - LNout147_categorical_accuracy: 0.4593 - LNout148_categorical_accuracy: 1.0000 - LNout149_categorical_accuracy: 0.2821 - LNout150_categorical_accuracy: 0.5968 - LNout151_categorical_accuracy: 0.1216 - LNout152_categorical_accuracy: 0.2712 - LNout153_categorical_accuracy: 0.5962 - LNout154_categorical_accuracy: 0.9002 - LNout155_categorical_accuracy: 0.5645 - LNout156_categorical_accuracy: 0.1032 - LNout157_categorical_accuracy: 0.9998 - LNout158_categorical_accuracy: 0.1232 - LNout159_categorical_accuracy: 1.0000 - LNout160_categorical_accuracy: 1.0000 - LNout161_categorical_accuracy: 0.0000e+00 - LNout162_categorical_accuracy: 0.0769 - LNout163_categorical_accuracy: 0.2051 - LNout164_categorical_accuracy: 0.4101 - LNout165_categorical_accuracy: 0.6206 - LNout166_categorical_accuracy: 0.4910 - LNout167_categorical_accuracy: 0.5693 - LNout168_categorical_accuracy: 0.0000e+00 - LNout169_categorical_accuracy: 0.7955 - LNout170_categorical_accuracy: 0.9645 - LNout171_categorical_accuracy: 0.1795 - LNout172_categorical_accuracy: 0.0000e+00 - LNout173_categorical_accuracy: 1.0000 - LNout174_categorical_accuracy: 0.3976 - LNout175_categorical_accuracy: 0.2047 - LNout176_categorical_accuracy: 1.0000 - LNout177_categorical_accuracy: 0.9665 - LNout178_categorical_accuracy: 0.8966 - LNout179_categorical_accuracy: 0.6901 - LNout180_categorical_accuracy: 0.2660 - LNout181_categorical_accuracy: 0.2420 - LNout182_categorical_accuracy: 0.3205 - LNout183_categorical_accuracy: 0.1520 - LNout184_categorical_accuracy: 0.9361 - LNout185_categorical_accuracy: 0.5156 - LNout186_categorical_accuracy: 0.6132 - LNout187_categorical_accuracy: 0.2528 - LNout188_categorical_accuracy: 0.4824 - LNout189_categorical_accuracy: 0.7843 - LNout190_categorical_accuracy: 0.9974 - LNout191_categorical_accuracy: 0.7887 - LNout192_categorical_accuracy: 0.9736 - LNout193_categorical_accuracy: 0.3652 - LNout194_categorical_accuracy: 0.2933 - LNout195_categorical_accuracy: 0.0000e+00 - LNout196_categorical_accuracy: 0.2003 - LNout197_categorical_accuracy: 0.2566 - LNout198_categorical_accuracy: 0.6597 - LNout199_categorical_accuracy: 1.0000 - LNout200_categorical_accuracy: 0.3339 - LNout201_categorical_accuracy: 0.0509 - LNout202_categorical_accuracy: 0.1540 - LNout203_categorical_accuracy: 0.3498 - LNout204_categorical_accuracy: 0.3077 - LNout205_categorical_accuracy: 1.0000 - LNout206_categorical_accuracy: 0.1805 - LNout207_categorical_accuracy: 0.3337 - LNout208_categorical_accuracy: 0.6953 - LNout209_categorical_accuracy: 0.3808 - LNout210_categorical_accuracy: 0.0000e+00 - LNout211_categorical_accuracy: 0.5903 - LNout212_categorical_accuracy: 0.2264 - LNout213_categorical_accuracy: 0.6905 - LNout214_categorical_accuracy: 0.2540 - LNout215_categorical_accuracy: 0.6641 - LNout216_categorical_accuracy: 0.8454 - LNout217_categorical_accuracy: 0.0000e+00 - LNout218_categorical_accuracy: 0.5675 - LNout219_categorical_accuracy: 0.5401 - LNout220_categorical_accuracy: 0.6971 - LNout221_categorical_accuracy: 0.0721 - LNout222_categorical_accuracy: 1.0000 - LNout223_categorical_accuracy: 0.9786 - LNout224_categorical_accuracy: 0.3155 - LNout225_categorical_accuracy: 1.0000 - LNout226_categorical_accuracy: 0.3295 - LNout227_categorical_accuracy: 0.2640 - LNout228_categorical_accuracy: 0.6715 - LNout229_categorical_accuracy: 1.0000 - LNout230_categorical_accuracy: 0.7364 - LNout231_categorical_accuracy: 0.6152 - LNout232_categorical_accuracy: 0.7005 - LNout233_categorical_accuracy: 0.7634 - LNout234_categorical_accuracy: 0.0000e+00 - LNout235_categorical_accuracy: 0.0529 - LNout236_categorical_accuracy: 1.0000 - LNout237_categorical_accuracy: 0.3067 - LNout238_categorical_accuracy: 1.0000 - LNout239_categorical_accuracy: 0.2542 - LNout240_categorical_accuracy: 0.9633 - LNout241_categorical_accuracy: 0.9427 - LNout242_categorical_accuracy: 0.0000e+00 - LNout243_categorical_accuracy: 0.6076 - LNout244_categorical_accuracy: 0.9938 - LNout245_categorical_accuracy: 0.5623 - LNout246_categorical_accuracy: 0.2550 - LNout247_categorical_accuracy: 0.0000e+00 - LNout248_categorical_accuracy: 0.5120 - LNout249_categorical_accuracy: 0.1705 - LNout250_categorical_accuracy: 0.0000e+00 - LNout251_categorical_accuracy: 0.5319 - LNout252_categorical_accuracy: 0.9998 - LNout253_categorical_accuracy: 0.5395 - LNout254_categorical_accuracy: 0.6410 - LNout255_categorical_accuracy: 0.0000e+00 - LNout256_categorical_accuracy: 0.8688 - LNout257_categorical_accuracy: 0.7985 - LNout258_categorical_accuracy: 0.9093 - LNout259_categorical_accuracy: 0.5086 - LNout260_categorical_accuracy: 1.0000 - LNout261_categorical_accuracy: 0.3772 - LNout262_categorical_accuracy: 0.4716 - LNout263_categorical_accuracy: 1.0000 - LNout264_categorical_accuracy: 0.9742 - LNout265_categorical_accuracy: 0.2654 - LNout266_categorical_accuracy: 0.9401 - LNout267_categorical_accuracy: 1.0000 - LNout268_categorical_accuracy: 0.8548 - LNout269_categorical_accuracy: 0.6701 - LNout270_categorical_accuracy: 1.0000 - LNout271_categorical_accuracy: 0.0000e+00 - LNout272_categorical_accuracy: 0.4872 - LNout273_categorical_accuracy: 0.5040 - LNout274_categorical_accuracy: 1.0000 - LNout275_categorical_accuracy: 0.9938 - LNout276_categorical_accuracy: 0.5122 - LNout277_categorical_accuracy: 0.8019 - LNout278_categorical_accuracy: 0.7592 - LNout279_categorical_accuracy: 0.8684 - LNout280_categorical_accuracy: 0.5978 - LNout281_categorical_accuracy: 0.4231 - LNout282_categorical_accuracy: 0.4748 - LNout283_categorical_accuracy: 0.3215 - LNout284_categorical_accuracy: 1.0000 - LNout285_categorical_accuracy: 0.2308 - LNout286_categorical_accuracy: 0.8225 - LNout287_categorical_accuracy: 1.0000 - LNout288_categorical_accuracy: 0.0000e+00 - LNout289_categorical_accuracy: 0.0000e+00 - LNout290_categorical_accuracy: 0.2815 - LNout291_categorical_accuracy: 1.0000 - LNout292_categorical_accuracy: 0.4097 - LNout293_categorical_accuracy: 0.8263 - LNout294_categorical_accuracy: 0.0769 - LNout295_categorical_accuracy: 0.8624 - LNout296_categorical_accuracy: 0.9159 - LNout297_categorical_accuracy: 0.3896 - LNout298_categorical_accuracy: 1.0000 - LNout299_categorical_accuracy: 0.6394 - LNout300_categorical_accuracy: 0.9511 - LNout301_categorical_accuracy: 0.6058 - LNout302_categorical_accuracy: 0.2308 - LNout303_categorical_accuracy: 0.3163 - LNout304_categorical_accuracy: 0.4089 - LNout305_categorical_accuracy: 0.5929 - LNout306_categorical_accuracy: 0.5296 - LNout307_categorical_accuracy: 0.5385 - LNout308_categorical_accuracy: 0.8478 - LNout309_categorical_accuracy: 0.2414 - LNout310_categorical_accuracy: 0.0000e+00 - LNout311_categorical_accuracy: 0.2552 - LNout312_categorical_accuracy: 0.9621 - LNout313_categorical_accuracy: 1.0000 - LNout314_categorical_accuracy: 0.2812 - LNout315_categorical_accuracy: 1.0000 - LNout316_categorical_accuracy: 1.0000 - LNout317_categorical_accuracy: 0.4926 - LNout318_categorical_accuracy: 0.5641 - LNout319_categorical_accuracy: 1.0000 - LNout320_categorical_accuracy: 0.8980 - LNout321_categorical_accuracy: 0.8724 - LNout322_categorical_accuracy: 0.6695 - LNout323_categorical_accuracy: 0.2921 - LNout324_categorical_accuracy: 0.6973 - LNout325_categorical_accuracy: 0.0000e+00 - LNout326_categorical_accuracy: 0.3075 - LNout327_categorical_accuracy: 0.1697 - LNout328_categorical_accuracy: 0.0990 - LNout329_categorical_accuracy: 0.1398 - LNout330_categorical_accuracy: 1.0000 - LNout331_categorical_accuracy: 0.7492 - LNout332_categorical_accuracy: 1.0000 - LNout333_categorical_accuracy: 0.5813 - LNout334_categorical_accuracy: 0.5383 - LNout335_categorical_accuracy: 0.4361 - LNout336_categorical_accuracy: 0.7895 - LNout337_categorical_accuracy: 0.9449 - LNout338_categorical_accuracy: 0.5447 - LNout339_categorical_accuracy: 1.0000 - LNout340_categorical_accuracy: 0.8446 - LNout341_categorical_accuracy: 0.0000e+00 - LNout342_categorical_accuracy: 0.6378 - LNout343_categorical_accuracy: 0.0000e+00 - LNout344_categorical_accuracy: 0.3331 - LNout345_categorical_accuracy: 1.0000 - LNout346_categorical_accuracy: 1.0000 - LNout347_categorical_accuracy: 0.4103 - LNout348_categorical_accuracy: 0.7766 - LNout349_categorical_accuracy: 0.0000e+00 - LNout350_categorical_accuracy: 0.0000e+00 - LNout351_categorical_accuracy: 0.6302 - LNout352_categorical_accuracy: 0.2853 - LNout353_categorical_accuracy: 0.0887 - LNout354_categorical_accuracy: 0.6388 - LNout355_categorical_accuracy: 0.4103 - LNout356_categorical_accuracy: 0.2430 - LNout357_categorical_accuracy: 0.3361 - LNout358_categorical_accuracy: 0.7973 - LNout359_categorical_accuracy: 1.0000 - val_loss: 137.8566 - val_error_feed_forward_output1_loss: 0.1634 - val_LNout0_loss: 3.5221 - val_LNout1_loss: 4.8140 - val_LNout2_loss: 1.2265 - val_LNout3_loss: 1.1952 - val_LNout4_loss: 0.6008 - val_LNout5_loss: 1.0048 - val_LNout6_loss: 0.4212 - val_LNout7_loss: 0.2625 - val_LNout8_loss: 0.2558 - val_LNout9_loss: 0.3084 - val_LNout10_loss: 0.2084 - val_LNout11_loss: 0.1958 - val_LNout12_loss: 0.4201 - val_LNout13_loss: 3.1045 - val_LNout14_loss: 0.2027 - val_LNout15_loss: 0.1332 - val_LNout16_loss: 3.4576 - val_LNout17_loss: 0.1592 - val_LNout18_loss: 0.1113 - val_LNout19_loss: 0.1404 - val_LNout20_loss: 0.1126 - val_LNout21_loss: 0.1288 - val_LNout22_loss: 0.9123 - val_LNout23_loss: 0.7755 - val_LNout24_loss: 0.1094 - val_LNout25_loss: 0.0933 - val_LNout26_loss: 0.0993 - val_LNout27_loss: 0.1104 - val_LNout28_loss: 0.0286 - val_LNout29_loss: 0.0345 - val_LNout30_loss: 0.1545 - val_LNout31_loss: 0.0301 - val_LNout32_loss: 0.0769 - val_LNout33_loss: 0.0351 - val_LNout34_loss: 0.0019 - val_LNout35_loss: 1.8411 - val_LNout36_loss: 0.1293 - val_LNout37_loss: 0.0051 - val_LNout38_loss: 3.1706 - val_LNout39_loss: 0.0024 - val_LNout40_loss: 5.6035e-04 - val_LNout41_loss: 0.0158 - val_LNout42_loss: 0.0536 - val_LNout43_loss: 0.0011 - val_LNout44_loss: 0.0018 - val_LNout45_loss: 0.6004 - val_LNout46_loss: 0.0043 - val_LNout47_loss: 1.3685 - val_LNout48_loss: 0.0282 - val_LNout49_loss: 5.4806e-04 - val_LNout50_loss: 0.0313 - val_LNout51_loss: 0.1465 - val_LNout52_loss: 0.0128 - val_LNout53_loss: 0.0146 - val_LNout54_loss: 0.0059 - val_LNout55_loss: 0.4771 - val_LNout56_loss: 0.0077 - val_LNout57_loss: 0.0136 - val_LNout58_loss: 0.4965 - val_LNout59_loss: 0.0147 - val_LNout60_loss: 1.7568 - val_LNout61_loss: 1.5848 - val_LNout62_loss: 0.7586 - val_LNout63_loss: 0.9982 - val_LNout64_loss: 3.0449 - val_LNout65_loss: 0.4916 - val_LNout66_loss: 6.7625 - val_LNout67_loss: 0.3622 - val_LNout68_loss: 0.3505 - val_LNout69_loss: 0.4450 - val_LNout70_loss: 0.2045 - val_LNout71_loss: 0.2177 - val_LNout72_loss: 0.7311 - val_LNout73_loss: 0.1885 - val_LNout74_loss: 0.1580 - val_LNout75_loss: 0.2644 - val_LNout76_loss: 0.7655 - val_LNout77_loss: 0.1482 - val_LNout78_loss: 0.1005 - val_LNout79_loss: 0.1259 - val_LNout80_loss: 0.0344 - val_LNout81_loss: 0.8191 - val_LNout82_loss: 0.0293 - val_LNout83_loss: 0.0384 - val_LNout84_loss: 0.0025 - val_LNout85_loss: 0.1495 - val_LNout86_loss: 0.0027 - val_LNout87_loss: 4.4316e-04 - val_LNout88_loss: 1.3439e-04 - val_LNout89_loss: 0.0093 - val_LNout90_loss: 2.1007e-04 - val_LNout91_loss: 0.0105 - val_LNout92_loss: 0.0025 - val_LNout93_loss: 0.0104 - val_LNout94_loss: 0.0011 - val_LNout95_loss: 8.0863e-05 - val_LNout96_loss: 0.2654 - val_LNout97_loss: 0.0083 - val_LNout98_loss: 0.0203 - val_LNout99_loss: 0.0950 - val_LNout100_loss: 0.0470 - val_LNout101_loss: 4.9444e-04 - val_LNout102_loss: 1.6138 - val_LNout103_loss: 0.2744 - val_LNout104_loss: 0.2620 - val_LNout105_loss: 0.3264 - val_LNout106_loss: 0.0269 - val_LNout107_loss: 0.0035 - val_LNout108_loss: 1.8027e-04 - val_LNout109_loss: 4.4689e-04 - val_LNout110_loss: 0.0140 - val_LNout111_loss: 7.0703e-04 - val_LNout112_loss: 0.0022 - val_LNout113_loss: 0.0016 - val_LNout114_loss: 0.0120 - val_LNout115_loss: 0.0089 - val_LNout116_loss: 0.0076 - val_LNout117_loss: 3.8277e-04 - val_LNout118_loss: 0.0176 - val_LNout119_loss: 0.0025 - val_LNout120_loss: 1.1140 - val_LNout121_loss: 0.9256 - val_LNout122_loss: 0.1929 - val_LNout123_loss: 0.3677 - val_LNout124_loss: 0.6012 - val_LNout125_loss: 0.2436 - val_LNout126_loss: 0.0054 - val_LNout127_loss: 7.4139e-04 - val_LNout128_loss: 0.0016 - val_LNout129_loss: 0.0065 - val_LNout130_loss: 2.0006 - val_LNout131_loss: 2.5044 - val_LNout132_loss: 6.3938e-04 - val_LNout133_loss: 0.0029 - val_LNout134_loss: 0.0813 - val_LNout135_loss: 0.0050 - val_LNout136_loss: 0.0048 - val_LNout137_loss: 0.0019 - val_LNout138_loss: 0.2188 - val_LNout139_loss: 0.0033 - val_LNout140_loss: 1.1626e-04 - val_LNout141_loss: 0.1174 - val_LNout142_loss: 0.0125 - val_LNout143_loss: 0.0295 - val_LNout144_loss: 4.6413e-04 - val_LNout145_loss: 0.0119 - val_LNout146_loss: 0.1900 - val_LNout147_loss: 0.0182 - val_LNout148_loss: 3.0879e-04 - val_LNout149_loss: 0.0452 - val_LNout150_loss: 0.0085 - val_LNout151_loss: 0.3787 - val_LNout152_loss: 0.1368 - val_LNout153_loss: 0.0204 - val_LNout154_loss: 5.0189e-04 - val_LNout155_loss: 0.0086 - val_LNout156_loss: 0.2289 - val_LNout157_loss: 3.9044e-04 - val_LNout158_loss: 0.7541 - val_LNout159_loss: 6.1069e-04 - val_LNout160_loss: 3.5759e-04 - val_LNout161_loss: 1.9911 - val_LNout162_loss: 0.4160 - val_LNout163_loss: 0.1891 - val_LNout164_loss: 0.0436 - val_LNout165_loss: 0.0029 - val_LNout166_loss: 0.0097 - val_LNout167_loss: 0.0094 - val_LNout168_loss: 1.3023 - val_LNout169_loss: 0.0027 - val_LNout170_loss: 0.0012 - val_LNout171_loss: 0.0954 - val_LNout172_loss: 1.8356 - val_LNout173_loss: 1.0493e-04 - val_LNout174_loss: 0.0320 - val_LNout175_loss: 0.1260 - val_LNout176_loss: 3.1997e-04 - val_LNout177_loss: 7.2312e-04 - val_LNout178_loss: 0.0022 - val_LNout179_loss: 0.0024 - val_LNout180_loss: 0.5645 - val_LNout181_loss: 0.5871 - val_LNout182_loss: 0.3540 - val_LNout183_loss: 0.4595 - val_LNout184_loss: 0.4158 - val_LNout185_loss: 0.3345 - val_LNout186_loss: 0.1927 - val_LNout187_loss: 0.1736 - val_LNout188_loss: 0.1449 - val_LNout189_loss: 0.1453 - val_LNout190_loss: 0.0387 - val_LNout191_loss: 0.0326 - val_LNout192_loss: 0.0025 - val_LNout193_loss: 0.0558 - val_LNout194_loss: 0.0537 - val_LNout195_loss: 3.6582 - val_LNout196_loss: 0.5174 - val_LNout197_loss: 0.0903 - val_LNout198_loss: 0.0079 - val_LNout199_loss: 2.3404e-04 - val_LNout200_loss: 0.0513 - val_LNout201_loss: 0.7480 - val_LNout202_loss: 0.1733 - val_LNout203_loss: 0.0240 - val_LNout204_loss: 0.0507 - val_LNout205_loss: 0.0016 - val_LNout206_loss: 0.1409 - val_LNout207_loss: 0.0436 - val_LNout208_loss: 0.0094 - val_LNout209_loss: 0.0438 - val_LNout210_loss: 0.6984 - val_LNout211_loss: 0.0085 - val_LNout212_loss: 0.0951 - val_LNout213_loss: 0.0082 - val_LNout214_loss: 0.0765 - val_LNout215_loss: 0.0060 - val_LNout216_loss: 0.0028 - val_LNout217_loss: 6.1970 - val_LNout218_loss: 0.0047 - val_LNout219_loss: 0.0073 - val_LNout220_loss: 0.0124 - val_LNout221_loss: 0.3434 - val_LNout222_loss: 0.0015 - val_LNout223_loss: 5.6411e-04 - val_LNout224_loss: 0.0343 - val_LNout225_loss: 3.1202e-05 - val_LNout226_loss: 0.0721 - val_LNout227_loss: 0.1641 - val_LNout228_loss: 0.0060 - val_LNout229_loss: 8.7251e-04 - val_LNout230_loss: 0.0065 - val_LNout231_loss: 0.0089 - val_LNout232_loss: 0.0084 - val_LNout233_loss: 0.0018 - val_LNout234_loss: 2.0211 - val_LNout235_loss: 0.9947 - val_LNout236_loss: 8.6988e-04 - val_LNout237_loss: 0.0966 - val_LNout238_loss: 2.9581e-06 - val_LNout239_loss: 0.1500 - val_LNout240_loss: 0.3758 - val_LNout241_loss: 0.2889 - val_LNout242_loss: 1.3053 - val_LNout243_loss: 0.0950 - val_LNout244_loss: 0.1063 - val_LNout245_loss: 0.1271 - val_LNout246_loss: 0.1752 - val_LNout247_loss: 6.0032 - val_LNout248_loss: 0.1000 - val_LNout249_loss: 0.4197 - val_LNout250_loss: 3.0368 - val_LNout251_loss: 0.0626 - val_LNout252_loss: 0.0011 - val_LNout253_loss: 0.0082 - val_LNout254_loss: 0.0036 - val_LNout255_loss: 1.0415 - val_LNout256_loss: 6.4861e-04 - val_LNout257_loss: 0.0033 - val_LNout258_loss: 6.1563e-04 - val_LNout259_loss: 0.0069 - val_LNout260_loss: 0.0013 - val_LNout261_loss: 0.0359 - val_LNout262_loss: 0.0117 - val_LNout263_loss: 4.4687e-04 - val_LNout264_loss: 0.0013 - val_LNout265_loss: 0.0865 - val_LNout266_loss: 5.9385e-04 - val_LNout267_loss: 6.1085e-04 - val_LNout268_loss: 0.0013 - val_LNout269_loss: 0.0065 - val_LNout270_loss: 5.8289e-04 - val_LNout271_loss: 4.2513 - val_LNout272_loss: 0.0161 - val_LNout273_loss: 0.0310 - val_LNout274_loss: 0.0013 - val_LNout275_loss: 0.0019 - val_LNout276_loss: 0.0044 - val_LNout277_loss: 0.0074 - val_LNout278_loss: 0.0011 - val_LNout279_loss: 0.0025 - val_LNout280_loss: 0.0033 - val_LNout281_loss: 0.0308 - val_LNout282_loss: 0.0235 - val_LNout283_loss: 0.0337 - val_LNout284_loss: 1.5059e-04 - val_LNout285_loss: 0.1365 - val_LNout286_loss: 0.0021 - val_LNout287_loss: 2.3363e-04 - val_LNout288_loss: 1.0526 - val_LNout289_loss: 1.9152 - val_LNout290_loss: 0.1481 - val_LNout291_loss: 7.8438e-04 - val_LNout292_loss: 0.0302 - val_LNout293_loss: 0.0026 - val_LNout294_loss: 0.4331 - val_LNout295_loss: 0.0015 - val_LNout296_loss: 0.0031 - val_LNout297_loss: 0.0334 - val_LNout298_loss: 3.8137e-04 - val_LNout299_loss: 0.0024 - val_LNout300_loss: 0.1154 - val_LNout301_loss: 0.0776 - val_LNout302_loss: 0.0777 - val_LNout303_loss: 0.0744 - val_LNout304_loss: 0.0678 - val_LNout305_loss: 0.0126 - val_LNout306_loss: 0.0093 - val_LNout307_loss: 0.0131 - val_LNout308_loss: 9.9985e-04 - val_LNout309_loss: 0.0776 - val_LNout310_loss: 3.3205 - val_LNout311_loss: 0.1431 - val_LNout312_loss: 0.0018 - val_LNout313_loss: 3.6065e-07 - val_LNout314_loss: 0.0658 - val_LNout315_loss: 3.9370e-05 - val_LNout316_loss: 4.7569e-04 - val_LNout317_loss: 0.0091 - val_LNout318_loss: 0.0116 - val_LNout319_loss: 1.4653e-04 - val_LNout320_loss: 7.5726e-04 - val_LNout321_loss: 0.0023 - val_LNout322_loss: 0.0093 - val_LNout323_loss: 0.0354 - val_LNout324_loss: 0.0027 - val_LNout325_loss: 1.4909 - val_LNout326_loss: 0.0679 - val_LNout327_loss: 0.1998 - val_LNout328_loss: 0.2772 - val_LNout329_loss: 0.1885 - val_LNout330_loss: 7.9571e-04 - val_LNout331_loss: 0.0039 - val_LNout332_loss: 7.9446e-04 - val_LNout333_loss: 0.0040 - val_LNout334_loss: 0.0079 - val_LNout335_loss: 0.0085 - val_LNout336_loss: 0.0015 - val_LNout337_loss: 0.0017 - val_LNout338_loss: 0.0088 - val_LNout339_loss: 8.3178e-05 - val_LNout340_loss: 6.1004e-04 - val_LNout341_loss: 4.9084 - val_LNout342_loss: 0.0072 - val_LNout343_loss: 3.2598 - val_LNout344_loss: 0.0677 - val_LNout345_loss: 0.0016 - val_LNout346_loss: 2.7503e-04 - val_LNout347_loss: 0.0146 - val_LNout348_loss: 0.0018 - val_LNout349_loss: 1.2726 - val_LNout350_loss: 6.8316 - val_LNout351_loss: 0.0106 - val_LNout352_loss: 0.0511 - val_LNout353_loss: 0.3045 - val_LNout354_loss: 0.0106 - val_LNout355_loss: 0.0134 - val_LNout356_loss: 0.0837 - val_LNout357_loss: 0.0524 - val_LNout358_loss: 0.0018 - val_LNout359_loss: 5.8507e-04 - val_error_feed_forward_output1_binary_accuracy: 0.9613 - val_LNout0_categorical_accuracy: 0.0859 - val_LNout1_categorical_accuracy: 0.0918 - val_LNout2_categorical_accuracy: 0.8164 - val_LNout3_categorical_accuracy: 0.8164 - val_LNout4_categorical_accuracy: 0.9355 - val_LNout5_categorical_accuracy: 0.9355 - val_LNout6_categorical_accuracy: 0.9746 - val_LNout7_categorical_accuracy: 0.9746 - val_LNout8_categorical_accuracy: 0.9785 - val_LNout9_categorical_accuracy: 0.9785 - val_LNout10_categorical_accuracy: 0.9863 - val_LNout11_categorical_accuracy: 0.9863 - val_LNout12_categorical_accuracy: 0.9883 - val_LNout13_categorical_accuracy: 0.0000e+00 - val_LNout14_categorical_accuracy: 0.9922 - val_LNout15_categorical_accuracy: 0.9922 - val_LNout16_categorical_accuracy: 0.0000e+00 - val_LNout17_categorical_accuracy: 0.9922 - val_LNout18_categorical_accuracy: 0.9922 - val_LNout19_categorical_accuracy: 0.9922 - val_LNout20_categorical_accuracy: 0.9922 - val_LNout21_categorical_accuracy: 0.9922 - val_LNout22_categorical_accuracy: 0.0000e+00 - val_LNout23_categorical_accuracy: 0.9922 - val_LNout24_categorical_accuracy: 0.9941 - val_LNout25_categorical_accuracy: 0.9941 - val_LNout26_categorical_accuracy: 0.9941 - val_LNout27_categorical_accuracy: 0.9941 - val_LNout28_categorical_accuracy: 0.9980 - val_LNout29_categorical_accuracy: 0.9980 - val_LNout30_categorical_accuracy: 0.9980 - val_LNout31_categorical_accuracy: 0.9980 - val_LNout32_categorical_accuracy: 0.9980 - val_LNout33_categorical_accuracy: 0.9980 - val_LNout34_categorical_accuracy: 1.0000 - val_LNout35_categorical_accuracy: 0.0000e+00 - val_LNout36_categorical_accuracy: 1.0000 - val_LNout37_categorical_accuracy: 1.0000 - val_LNout38_categorical_accuracy: 0.0000e+00 - val_LNout39_categorical_accuracy: 1.0000 - val_LNout40_categorical_accuracy: 1.0000 - val_LNout41_categorical_accuracy: 1.0000 - val_LNout42_categorical_accuracy: 1.0000 - val_LNout43_categorical_accuracy: 1.0000 - val_LNout44_categorical_accuracy: 1.0000 - val_LNout45_categorical_accuracy: 1.0000 - val_LNout46_categorical_accuracy: 1.0000 - val_LNout47_categorical_accuracy: 0.9902 - val_LNout48_categorical_accuracy: 1.0000 - val_LNout49_categorical_accuracy: 1.0000 - val_LNout50_categorical_accuracy: 1.0000 - val_LNout51_categorical_accuracy: 1.0000 - val_LNout52_categorical_accuracy: 1.0000 - val_LNout53_categorical_accuracy: 1.0000 - val_LNout54_categorical_accuracy: 1.0000 - val_LNout55_categorical_accuracy: 1.0000 - val_LNout56_categorical_accuracy: 1.0000 - val_LNout57_categorical_accuracy: 1.0000 - val_LNout58_categorical_accuracy: 1.0000 - val_LNout59_categorical_accuracy: 1.0000 - val_LNout60_categorical_accuracy: 0.7969 - val_LNout61_categorical_accuracy: 0.7969 - val_LNout62_categorical_accuracy: 0.9219 - val_LNout63_categorical_accuracy: 0.9219 - val_LNout64_categorical_accuracy: 0.0000e+00 - val_LNout65_categorical_accuracy: 0.9551 - val_LNout66_categorical_accuracy: 0.0000e+00 - val_LNout67_categorical_accuracy: 0.9707 - val_LNout68_categorical_accuracy: 0.9727 - val_LNout69_categorical_accuracy: 0.9727 - val_LNout70_categorical_accuracy: 0.9844 - val_LNout71_categorical_accuracy: 0.9844 - val_LNout72_categorical_accuracy: 0.9863 - val_LNout73_categorical_accuracy: 0.9863 - val_LNout74_categorical_accuracy: 0.9863 - val_LNout75_categorical_accuracy: 0.9863 - val_LNout76_categorical_accuracy: 0.9902 - val_LNout77_categorical_accuracy: 0.9902 - val_LNout78_categorical_accuracy: 0.9922 - val_LNout79_categorical_accuracy: 0.9922 - val_LNout80_categorical_accuracy: 0.9980 - val_LNout81_categorical_accuracy: 0.9980 - val_LNout82_categorical_accuracy: 0.9980 - val_LNout83_categorical_accuracy: 0.9980 - val_LNout84_categorical_accuracy: 1.0000 - val_LNout85_categorical_accuracy: 1.0000 - val_LNout86_categorical_accuracy: 1.0000 - val_LNout87_categorical_accuracy: 1.0000 - val_LNout88_categorical_accuracy: 1.0000 - val_LNout89_categorical_accuracy: 1.0000 - val_LNout90_categorical_accuracy: 1.0000 - val_LNout91_categorical_accuracy: 1.0000 - val_LNout92_categorical_accuracy: 1.0000 - val_LNout93_categorical_accuracy: 1.0000 - val_LNout94_categorical_accuracy: 1.0000 - val_LNout95_categorical_accuracy: 1.0000 - val_LNout96_categorical_accuracy: 1.0000 - val_LNout97_categorical_accuracy: 1.0000 - val_LNout98_categorical_accuracy: 1.0000 - val_LNout99_categorical_accuracy: 1.0000 - val_LNout100_categorical_accuracy: 1.0000 - val_LNout101_categorical_accuracy: 1.0000 - val_LNout102_categorical_accuracy: 0.0000e+00 - val_LNout103_categorical_accuracy: 1.0000 - val_LNout104_categorical_accuracy: 1.0000 - val_LNout105_categorical_accuracy: 1.0000 - val_LNout106_categorical_accuracy: 1.0000 - val_LNout107_categorical_accuracy: 1.0000 - val_LNout108_categorical_accuracy: 1.0000 - val_LNout109_categorical_accuracy: 1.0000 - val_LNout110_categorical_accuracy: 1.0000 - val_LNout111_categorical_accuracy: 1.0000 - val_LNout112_categorical_accuracy: 1.0000 - val_LNout113_categorical_accuracy: 1.0000 - val_LNout114_categorical_accuracy: 1.0000 - val_LNout115_categorical_accuracy: 1.0000 - val_LNout116_categorical_accuracy: 1.0000 - val_LNout117_categorical_accuracy: 1.0000 - val_LNout118_categorical_accuracy: 1.0000 - val_LNout119_categorical_accuracy: 1.0000 - val_LNout120_categorical_accuracy: 0.9023 - val_LNout121_categorical_accuracy: 0.9023 - val_LNout122_categorical_accuracy: 0.9727 - val_LNout123_categorical_accuracy: 0.9727 - val_LNout124_categorical_accuracy: 0.9727 - val_LNout125_categorical_accuracy: 0.9727 - val_LNout126_categorical_accuracy: 1.0000 - val_LNout127_categorical_accuracy: 1.0000 - val_LNout128_categorical_accuracy: 1.0000 - val_LNout129_categorical_accuracy: 1.0000 - val_LNout130_categorical_accuracy: 0.0000e+00 - val_LNout131_categorical_accuracy: 0.0000e+00 - val_LNout132_categorical_accuracy: 1.0000 - val_LNout133_categorical_accuracy: 1.0000 - val_LNout134_categorical_accuracy: 1.0000 - val_LNout135_categorical_accuracy: 1.0000 - val_LNout136_categorical_accuracy: 1.0000 - val_LNout137_categorical_accuracy: 1.0000 - val_LNout138_categorical_accuracy: 1.0000 - val_LNout139_categorical_accuracy: 1.0000 - val_LNout140_categorical_accuracy: 1.0000 - val_LNout141_categorical_accuracy: 1.0000 - val_LNout142_categorical_accuracy: 1.0000 - val_LNout143_categorical_accuracy: 1.0000 - val_LNout144_categorical_accuracy: 1.0000 - val_LNout145_categorical_accuracy: 1.0000 - val_LNout146_categorical_accuracy: 1.0000 - val_LNout147_categorical_accuracy: 1.0000 - val_LNout148_categorical_accuracy: 1.0000 - val_LNout149_categorical_accuracy: 1.0000 - val_LNout150_categorical_accuracy: 1.0000 - val_LNout151_categorical_accuracy: 1.0000 - val_LNout152_categorical_accuracy: 1.0000 - val_LNout153_categorical_accuracy: 1.0000 - val_LNout154_categorical_accuracy: 1.0000 - val_LNout155_categorical_accuracy: 1.0000 - val_LNout156_categorical_accuracy: 1.0000 - val_LNout157_categorical_accuracy: 1.0000 - val_LNout158_categorical_accuracy: 1.0000 - val_LNout159_categorical_accuracy: 1.0000 - val_LNout160_categorical_accuracy: 1.0000 - val_LNout161_categorical_accuracy: 0.0000e+00 - val_LNout162_categorical_accuracy: 1.0000 - val_LNout163_categorical_accuracy: 1.0000 - val_LNout164_categorical_accuracy: 1.0000 - val_LNout165_categorical_accuracy: 1.0000 - val_LNout166_categorical_accuracy: 1.0000 - val_LNout167_categorical_accuracy: 1.0000 - val_LNout168_categorical_accuracy: 0.0000e+00 - val_LNout169_categorical_accuracy: 1.0000 - val_LNout170_categorical_accuracy: 1.0000 - val_LNout171_categorical_accuracy: 1.0000 - val_LNout172_categorical_accuracy: 0.0000e+00 - val_LNout173_categorical_accuracy: 1.0000 - val_LNout174_categorical_accuracy: 1.0000 - val_LNout175_categorical_accuracy: 1.0000 - val_LNout176_categorical_accuracy: 1.0000 - val_LNout177_categorical_accuracy: 1.0000 - val_LNout178_categorical_accuracy: 1.0000 - val_LNout179_categorical_accuracy: 1.0000 - val_LNout180_categorical_accuracy: 0.9355 - val_LNout181_categorical_accuracy: 0.9355 - val_LNout182_categorical_accuracy: 0.9688 - val_LNout183_categorical_accuracy: 0.9688 - val_LNout184_categorical_accuracy: 0.9707 - val_LNout185_categorical_accuracy: 0.9707 - val_LNout186_categorical_accuracy: 0.9844 - val_LNout187_categorical_accuracy: 0.9844 - val_LNout188_categorical_accuracy: 0.9863 - val_LNout189_categorical_accuracy: 0.9863 - val_LNout190_categorical_accuracy: 0.9980 - val_LNout191_categorical_accuracy: 0.9980 - val_LNout192_categorical_accuracy: 1.0000 - val_LNout193_categorical_accuracy: 1.0000 - val_LNout194_categorical_accuracy: 1.0000 - val_LNout195_categorical_accuracy: 0.0000e+00 - val_LNout196_categorical_accuracy: 1.0000 - val_LNout197_categorical_accuracy: 1.0000 - val_LNout198_categorical_accuracy: 1.0000 - val_LNout199_categorical_accuracy: 1.0000 - val_LNout200_categorical_accuracy: 1.0000 - val_LNout201_categorical_accuracy: 1.0000 - val_LNout202_categorical_accuracy: 1.0000 - val_LNout203_categorical_accuracy: 1.0000 - val_LNout204_categorical_accuracy: 1.0000 - val_LNout205_categorical_accuracy: 1.0000 - val_LNout206_categorical_accuracy: 1.0000 - val_LNout207_categorical_accuracy: 1.0000 - val_LNout208_categorical_accuracy: 1.0000 - val_LNout209_categorical_accuracy: 1.0000 - val_LNout210_categorical_accuracy: 1.0000 - val_LNout211_categorical_accuracy: 1.0000 - val_LNout212_categorical_accuracy: 1.0000 - val_LNout213_categorical_accuracy: 1.0000 - val_LNout214_categorical_accuracy: 1.0000 - val_LNout215_categorical_accuracy: 1.0000 - val_LNout216_categorical_accuracy: 1.0000 - val_LNout217_categorical_accuracy: 0.0000e+00 - val_LNout218_categorical_accuracy: 1.0000 - val_LNout219_categorical_accuracy: 1.0000 - val_LNout220_categorical_accuracy: 1.0000 - val_LNout221_categorical_accuracy: 1.0000 - val_LNout222_categorical_accuracy: 1.0000 - val_LNout223_categorical_accuracy: 1.0000 - val_LNout224_categorical_accuracy: 1.0000 - val_LNout225_categorical_accuracy: 1.0000 - val_LNout226_categorical_accuracy: 1.0000 - val_LNout227_categorical_accuracy: 1.0000 - val_LNout228_categorical_accuracy: 1.0000 - val_LNout229_categorical_accuracy: 1.0000 - val_LNout230_categorical_accuracy: 1.0000 - val_LNout231_categorical_accuracy: 1.0000 - val_LNout232_categorical_accuracy: 1.0000 - val_LNout233_categorical_accuracy: 1.0000 - val_LNout234_categorical_accuracy: 0.0000e+00 - val_LNout235_categorical_accuracy: 1.0000 - val_LNout236_categorical_accuracy: 1.0000 - val_LNout237_categorical_accuracy: 1.0000 - val_LNout238_categorical_accuracy: 1.0000 - val_LNout239_categorical_accuracy: 1.0000 - val_LNout240_categorical_accuracy: 0.9785 - val_LNout241_categorical_accuracy: 0.9785 - val_LNout242_categorical_accuracy: 0.0000e+00 - val_LNout243_categorical_accuracy: 0.9922 - val_LNout244_categorical_accuracy: 0.9922 - val_LNout245_categorical_accuracy: 0.9922 - val_LNout246_categorical_accuracy: 0.9922 - val_LNout247_categorical_accuracy: 0.0000e+00 - val_LNout248_categorical_accuracy: 0.9922 - val_LNout249_categorical_accuracy: 0.9922 - val_LNout250_categorical_accuracy: 0.0000e+00 - val_LNout251_categorical_accuracy: 0.9961 - val_LNout252_categorical_accuracy: 1.0000 - val_LNout253_categorical_accuracy: 1.0000 - val_LNout254_categorical_accuracy: 1.0000 - val_LNout255_categorical_accuracy: 0.0000e+00 - val_LNout256_categorical_accuracy: 1.0000 - val_LNout257_categorical_accuracy: 1.0000 - val_LNout258_categorical_accuracy: 1.0000 - val_LNout259_categorical_accuracy: 1.0000 - val_LNout260_categorical_accuracy: 1.0000 - val_LNout261_categorical_accuracy: 1.0000 - val_LNout262_categorical_accuracy: 1.0000 - val_LNout263_categorical_accuracy: 1.0000 - val_LNout264_categorical_accuracy: 1.0000 - val_LNout265_categorical_accuracy: 1.0000 - val_LNout266_categorical_accuracy: 1.0000 - val_LNout267_categorical_accuracy: 1.0000 - val_LNout268_categorical_accuracy: 1.0000 - val_LNout269_categorical_accuracy: 1.0000 - val_LNout270_categorical_accuracy: 1.0000 - val_LNout271_categorical_accuracy: 0.0000e+00 - val_LNout272_categorical_accuracy: 1.0000 - val_LNout273_categorical_accuracy: 1.0000 - val_LNout274_categorical_accuracy: 1.0000 - val_LNout275_categorical_accuracy: 1.0000 - val_LNout276_categorical_accuracy: 1.0000 - val_LNout277_categorical_accuracy: 1.0000 - val_LNout278_categorical_accuracy: 1.0000 - val_LNout279_categorical_accuracy: 1.0000 - val_LNout280_categorical_accuracy: 1.0000 - val_LNout281_categorical_accuracy: 1.0000 - val_LNout282_categorical_accuracy: 1.0000 - val_LNout283_categorical_accuracy: 1.0000 - val_LNout284_categorical_accuracy: 1.0000 - val_LNout285_categorical_accuracy: 1.0000 - val_LNout286_categorical_accuracy: 1.0000 - val_LNout287_categorical_accuracy: 1.0000 - val_LNout288_categorical_accuracy: 0.0000e+00 - val_LNout289_categorical_accuracy: 0.0000e+00 - val_LNout290_categorical_accuracy: 1.0000 - val_LNout291_categorical_accuracy: 1.0000 - val_LNout292_categorical_accuracy: 1.0000 - val_LNout293_categorical_accuracy: 1.0000 - val_LNout294_categorical_accuracy: 1.0000 - val_LNout295_categorical_accuracy: 1.0000 - val_LNout296_categorical_accuracy: 1.0000 - val_LNout297_categorical_accuracy: 1.0000 - val_LNout298_categorical_accuracy: 1.0000 - val_LNout299_categorical_accuracy: 1.0000 - val_LNout300_categorical_accuracy: 0.9922 - val_LNout301_categorical_accuracy: 0.9922 - val_LNout302_categorical_accuracy: 1.0000 - val_LNout303_categorical_accuracy: 1.0000 - val_LNout304_categorical_accuracy: 1.0000 - val_LNout305_categorical_accuracy: 1.0000 - val_LNout306_categorical_accuracy: 1.0000 - val_LNout307_categorical_accuracy: 1.0000 - val_LNout308_categorical_accuracy: 1.0000 - val_LNout309_categorical_accuracy: 1.0000 - val_LNout310_categorical_accuracy: 0.0000e+00 - val_LNout311_categorical_accuracy: 1.0000 - val_LNout312_categorical_accuracy: 1.0000 - val_LNout313_categorical_accuracy: 1.0000 - val_LNout314_categorical_accuracy: 1.0000 - val_LNout315_categorical_accuracy: 1.0000 - val_LNout316_categorical_accuracy: 1.0000 - val_LNout317_categorical_accuracy: 1.0000 - val_LNout318_categorical_accuracy: 1.0000 - val_LNout319_categorical_accuracy: 1.0000 - val_LNout320_categorical_accuracy: 1.0000 - val_LNout321_categorical_accuracy: 1.0000 - val_LNout322_categorical_accuracy: 1.0000 - val_LNout323_categorical_accuracy: 1.0000 - val_LNout324_categorical_accuracy: 1.0000 - val_LNout325_categorical_accuracy: 0.0000e+00 - val_LNout326_categorical_accuracy: 1.0000 - val_LNout327_categorical_accuracy: 1.0000 - val_LNout328_categorical_accuracy: 1.0000 - val_LNout329_categorical_accuracy: 1.0000 - val_LNout330_categorical_accuracy: 1.0000 - val_LNout331_categorical_accuracy: 1.0000 - val_LNout332_categorical_accuracy: 1.0000 - val_LNout333_categorical_accuracy: 1.0000 - val_LNout334_categorical_accuracy: 1.0000 - val_LNout335_categorical_accuracy: 1.0000 - val_LNout336_categorical_accuracy: 1.0000 - val_LNout337_categorical_accuracy: 1.0000 - val_LNout338_categorical_accuracy: 1.0000 - val_LNout339_categorical_accuracy: 1.0000 - val_LNout340_categorical_accuracy: 1.0000 - val_LNout341_categorical_accuracy: 0.0000e+00 - val_LNout342_categorical_accuracy: 1.0000 - val_LNout343_categorical_accuracy: 0.0000e+00 - val_LNout344_categorical_accuracy: 1.0000 - val_LNout345_categorical_accuracy: 1.0000 - val_LNout346_categorical_accuracy: 1.0000 - val_LNout347_categorical_accuracy: 1.0000 - val_LNout348_categorical_accuracy: 1.0000 - val_LNout349_categorical_accuracy: 0.0000e+00 - val_LNout350_categorical_accuracy: 0.0000e+00 - val_LNout351_categorical_accuracy: 1.0000 - val_LNout352_categorical_accuracy: 1.0000 - val_LNout353_categorical_accuracy: 1.0000 - val_LNout354_categorical_accuracy: 1.0000 - val_LNout355_categorical_accuracy: 1.0000 - val_LNout356_categorical_accuracy: 1.0000 - val_LNout357_categorical_accuracy: 1.0000 - val_LNout358_categorical_accuracy: 1.0000 - val_LNout359_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    }
   ],
   "source": [
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ModelScoreRater as msr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model para\n",
    "model_name = \"test_model1.h5\"\n",
    "model_path = \"Trained_models\\Split-500-reduce-softmax\"\n",
    "training_model_path = \"Model-for-training\\Split-500-reduce-softmax\"\n",
    "x_test_model = \"x_test_0.npy\" \n",
    "y_test_mdodel1 = \"y_test[0]_0.npy\" \n",
    "y_test_mdodel2 = \"y_test[1]_0.npy\" \n",
    "max_len = x.getsource_max_lan()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ep, ans_ep, test_lb, ans_lb = msr.loadmodel(model_path, \n",
    "                                                 model_name, \n",
    "                                                 training_model_path, \n",
    "                                                 x_test_model, \n",
    "                                                 y_test_mdodel1, \n",
    "                                                 y_test_mdodel2, \n",
    "                                                 max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pre, avg_rec, avg_acc = msr.errortype_totalscore(test_ep, ans_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr.error_type_F_score(avg_pre, avg_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_pre, avg_rec = msr.errorline_totalscore(test_lb, ans_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr.error_line_F_score(avg_pre, avg_rec)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y-JMBpEFcJWh",
    "psq7nyP0ca_Z"
   ],
   "name": "Main_Colab擴增版.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
