{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d94ffe56",
    "outputId": "1249cd6b-bff9-44bc-f32e-212835586c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\W.R_Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performer as tfr\n",
    "import nltk\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "9b54d338",
    "outputId": "805aecc7-247e-4761-a0f1-f2ac4bb1e054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef solve_cudnn_error():\\n    import tensorflow as tf\\n    gpus = tf.config.experimental.list_physical_devices(\\'GPU\\')\\n    if gpus:\\n        try:\\n            # Currently, memory growth needs to be the same across GPUs\\n            for gpu in gpus:\\n                tf.config.experimental.set_memory_growth(gpu, True)\\n            logical_gpus = tf.config.experimental.list_logical_devices(\\'GPU\\')\\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n        except RuntimeError as e:\\n            # Memory growth must be set before GPUs have been initialized\\n            print(e)\\n            \\nsolve_cudnn_error()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def solve_cudnn_error():\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "            \n",
    "solve_cudnn_error()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "22361e8f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(file_name):\n",
    "    errlist=[]\n",
    "    LBlist=[]\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "    #讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows: \n",
    "            RL=list(row.values())\n",
    "            #print(\"RL[0]: \", type(RL[0]), \"RL[1]: \", type(RL[1]))\n",
    "            RL[1:] = list(map(int, RL[1:]))\n",
    "            errs=RL[1:37]\n",
    "            LB=RL[37:]\n",
    "            errlist.append(errs)\n",
    "            LBlist.append(LB)\n",
    "    return errlist,LBlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b92cf71a"
   },
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1b88909b"
   },
   "outputs": [],
   "source": [
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "abb41dde"
   },
   "outputs": [],
   "source": [
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0db5e0b1"
   },
   "outputs": [],
   "source": [
    "def parseSentence(x):\t\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        #print(ord(x[i]))\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"ASCII\":\n",
    "            if inp==\"E\":\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  # nltk.word_tokenize(chrs) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e88de533"
   },
   "outputs": [],
   "source": [
    "def readcode(fname):\n",
    "    with open(fname,encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f1JYt9ELGe5Z"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plotTrainingLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingErrorLineAcc(history):\n",
    "    #accuracy\n",
    "    acc = [0.0]*360\n",
    "    for i in range(360):\n",
    "        name = \"LNout\" + str(i) +\"_accuracy\"\n",
    "        acc = np.asarray(acc) + np.asarray(history.history[name])\n",
    "    acc = list(acc/360)    \n",
    "    plt.plot(acc) #draw acc\n",
    "    #validation accuary\n",
    "    val_acc = [0.0]*360\n",
    "    for i in range(360):\n",
    "        name = \"val_LNout\" + str(i) +\"_accuracy\"\n",
    "        val_acc = np.asarray(val_acc) + np.asarray(history.history[name])\n",
    "    val_acc = list(val_acc/360)\n",
    "    plt.plot(val_acc) #draw val_acc\n",
    "    plt.title('model acc')\n",
    "    plt.ylabel('acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingErrorTypeAcc(history):\n",
    "    plt.plot(history.history['error_feed_forward_output1_accuracy'])\n",
    "    plt.plot(history.history['val_error_feed_forward_output1_accuracy'])\n",
    "    plt.title('model error_feed_forward_output1_accuracy')\n",
    "    plt.ylabel('error_feed_forward_output1_accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def listdir_fullpath(d):\n",
    "    return [f for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ec120d95"
   },
   "outputs": [],
   "source": [
    "#save model for training\n",
    "class TestTranslate(unittest.TestCase):\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<CR>': 5,\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "    \n",
    "    def test_translate(self):\n",
    "        #print(\"i am here: \" )\n",
    "        #source_file=[]\n",
    "        #Set Para\n",
    "        max_javaline_length = 160 #Max number of lines\n",
    "        #set path\n",
    "        Output_Path = \"Trianing\\InputCSV\\Split-500\"\n",
    "        Input_Path = \"Trianing\\InputTxt\\Split-500\"\n",
    "        model_for_training_org_path = \"Model-for-training-org\\Split-500\"\n",
    "        model_for_training_path = \"Model-for-training\\Split-500\"\n",
    "        Trained_model_Path = \"Trained_models\\Spilt-500\"\n",
    "        #get all txt file in input path\n",
    "        in_path = (glob.glob(Input_Path + \"/**/*.txt\"))\n",
    "        #print(\"in_path: \", in_path)\n",
    "        #cases = listdir_fullpath(Input_Path)\n",
    "        source_max_len = 0\n",
    "        target_max_len = 0\n",
    "        token_num = 0\n",
    "        all_sample_num = 16644 #all sample number\n",
    "        block_num = 16644 #sample num e.g 10000 sample have 10*1000\n",
    "        \n",
    "        #''' <----dust switch, if you need to create npy model for traing open this\n",
    "        self.sl = 0\n",
    "        \n",
    "        import math\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version \n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"First loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            #print(\"dirs: \", dirs)\n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])                    \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "                    #print(\"source_tokens length: \", len(source_tokens))\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            #get csv file     \n",
    "            out_path = Output_Path + \"/\" + \"test\" + str(loop)+\".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            \n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err,lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            dd = np.asarray(target_errors)\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]     \n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            \n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            \n",
    "            #output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens] \n",
    "            \n",
    "            self.sl = max(list(map(len, encode_tokens))+[self.sl])\n",
    "            source_max_len = self.sl\n",
    "\n",
    "        #padding here\n",
    "        print(\"source_max_len:\", source_max_len)\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version\n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"Second loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])\n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "\n",
    "            out_path = Output_Path + \"/\" + \"test\"+ str(loop)+ \".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err, lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "            #if len(source_tokens)>max_files: break\n",
    "            dd = np.asarray(target_errors)\n",
    "            #print(\"AAAA: \", dd.shape)\n",
    "            #print(\"aaaa: \" , type(target_errors[0][0]))\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]     \n",
    "            #print(\"source_token legth: \" , len(source_tokens))\n",
    "            #print(\"YYYY: \" , type(target_errors[0][0]))\n",
    "            #print(\"ZZZZ: \" , len(target_LB))\n",
    "            #print(\"ZZZZ len(target_LB[0]): \" , len(target_LB[0]))\n",
    "            #print(\"XXXX2: \" , len(source_tokens))\n",
    "            # Generate dictionaries        \n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "\n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            #print(\"encode_tokens1: \", encode_tokens)\n",
    "            encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_tokens2: \", encode_tokens)\n",
    "            encode_input = [list(map(lambda x: self.source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_input1: \", encode_input)\n",
    "            token_num = len(self.source_token_dict)\n",
    "            #print(\"token num: \", token_num)\n",
    "            #print(token_num)\n",
    "            #print(type(token_num))\n",
    "            #define save path and save dict  \n",
    "            dict_name = \"source_token_dict.pickle\"\n",
    "            #save for org\n",
    "            saveDictionary(self.source_token_dict, model_for_training_org_path + \"/\" + dict_name)\n",
    "            #save for training\n",
    "            saveDictionary(self.source_token_dict, model_for_training_path + \"/\" + dict_name)\n",
    "            #print(\"x.shape\", np.asarray(encode_input).shape)  #x.shape (2,  9)\n",
    "            \n",
    "            #x=[np.array(encode_input * 1)]\n",
    "            #y=[np.array(target_errors * 1),np.array(target_LB * 1)]\n",
    "\n",
    "            #print(\"x.shape\", np.asarray(x).shape)  #x.shape (2, 2048, 9)\n",
    "\n",
    "            ####  Split the data set into train and test_model\n",
    "            x = np.asarray(encode_input)\n",
    "            y = list(zip(np.asarray(target_errors), np.asarray(target_LB)))\n",
    "\n",
    "            #print(\"x.shape: \", x.shape)\n",
    "            #print(\"y length: \", len(y))\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=42)\n",
    "            \n",
    "            '''\n",
    "            y_train = list(zip(*y_train))  \n",
    "            y_train[0] = np.asarray(y_train[0])\n",
    "            y_train[1] = np.asarray(y_train[1])\n",
    "\n",
    "            y_train[1] = to_categorical(y_train[1], num_classes=max_javaline_length) #將類別向量轉換為二進制矩陣\n",
    "            #y_train = list(zip(y_train[0], y_train[1]))\n",
    "\n",
    "            #print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:\" , y_train[0].shape)\n",
    "            #print(\"y_train[1].shape\", y_train[1].shape)\n",
    "            #print(\"???????????: y_train.length \", len(y_train))\n",
    "            #print(\"???????????: y_train.[1] type \", type(y_train[1]))\n",
    "            y_train[1] = np.split(y_train[1], indices_or_sections=len(target_LB[0]), axis=1)\n",
    "            y_train[1] = [np.squeeze(elm, axis = 1) for elm in y_train[1]]\n",
    "            #print(\"after change->len(y_train[1].shape)\", len(y_train[1]) )\n",
    "            '''\n",
    "            \n",
    "            y_test = list(zip(*y_test))\n",
    "            y_test[0] = np.asarray(y_test[0])\n",
    "            y_test[1] = np.asarray(y_test[1])\n",
    "            y_test[1] = to_categorical(y_test[1], num_classes = max_javaline_length) \n",
    "            #y_test = list(zip(y_test[0], y_test[1])) \n",
    "            y_test[1] = np.split(y_test[1], indices_or_sections=len(target_LB[0]), axis=1) \n",
    "            y_test[1] = [np.squeeze(elm, axis = 1) for elm in y_test[1]]           \n",
    "            \n",
    "            #=============================#\n",
    "            x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.1, random_state=42)\n",
    "            print(\"Split x_train shape: \", (x_train).shape)\n",
    "            print(\"Split x_validation shape: \", (x_validation).shape)\n",
    "            \n",
    "            y_train = list(zip(*y_train))  \n",
    "            y_train[0] = np.asarray(y_train[0])\n",
    "            y_train[1] = np.asarray(y_train[1])\n",
    "\n",
    "            y_train[1] = to_categorical(y_train[1], num_classes=max_javaline_length) #將類別向量轉換為二進制矩陣\n",
    "            #y_train = list(zip(y_train[0], y_train[1]))\n",
    "\n",
    "            #print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:\" , y_train[0].shape)\n",
    "            #print(\"y_train[1].shape\", y_train[1].shape)\n",
    "            #print(\"???????????: y_train.length \", len(y_train))\n",
    "            #print(\"???????????: y_train.[1] type \", type(y_train[1]))\n",
    "            y_train[1] = np.split(y_train[1], indices_or_sections=len(target_LB[0]), axis=1)\n",
    "            y_train[1] = [np.squeeze(elm, axis = 1) for elm in y_train[1]]\n",
    "            #print(\"after change->len(y_train[1].shape)\", len(y_train[1]) )\n",
    "\n",
    "            y_validation = list(zip(*y_validation))\n",
    "            y_validation[0] = np.asarray(y_validation[0])\n",
    "            y_validation[1] = np.asarray(y_validation[1])\n",
    "            y_validation[1] = to_categorical(y_validation[1], num_classes = max_javaline_length) \n",
    "            #y_test = list(zip(y_test[0], y_test[1])) \n",
    "            y_validation[1] = np.split(y_validation[1], indices_or_sections=len(target_LB[0]), axis=1) \n",
    "            y_validation[1] = [np.squeeze(elm, axis = 1) for elm in y_validation[1]]           \n",
    "            \n",
    "            \n",
    "            #save org model for training \n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            \n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "            \n",
    "            #=========================================\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_validation_\" + str(loop) + \".npy\", x_validation)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_validation[0]_\" + str(loop) + \".npy\", y_validation[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_validation[1]_\" + str(loop) + \".npy\", y_validation[1])\n",
    "            \n",
    "            #transform x_train\n",
    "            #Set model para\n",
    "            training_source_max_len = 1200 #for replace source_max_len\n",
    "            print(\"org x_train shape: \", (x_train).shape)\n",
    "            print(\"org x_test shape: \", (x_test).shape)\n",
    "            #reshape x_train\n",
    "            new_x_train = []\n",
    "            for i in range(len(x_train)):\n",
    "                #print(\"i: \", i)\n",
    "                d_two = []\n",
    "                for k in range(training_source_max_len): #set length\n",
    "                    #print(\"k: \", k)\n",
    "                    d_two.append(x_train[i][k])\n",
    "                #switch to np array\n",
    "                d_two = np.asarray(d_two)\n",
    "                #give np array\n",
    "                new_x_train.append(d_two)\n",
    "            #switch to np array\n",
    "            x_train = np.asarray(new_x_train)\n",
    "            #save split model for training\n",
    "            \n",
    "            #reshape x_test\n",
    "            new_x_test = []\n",
    "            for i in range(len(x_test)):\n",
    "                #print(\"i: \", i)\n",
    "                d_two = []\n",
    "                for k in range(training_source_max_len): #set length\n",
    "                    #print(\"k: \", k)\n",
    "                    d_two.append(x_test[i][k])\n",
    "                #switch to np array\n",
    "                d_two = np.asarray(d_two)\n",
    "                #give np array\n",
    "                new_x_test.append(d_two)\n",
    "            #switch to np array\n",
    "            x_test = np.asarray(new_x_test)\n",
    "            \n",
    "            #reshape x_validation\n",
    "            new_x_validation = []\n",
    "            for i in range(len(x_validation)):\n",
    "                #print(\"i: \", i)\n",
    "                d_two = []\n",
    "                for k in range(training_source_max_len): #set length\n",
    "                    #print(\"k: \", k)\n",
    "                    d_two.append(x_validation[i][k])\n",
    "                #switch to np array\n",
    "                d_two = np.asarray(d_two)\n",
    "                #give np array\n",
    "                new_x_validation.append(d_two)\n",
    "            #switch to np array\n",
    "            x_validation = np.asarray(new_x_validation)\n",
    "            \n",
    "            #save split model for training\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            \n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "            \n",
    "            #=====================================\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_validation_\" + str(loop) + \".npy\", x_validation)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_validation[0]_\" + str(loop) + \".npy\", y_validation[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_validation[1]_\" + str(loop) + \".npy\", y_validation[1])\n",
    "        print(\"Training model save successful...\")    \n",
    "        #'''\n",
    "        \n",
    "        \n",
    "        #start training\n",
    "        import DataGeneratorTrain as DGTrain\n",
    "        import DataGeneratorValidation as DGValidation\n",
    "        import DataBuffer as db\n",
    "        from random import randrange\n",
    "        #Set driver path\n",
    "        source_max_len = 1200 #set max len  #default : 2889\n",
    "        line_block_num = 360 #lbNum\n",
    "        source_token_dict_name = \"source_token_dict.pickle\"\n",
    "        #load source_token_dict\n",
    "        source_token_dict = loadDictionary(model_for_training_path + \"/\" + source_token_dict_name)\n",
    "        #Set model para    \n",
    "        model = tfr.get_model(max_input_len=(source_max_len),\n",
    "                              max_javaline_length=160,\n",
    "                              errNum=36,\n",
    "                              lbNum=line_block_num, #lbNum=len(target_LB[0]), #160\n",
    "                              token_num=len(source_token_dict),\n",
    "                              embed_dim=256, #32, try 32 or 64\n",
    "                              encoder_num=4, #2 max = 6\n",
    "                              head_num=4,#4\n",
    "                              hidden_dim=128, #128\n",
    "                              dropout_rate=0.05 #0.05\n",
    "                             )\n",
    "        #Set losses\n",
    "        losses = {\"error_feed_forward_output1\": \"binary_crossentropy\"}\n",
    "        #error type weight\n",
    "        lossWeights = {\"error_feed_forward_output1\": 1.0}\n",
    "        \n",
    "        #metrics = {\"error_feed_forward_output1\": tf.keras.metrics.Accuracy()}\n",
    "        \n",
    "        #error line weight\n",
    "        for i in range(line_block_num):\n",
    "            name = \"LNout\" + str(i)\n",
    "            losses[name] = \"categorical_crossentropy\"\n",
    "            lossWeights[name] = 100 #error_feed_forward_output2[] weight\n",
    "            #metrics[name] = tf.keras.metrics.CategoricalAccuracy()\n",
    "        \n",
    "        \n",
    "        #set complie para\n",
    "        model.compile(optimizer=Adam(learning_rate=0.000001), loss=losses, loss_weights=lossWeights, metrics=[\"accuracy\"])\n",
    "        \n",
    "        #for output\n",
    "        #for x\n",
    "        input_buffer_params = { \n",
    "            \"data_path\": model_for_training_path,\n",
    "            \"data_number\": 13481,\n",
    "            \"data_type\": int,\n",
    "            \"block_size\": 13481 \n",
    "            }\n",
    "        \n",
    "        #for input\n",
    "        #for y\n",
    "        output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path, model_for_training_path],\n",
    "            \"data_number\": [13481, 13481],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [13481, 13481] \n",
    "            }\n",
    "        \n",
    "        #===========================================\n",
    "        #for output\n",
    "        #for x\n",
    "        validation_input_buffer_params = { \n",
    "            \"data_path\": model_for_training_path,\n",
    "            \"data_number\": 1498,\n",
    "            \"data_type\": int,\n",
    "            \"block_size\": 1498 \n",
    "            }\n",
    "        \n",
    "        #for input\n",
    "        #for y\n",
    "        validation_output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path, model_for_training_path],\n",
    "            \"data_number\": [1498, 1498],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [1498, 1498] \n",
    "            }\n",
    "        \n",
    "        \n",
    "        #Create Generators\n",
    "        print(\"Creating training generator...\")\n",
    "        training_generator = DGTrain.DataGeneratorTrain(input_buffer_params,\n",
    "                                                  output_buffer_params,\n",
    "                                                  [list(range(13481)), list(range(13481))] \n",
    "                                                )\n",
    "        #Create Generators\n",
    "        print(\"Creating validation generator...\")\n",
    "        validation_generator = DGValidation.DataGeneratorValidation(validation_input_buffer_params,\n",
    "                                                  validation_output_buffer_params,\n",
    "                                                  [list(range(1498)), list(range(1498))] \n",
    "                                                )\n",
    "        \n",
    "        \n",
    "        #Start training\n",
    "        print(\"Strat training...\")\n",
    "        history = model.fit_generator(generator = training_generator,\n",
    "                                      epochs = 2, #100 200 500 3000\n",
    "                                      verbose = 2, #set visibility\n",
    "                                      validation_data = validation_generator\n",
    "                                     )\n",
    "        print(\"history.history.keys: \", history.history.keys())\n",
    "        #show loss grapgh\n",
    "        plotTrainingLoss(history)\n",
    "        plotTrainingErrorTypeAcc(history)\n",
    "        plotTrainingErrorLineAcc(history)\n",
    "        \n",
    "        print(\"Model training completed...\")\n",
    "        #save model\n",
    "        print(\"Saving model...\")\n",
    "        model.save(Trained_model_Path + \"/\" + \"test_model1.h5\")\n",
    "        print(\"Model saving completed...\")\n",
    "        \n",
    "        '''\n",
    "        model, source_token_dict = load(\"test_model1.h5\")\n",
    "        \n",
    "        out1, out2 = tfr.decode(\n",
    "                model,\n",
    "                #encode_input,\n",
    "                x_test_loaded,max_len=source_max_len\n",
    "            )\n",
    "        #'''\n",
    "        \n",
    "    def getsource_max_lan(self):\n",
    "        return self.sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7aa3c824"
   },
   "outputs": [],
   "source": [
    "def saveDictionary(dt, file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"wb\")\n",
    "        pickle.dump(dt, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "a6ddfb54"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"rb\")\n",
    "        dt = pickle.load(a_file)\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "20063beb"
   },
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "34dcfe7d"
   },
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ba969ac3"
   },
   "outputs": [],
   "source": [
    "def load(model_name):\n",
    "        import sys\n",
    "        #sys.path.append('/content/drive/MyDrive/Final_Edition_include_model')\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_layer_normalization\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_performer\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_position_wise_feed_forward\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\tensorflow_fast_attention\")\n",
    "\n",
    "        from keras_performer import performer\n",
    "        from tensorflow import keras\n",
    "        from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "        from keras_pos_embd import TrigPosEmbedding\n",
    "        from tensorflow_fast_attention.fast_attention import  Attention, SelfAttention\n",
    "        from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "        co = performer.get_custom_objects()\n",
    "\n",
    "        model = keras.models.load_model(model_name, custom_objects= co)\n",
    "        source_token_dict = loadDictionary(\"source_token_dict.pickle\")\n",
    "       # t = loadDictionary(target_token_dict, 'target_token_dict.pickle')\n",
    "       # t_inv = loadDictionary(target_token_dict_inv, 'target_token_dict_inv.pickle')\n",
    "        return model, source_token_dict,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First loop:  0\n",
      "source_max_len: 2914\n",
      "Second loop:  0\n",
      "Split x_train shape:  (13481, 2914)\n",
      "Split x_validation shape:  (1498, 2914)\n",
      "org x_train shape:  (13481, 2914)\n",
      "org x_test shape:  (1665, 2914)\n",
      "Training model save successful...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Creating training generator...\n",
      "Creating validation generator...\n",
      "Strat training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W.R_Chen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "105/105 - 481s - loss: 179998.7500 - error_feed_forward_output1_loss: 0.6246 - LNout0_loss: 5.2127 - LNout1_loss: 5.1357 - LNout2_loss: 4.4960 - LNout3_loss: 4.9395 - LNout4_loss: 4.9898 - LNout5_loss: 4.5172 - LNout6_loss: 4.1364 - LNout7_loss: 5.0524 - LNout8_loss: 4.4675 - LNout9_loss: 4.5799 - LNout10_loss: 5.6782 - LNout11_loss: 5.2920 - LNout12_loss: 5.4254 - LNout13_loss: 5.2177 - LNout14_loss: 5.7350 - LNout15_loss: 5.2516 - LNout16_loss: 5.2842 - LNout17_loss: 5.2238 - LNout18_loss: 5.6546 - LNout19_loss: 4.6166 - LNout20_loss: 4.7175 - LNout21_loss: 5.2781 - LNout22_loss: 5.4132 - LNout23_loss: 4.6006 - LNout24_loss: 5.8371 - LNout25_loss: 5.4414 - LNout26_loss: 5.0301 - LNout27_loss: 4.8073 - LNout28_loss: 4.6907 - LNout29_loss: 5.6720 - LNout30_loss: 4.5406 - LNout31_loss: 4.1037 - LNout32_loss: 5.4660 - LNout33_loss: 5.2265 - LNout34_loss: 5.3794 - LNout35_loss: 5.0296 - LNout36_loss: 5.6938 - LNout37_loss: 5.0905 - LNout38_loss: 5.5360 - LNout39_loss: 4.8072 - LNout40_loss: 4.8476 - LNout41_loss: 5.6929 - LNout42_loss: 5.2433 - LNout43_loss: 5.3403 - LNout44_loss: 4.8302 - LNout45_loss: 5.1578 - LNout46_loss: 5.0787 - LNout47_loss: 4.8095 - LNout48_loss: 5.3435 - LNout49_loss: 5.3579 - LNout50_loss: 4.7724 - LNout51_loss: 5.2871 - LNout52_loss: 4.9477 - LNout53_loss: 5.0650 - LNout54_loss: 5.4193 - LNout55_loss: 4.4384 - LNout56_loss: 4.6672 - LNout57_loss: 5.2452 - LNout58_loss: 5.3392 - LNout59_loss: 5.0609 - LNout60_loss: 4.8668 - LNout61_loss: 4.6699 - LNout62_loss: 5.3719 - LNout63_loss: 4.7018 - LNout64_loss: 5.1200 - LNout65_loss: 5.0603 - LNout66_loss: 4.7509 - LNout67_loss: 4.6570 - LNout68_loss: 4.4951 - LNout69_loss: 4.9482 - LNout70_loss: 4.4717 - LNout71_loss: 5.2126 - LNout72_loss: 4.6522 - LNout73_loss: 4.4899 - LNout74_loss: 4.8167 - LNout75_loss: 5.5300 - LNout76_loss: 5.0691 - LNout77_loss: 5.6304 - LNout78_loss: 5.0897 - LNout79_loss: 5.6671 - LNout80_loss: 4.7826 - LNout81_loss: 5.2324 - LNout82_loss: 5.3030 - LNout83_loss: 4.9504 - LNout84_loss: 4.8349 - LNout85_loss: 4.7539 - LNout86_loss: 4.8828 - LNout87_loss: 5.5199 - LNout88_loss: 5.1985 - LNout89_loss: 4.6099 - LNout90_loss: 5.5538 - LNout91_loss: 4.8222 - LNout92_loss: 4.4056 - LNout93_loss: 4.7221 - LNout94_loss: 5.7663 - LNout95_loss: 5.0555 - LNout96_loss: 5.3822 - LNout97_loss: 5.7582 - LNout98_loss: 4.7888 - LNout99_loss: 4.6757 - LNout100_loss: 5.3229 - LNout101_loss: 5.0907 - LNout102_loss: 4.6491 - LNout103_loss: 5.0671 - LNout104_loss: 5.2319 - LNout105_loss: 5.3287 - LNout106_loss: 4.5887 - LNout107_loss: 4.7122 - LNout108_loss: 4.0536 - LNout109_loss: 4.9011 - LNout110_loss: 4.5596 - LNout111_loss: 5.5386 - LNout112_loss: 4.1919 - LNout113_loss: 5.3584 - LNout114_loss: 4.9516 - LNout115_loss: 4.5033 - LNout116_loss: 3.9562 - LNout117_loss: 5.2878 - LNout118_loss: 4.6443 - LNout119_loss: 4.5070 - LNout120_loss: 4.6529 - LNout121_loss: 5.1545 - LNout122_loss: 5.4713 - LNout123_loss: 5.6519 - LNout124_loss: 5.0023 - LNout125_loss: 4.5801 - LNout126_loss: 4.4495 - LNout127_loss: 5.5874 - LNout128_loss: 5.2470 - LNout129_loss: 6.0251 - LNout130_loss: 5.1905 - LNout131_loss: 4.2112 - LNout132_loss: 4.6334 - LNout133_loss: 5.3653 - LNout134_loss: 5.4746 - LNout135_loss: 4.4085 - LNout136_loss: 4.5321 - LNout137_loss: 4.6002 - LNout138_loss: 5.5195 - LNout139_loss: 5.7502 - LNout140_loss: 5.5242 - LNout141_loss: 5.3529 - LNout142_loss: 4.8611 - LNout143_loss: 4.9549 - LNout144_loss: 4.4919 - LNout145_loss: 5.0453 - LNout146_loss: 5.1386 - LNout147_loss: 5.2131 - LNout148_loss: 4.8333 - LNout149_loss: 5.3359 - LNout150_loss: 5.3983 - LNout151_loss: 4.5122 - LNout152_loss: 5.0792 - LNout153_loss: 4.7501 - LNout154_loss: 4.9679 - LNout155_loss: 4.8956 - LNout156_loss: 5.6314 - LNout157_loss: 5.2317 - LNout158_loss: 5.5974 - LNout159_loss: 5.0528 - LNout160_loss: 5.2314 - LNout161_loss: 6.0205 - LNout162_loss: 4.7430 - LNout163_loss: 5.6404 - LNout164_loss: 4.8666 - LNout165_loss: 4.1268 - LNout166_loss: 4.7219 - LNout167_loss: 4.2079 - LNout168_loss: 5.0082 - LNout169_loss: 5.2180 - LNout170_loss: 5.6147 - LNout171_loss: 4.5508 - LNout172_loss: 5.7565 - LNout173_loss: 5.5858 - LNout174_loss: 4.7651 - LNout175_loss: 4.9414 - LNout176_loss: 5.7365 - LNout177_loss: 4.5025 - LNout178_loss: 4.9480 - LNout179_loss: 6.1201 - LNout180_loss: 4.8801 - LNout181_loss: 5.2386 - LNout182_loss: 4.8826 - LNout183_loss: 5.0620 - LNout184_loss: 4.6408 - LNout185_loss: 4.6087 - LNout186_loss: 5.2516 - LNout187_loss: 4.9801 - LNout188_loss: 5.2995 - LNout189_loss: 5.0817 - LNout190_loss: 4.9019 - LNout191_loss: 4.6695 - LNout192_loss: 5.0207 - LNout193_loss: 4.4358 - LNout194_loss: 5.4605 - LNout195_loss: 5.1752 - LNout196_loss: 4.8027 - LNout197_loss: 5.0797 - LNout198_loss: 5.5982 - LNout199_loss: 4.9315 - LNout200_loss: 4.9147 - LNout201_loss: 5.7168 - LNout202_loss: 5.3433 - LNout203_loss: 4.7608 - LNout204_loss: 4.9079 - LNout205_loss: 4.9630 - LNout206_loss: 4.4212 - LNout207_loss: 5.0551 - LNout208_loss: 4.6675 - LNout209_loss: 4.7585 - LNout210_loss: 4.3827 - LNout211_loss: 4.9577 - LNout212_loss: 5.3551 - LNout213_loss: 5.4513 - LNout214_loss: 4.1750 - LNout215_loss: 3.9810 - LNout216_loss: 5.4781 - LNout217_loss: 4.6334 - LNout218_loss: 5.0367 - LNout219_loss: 5.2026 - LNout220_loss: 5.1720 - LNout221_loss: 4.7879 - LNout222_loss: 4.9864 - LNout223_loss: 4.4633 - LNout224_loss: 4.7421 - LNout225_loss: 5.5604 - LNout226_loss: 4.7983 - LNout227_loss: 5.1289 - LNout228_loss: 4.7201 - LNout229_loss: 5.2749 - LNout230_loss: 5.2860 - LNout231_loss: 4.6238 - LNout232_loss: 5.7848 - LNout233_loss: 5.3553 - LNout234_loss: 4.2903 - LNout235_loss: 4.8045 - LNout236_loss: 5.2189 - LNout237_loss: 5.2096 - LNout238_loss: 4.5140 - LNout239_loss: 5.5759 - LNout240_loss: 4.6617 - LNout241_loss: 4.7139 - LNout242_loss: 4.1560 - LNout243_loss: 4.7938 - LNout244_loss: 4.9622 - LNout245_loss: 4.9938 - LNout246_loss: 4.7201 - LNout247_loss: 4.9538 - LNout248_loss: 5.9778 - LNout249_loss: 4.3310 - LNout250_loss: 5.7959 - LNout251_loss: 4.0638 - LNout252_loss: 5.7127 - LNout253_loss: 5.3574 - LNout254_loss: 5.0149 - LNout255_loss: 4.8778 - LNout256_loss: 5.3474 - LNout257_loss: 5.0071 - LNout258_loss: 5.5076 - LNout259_loss: 4.8465 - LNout260_loss: 4.2717 - LNout261_loss: 4.5794 - LNout262_loss: 5.5595 - LNout263_loss: 4.2350 - LNout264_loss: 5.6760 - LNout265_loss: 5.6864 - LNout266_loss: 5.4627 - LNout267_loss: 4.9928 - LNout268_loss: 4.9197 - LNout269_loss: 5.2755 - LNout270_loss: 5.2407 - LNout271_loss: 5.3699 - LNout272_loss: 4.8528 - LNout273_loss: 4.5703 - LNout274_loss: 4.9125 - LNout275_loss: 4.6122 - LNout276_loss: 4.5526 - LNout277_loss: 5.6094 - LNout278_loss: 4.9124 - LNout279_loss: 4.4532 - LNout280_loss: 4.3411 - LNout281_loss: 5.8028 - LNout282_loss: 4.8397 - LNout283_loss: 4.9983 - LNout284_loss: 4.8564 - LNout285_loss: 4.4282 - LNout286_loss: 5.3578 - LNout287_loss: 5.0265 - LNout288_loss: 4.8888 - LNout289_loss: 4.4914 - LNout290_loss: 5.0981 - LNout291_loss: 5.0808 - LNout292_loss: 5.2888 - LNout293_loss: 4.6672 - LNout294_loss: 5.9583 - LNout295_loss: 5.0362 - LNout296_loss: 4.5988 - LNout297_loss: 4.8938 - LNout298_loss: 4.6711 - LNout299_loss: 5.7387 - LNout300_loss: 4.3930 - LNout301_loss: 5.4096 - LNout302_loss: 5.1857 - LNout303_loss: 3.8439 - LNout304_loss: 5.6855 - LNout305_loss: 5.1634 - LNout306_loss: 5.0827 - LNout307_loss: 5.0553 - LNout308_loss: 5.8465 - LNout309_loss: 5.1754 - LNout310_loss: 5.2521 - LNout311_loss: 4.7122 - LNout312_loss: 5.2325 - LNout313_loss: 4.7324 - LNout314_loss: 4.7084 - LNout315_loss: 4.4190 - LNout316_loss: 4.4535 - LNout317_loss: 5.4106 - LNout318_loss: 4.8755 - LNout319_loss: 4.3683 - LNout320_loss: 5.0030 - LNout321_loss: 4.7663 - LNout322_loss: 4.6491 - LNout323_loss: 5.3996 - LNout324_loss: 5.3669 - LNout325_loss: 4.8052 - LNout326_loss: 4.4176 - LNout327_loss: 5.1272 - LNout328_loss: 4.8999 - LNout329_loss: 5.6240 - LNout330_loss: 4.1659 - LNout331_loss: 5.2643 - LNout332_loss: 4.8828 - LNout333_loss: 5.5751 - LNout334_loss: 4.7802 - LNout335_loss: 4.4499 - LNout336_loss: 5.1261 - LNout337_loss: 5.7803 - LNout338_loss: 4.0464 - LNout339_loss: 4.2289 - LNout340_loss: 5.1595 - LNout341_loss: 5.3209 - LNout342_loss: 5.6919 - LNout343_loss: 4.2627 - LNout344_loss: 4.3534 - LNout345_loss: 4.6471 - LNout346_loss: 5.0087 - LNout347_loss: 5.2170 - LNout348_loss: 4.1919 - LNout349_loss: 4.2367 - LNout350_loss: 4.7396 - LNout351_loss: 4.9464 - LNout352_loss: 4.6227 - LNout353_loss: 4.8956 - LNout354_loss: 5.0805 - LNout355_loss: 5.4973 - LNout356_loss: 4.8506 - LNout357_loss: 5.3851 - LNout358_loss: 4.5984 - LNout359_loss: 4.2388 - error_feed_forward_output1_accuracy: 0.0208 - LNout0_accuracy: 7.4405e-04 - LNout1_accuracy: 0.0034 - LNout2_accuracy: 0.0280 - LNout3_accuracy: 9.6726e-04 - LNout4_accuracy: 0.0023 - LNout5_accuracy: 0.0504 - LNout6_accuracy: 0.3266 - LNout7_accuracy: 1.4881e-04 - LNout8_accuracy: 0.0104 - LNout9_accuracy: 0.0013 - LNout10_accuracy: 0.0000e+00 - LNout11_accuracy: 7.4405e-05 - LNout12_accuracy: 2.2321e-04 - LNout13_accuracy: 3.7202e-04 - LNout14_accuracy: 7.4405e-05 - LNout15_accuracy: 0.0000e+00 - LNout16_accuracy: 1.4881e-04 - LNout17_accuracy: 0.0000e+00 - LNout18_accuracy: 7.4405e-05 - LNout19_accuracy: 2.9762e-04 - LNout20_accuracy: 4.4643e-04 - LNout21_accuracy: 7.4405e-05 - LNout22_accuracy: 1.4881e-04 - LNout23_accuracy: 3.7202e-04 - LNout24_accuracy: 1.4881e-04 - LNout25_accuracy: 2.2321e-04 - LNout26_accuracy: 1.4881e-04 - LNout27_accuracy: 0.0000e+00 - LNout28_accuracy: 5.9524e-04 - LNout29_accuracy: 1.4881e-04 - LNout30_accuracy: 0.0015 - LNout31_accuracy: 0.1605 - LNout32_accuracy: 1.4881e-04 - LNout33_accuracy: 0.0000e+00 - LNout34_accuracy: 0.0000e+00 - LNout35_accuracy: 0.0000e+00 - LNout36_accuracy: 7.4405e-05 - LNout37_accuracy: 0.0000e+00 - LNout38_accuracy: 0.0000e+00 - LNout39_accuracy: 7.4405e-05 - LNout40_accuracy: 0.0000e+00 - LNout41_accuracy: 0.0000e+00 - LNout42_accuracy: 0.0000e+00 - LNout43_accuracy: 0.0000e+00 - LNout44_accuracy: 0.0000e+00 - LNout45_accuracy: 0.0000e+00 - LNout46_accuracy: 0.0000e+00 - LNout47_accuracy: 0.0000e+00 - LNout48_accuracy: 0.0000e+00 - LNout49_accuracy: 0.0000e+00 - LNout50_accuracy: 7.4405e-05 - LNout51_accuracy: 0.0000e+00 - LNout52_accuracy: 0.0000e+00 - LNout53_accuracy: 0.0000e+00 - LNout54_accuracy: 0.0000e+00 - LNout55_accuracy: 0.0070 - LNout56_accuracy: 0.0000e+00 - LNout57_accuracy: 0.0000e+00 - LNout58_accuracy: 0.0000e+00 - LNout59_accuracy: 0.0000e+00 - LNout60_accuracy: 2.2321e-04 - LNout61_accuracy: 0.0634 - LNout62_accuracy: 0.0010 - LNout63_accuracy: 2.2321e-04 - LNout64_accuracy: 2.9762e-04 - LNout65_accuracy: 0.0013 - LNout66_accuracy: 0.0000e+00 - LNout67_accuracy: 0.0184 - LNout68_accuracy: 0.0022 - LNout69_accuracy: 2.2321e-04 - LNout70_accuracy: 0.0090 - LNout71_accuracy: 7.4405e-05 - LNout72_accuracy: 0.0526 - LNout73_accuracy: 0.0034 - LNout74_accuracy: 3.7202e-04 - LNout75_accuracy: 7.4405e-05 - LNout76_accuracy: 2.2321e-04 - LNout77_accuracy: 0.0000e+00 - LNout78_accuracy: 0.0000e+00 - LNout79_accuracy: 7.4405e-05 - LNout80_accuracy: 7.4405e-05 - LNout81_accuracy: 0.0000e+00 - LNout82_accuracy: 0.0000e+00 - LNout83_accuracy: 1.4881e-04 - LNout84_accuracy: 0.0000e+00 - LNout85_accuracy: 0.0000e+00 - LNout86_accuracy: 7.4405e-05 - LNout87_accuracy: 7.4405e-05 - LNout88_accuracy: 0.0000e+00 - LNout89_accuracy: 0.0000e+00 - LNout90_accuracy: 0.0000e+00 - LNout91_accuracy: 0.0000e+00 - LNout92_accuracy: 0.0088 - LNout93_accuracy: 7.4405e-05 - LNout94_accuracy: 0.0000e+00 - LNout95_accuracy: 0.0000e+00 - LNout96_accuracy: 0.0000e+00 - LNout97_accuracy: 0.0000e+00 - LNout98_accuracy: 0.0000e+00 - LNout99_accuracy: 0.0016 - LNout100_accuracy: 0.0000e+00 - LNout101_accuracy: 0.0000e+00 - LNout102_accuracy: 0.0072 - LNout103_accuracy: 0.0000e+00 - LNout104_accuracy: 0.0000e+00 - LNout105_accuracy: 0.0000e+00 - LNout106_accuracy: 9.6726e-04 - LNout107_accuracy: 1.4881e-04 - LNout108_accuracy: 0.1629 - LNout109_accuracy: 7.4405e-05 - LNout110_accuracy: 1.4881e-04 - LNout111_accuracy: 0.0000e+00 - LNout112_accuracy: 0.3287 - LNout113_accuracy: 0.0000e+00 - LNout114_accuracy: 0.0000e+00 - LNout115_accuracy: 0.0064 - LNout116_accuracy: 0.8295 - LNout117_accuracy: 0.0000e+00 - LNout118_accuracy: 7.4405e-04 - LNout119_accuracy: 0.0327 - LNout120_accuracy: 4.4643e-04 - LNout121_accuracy: 7.4405e-05 - LNout122_accuracy: 0.0036 - LNout123_accuracy: 7.4405e-05 - LNout124_accuracy: 5.9524e-04 - LNout125_accuracy: 0.0025 - LNout126_accuracy: 0.0094 - LNout127_accuracy: 8.9286e-04 - LNout128_accuracy: 2.2321e-04 - LNout129_accuracy: 0.0000e+00 - LNout130_accuracy: 0.0000e+00 - LNout131_accuracy: 0.4605 - LNout132_accuracy: 7.4405e-04 - LNout133_accuracy: 0.0000e+00 - LNout134_accuracy: 0.0000e+00 - LNout135_accuracy: 0.0022 - LNout136_accuracy: 0.0000e+00 - LNout137_accuracy: 7.4405e-05 - LNout138_accuracy: 0.0000e+00 - LNout139_accuracy: 7.4405e-05 - LNout140_accuracy: 0.0000e+00 - LNout141_accuracy: 0.0000e+00 - LNout142_accuracy: 0.0057 - LNout143_accuracy: 0.0000e+00 - LNout144_accuracy: 0.0057 - LNout145_accuracy: 0.0000e+00 - LNout146_accuracy: 0.0000e+00 - LNout147_accuracy: 0.0000e+00 - LNout148_accuracy: 7.4405e-05 - LNout149_accuracy: 0.0000e+00 - LNout150_accuracy: 0.0000e+00 - LNout151_accuracy: 0.0013 - LNout152_accuracy: 0.0000e+00 - LNout153_accuracy: 0.0000e+00 - LNout154_accuracy: 0.0000e+00 - LNout155_accuracy: 0.0000e+00 - LNout156_accuracy: 0.0000e+00 - LNout157_accuracy: 0.0000e+00 - LNout158_accuracy: 0.0000e+00 - LNout159_accuracy: 0.0000e+00 - LNout160_accuracy: 0.0000e+00 - LNout161_accuracy: 0.0000e+00 - LNout162_accuracy: 0.0153 - LNout163_accuracy: 0.0000e+00 - LNout164_accuracy: 0.0000e+00 - LNout165_accuracy: 0.4711 - LNout166_accuracy: 0.0026 - LNout167_accuracy: 0.0112 - LNout168_accuracy: 0.0000e+00 - LNout169_accuracy: 0.0000e+00 - LNout170_accuracy: 0.0000e+00 - LNout171_accuracy: 0.0103 - LNout172_accuracy: 0.0000e+00 - LNout173_accuracy: 0.0000e+00 - LNout174_accuracy: 0.0000e+00 - LNout175_accuracy: 0.0012 - LNout176_accuracy: 0.0000e+00 - LNout177_accuracy: 0.0016 - LNout178_accuracy: 0.0000e+00 - LNout179_accuracy: 0.0000e+00 - LNout180_accuracy: 1.4881e-04 - LNout181_accuracy: 7.4405e-05 - LNout182_accuracy: 0.0013 - LNout183_accuracy: 0.0016 - LNout184_accuracy: 0.0179 - LNout185_accuracy: 1.4881e-04 - LNout186_accuracy: 0.0000e+00 - LNout187_accuracy: 0.0000e+00 - LNout188_accuracy: 0.0000e+00 - LNout189_accuracy: 0.0000e+00 - LNout190_accuracy: 1.4881e-04 - LNout191_accuracy: 0.0031 - LNout192_accuracy: 0.0000e+00 - LNout193_accuracy: 0.0312 - LNout194_accuracy: 0.0000e+00 - LNout195_accuracy: 0.0000e+00 - LNout196_accuracy: 0.0000e+00 - LNout197_accuracy: 0.0000e+00 - LNout198_accuracy: 0.0000e+00 - LNout199_accuracy: 3.7202e-04 - LNout200_accuracy: 0.0116 - LNout201_accuracy: 0.0000e+00 - LNout202_accuracy: 2.2321e-04 - LNout203_accuracy: 5.2083e-04 - LNout204_accuracy: 0.0000e+00 - LNout205_accuracy: 0.0000e+00 - LNout206_accuracy: 0.0028 - LNout207_accuracy: 0.0011 - LNout208_accuracy: 0.0000e+00 - LNout209_accuracy: 0.0000e+00 - LNout210_accuracy: 0.0356 - LNout211_accuracy: 0.0000e+00 - LNout212_accuracy: 0.0000e+00 - LNout213_accuracy: 0.0000e+00 - LNout214_accuracy: 0.0945 - LNout215_accuracy: 0.4786 - LNout216_accuracy: 0.0000e+00 - LNout217_accuracy: 0.0109 - LNout218_accuracy: 7.4405e-05 - LNout219_accuracy: 1.4881e-04 - LNout220_accuracy: 0.0000e+00 - LNout221_accuracy: 0.0000e+00 - LNout222_accuracy: 0.0000e+00 - LNout223_accuracy: 0.0481 - LNout224_accuracy: 0.0000e+00 - LNout225_accuracy: 0.0000e+00 - LNout226_accuracy: 5.2083e-04 - LNout227_accuracy: 0.0000e+00 - LNout228_accuracy: 0.0000e+00 - LNout229_accuracy: 0.0000e+00 - LNout230_accuracy: 0.0000e+00 - LNout231_accuracy: 0.0019 - LNout232_accuracy: 0.0000e+00 - LNout233_accuracy: 0.0000e+00 - LNout234_accuracy: 0.0022 - LNout235_accuracy: 0.0000e+00 - LNout236_accuracy: 0.0000e+00 - LNout237_accuracy: 0.0000e+00 - LNout238_accuracy: 0.0389 - LNout239_accuracy: 0.0000e+00 - LNout240_accuracy: 2.2321e-04 - LNout241_accuracy: 0.0023 - LNout242_accuracy: 0.0703 - LNout243_accuracy: 0.0000e+00 - LNout244_accuracy: 7.4405e-05 - LNout245_accuracy: 7.4405e-05 - LNout246_accuracy: 0.0000e+00 - LNout247_accuracy: 0.0000e+00 - LNout248_accuracy: 0.0000e+00 - LNout249_accuracy: 0.0186 - LNout250_accuracy: 0.0000e+00 - LNout251_accuracy: 0.5250 - LNout252_accuracy: 0.0000e+00 - LNout253_accuracy: 0.0000e+00 - LNout254_accuracy: 0.0000e+00 - LNout255_accuracy: 0.0000e+00 - LNout256_accuracy: 7.4405e-05 - LNout257_accuracy: 0.0000e+00 - LNout258_accuracy: 0.0000e+00 - LNout259_accuracy: 7.4405e-05 - LNout260_accuracy: 0.1560 - LNout261_accuracy: 0.0000e+00 - LNout262_accuracy: 0.0000e+00 - LNout263_accuracy: 0.1027 - LNout264_accuracy: 0.0000e+00 - LNout265_accuracy: 0.0000e+00 - LNout266_accuracy: 0.0000e+00 - LNout267_accuracy: 0.0000e+00 - LNout268_accuracy: 0.0000e+00 - LNout269_accuracy: 0.0000e+00 - LNout270_accuracy: 0.0000e+00 - LNout271_accuracy: 0.0000e+00 - LNout272_accuracy: 0.0000e+00 - LNout273_accuracy: 1.4881e-04 - LNout274_accuracy: 0.0000e+00 - LNout275_accuracy: 0.0000e+00 - LNout276_accuracy: 2.2321e-04 - LNout277_accuracy: 0.0000e+00 - LNout278_accuracy: 0.0000e+00 - LNout279_accuracy: 0.0126 - LNout280_accuracy: 0.0591 - LNout281_accuracy: 0.0000e+00 - LNout282_accuracy: 0.0000e+00 - LNout283_accuracy: 0.0000e+00 - LNout284_accuracy: 0.0000e+00 - LNout285_accuracy: 0.0022 - LNout286_accuracy: 0.0000e+00 - LNout287_accuracy: 0.0000e+00 - LNout288_accuracy: 0.0000e+00 - LNout289_accuracy: 0.0131 - LNout290_accuracy: 0.0000e+00 - LNout291_accuracy: 0.0000e+00 - LNout292_accuracy: 0.0000e+00 - LNout293_accuracy: 3.7202e-04 - LNout294_accuracy: 0.0000e+00 - LNout295_accuracy: 0.0000e+00 - LNout296_accuracy: 0.0000e+00 - LNout297_accuracy: 0.0000e+00 - LNout298_accuracy: 3.7202e-04 - LNout299_accuracy: 0.0000e+00 - LNout300_accuracy: 0.0183 - LNout301_accuracy: 0.0000e+00 - LNout302_accuracy: 0.0000e+00 - LNout303_accuracy: 0.5594 - LNout304_accuracy: 7.4405e-05 - LNout305_accuracy: 0.0000e+00 - LNout306_accuracy: 0.0000e+00 - LNout307_accuracy: 0.0000e+00 - LNout308_accuracy: 0.0000e+00 - LNout309_accuracy: 0.0000e+00 - LNout310_accuracy: 0.0000e+00 - LNout311_accuracy: 0.0206 - LNout312_accuracy: 0.0000e+00 - LNout313_accuracy: 0.0087 - LNout314_accuracy: 0.0061 - LNout315_accuracy: 0.0450 - LNout316_accuracy: 0.0079 - LNout317_accuracy: 0.0000e+00 - LNout318_accuracy: 0.0000e+00 - LNout319_accuracy: 0.0280 - LNout320_accuracy: 0.0000e+00 - LNout321_accuracy: 7.4405e-05 - LNout322_accuracy: 0.0000e+00 - LNout323_accuracy: 0.0000e+00 - LNout324_accuracy: 0.0000e+00 - LNout325_accuracy: 0.0000e+00 - LNout326_accuracy: 0.0153 - LNout327_accuracy: 0.0000e+00 - LNout328_accuracy: 0.0000e+00 - LNout329_accuracy: 0.0000e+00 - LNout330_accuracy: 0.0177 - LNout331_accuracy: 0.0000e+00 - LNout332_accuracy: 0.0000e+00 - LNout333_accuracy: 0.0000e+00 - LNout334_accuracy: 0.0000e+00 - LNout335_accuracy: 0.0307 - LNout336_accuracy: 0.0000e+00 - LNout337_accuracy: 0.0000e+00 - LNout338_accuracy: 0.3764 - LNout339_accuracy: 0.2024 - LNout340_accuracy: 0.0000e+00 - LNout341_accuracy: 0.0000e+00 - LNout342_accuracy: 0.0000e+00 - LNout343_accuracy: 0.1112 - LNout344_accuracy: 0.0218 - LNout345_accuracy: 4.4643e-04 - LNout346_accuracy: 0.0000e+00 - LNout347_accuracy: 0.0056 - LNout348_accuracy: 0.2426 - LNout349_accuracy: 0.0780 - LNout350_accuracy: 7.4405e-05 - LNout351_accuracy: 0.0000e+00 - LNout352_accuracy: 7.4405e-05 - LNout353_accuracy: 0.0000e+00 - LNout354_accuracy: 0.0000e+00 - LNout355_accuracy: 0.0000e+00 - LNout356_accuracy: 7.4405e-05 - LNout357_accuracy: 0.0000e+00 - LNout358_accuracy: 0.0108 - LNout359_accuracy: 0.0553 - val_loss: 178928.2656 - val_error_feed_forward_output1_loss: 0.3856 - val_LNout0_loss: 5.2589 - val_LNout1_loss: 5.1809 - val_LNout2_loss: 4.3836 - val_LNout3_loss: 4.9686 - val_LNout4_loss: 4.6141 - val_LNout5_loss: 4.6946 - val_LNout6_loss: 3.8486 - val_LNout7_loss: 5.0909 - val_LNout8_loss: 4.4385 - val_LNout9_loss: 4.6167 - val_LNout10_loss: 5.6600 - val_LNout11_loss: 5.2954 - val_LNout12_loss: 5.5217 - val_LNout13_loss: 5.0235 - val_LNout14_loss: 5.7399 - val_LNout15_loss: 5.3574 - val_LNout16_loss: 5.1048 - val_LNout17_loss: 5.5180 - val_LNout18_loss: 5.8721 - val_LNout19_loss: 4.5585 - val_LNout20_loss: 4.4865 - val_LNout21_loss: 5.3242 - val_LNout22_loss: 5.1545 - val_LNout23_loss: 4.4178 - val_LNout24_loss: 5.9033 - val_LNout25_loss: 5.2202 - val_LNout26_loss: 4.9373 - val_LNout27_loss: 4.9351 - val_LNout28_loss: 4.3136 - val_LNout29_loss: 5.5815 - val_LNout30_loss: 4.2888 - val_LNout31_loss: 4.0263 - val_LNout32_loss: 5.4775 - val_LNout33_loss: 5.1339 - val_LNout34_loss: 5.4469 - val_LNout35_loss: 4.9346 - val_LNout36_loss: 6.0040 - val_LNout37_loss: 5.2505 - val_LNout38_loss: 5.5649 - val_LNout39_loss: 4.7524 - val_LNout40_loss: 4.7425 - val_LNout41_loss: 5.4245 - val_LNout42_loss: 5.0001 - val_LNout43_loss: 5.2696 - val_LNout44_loss: 4.8492 - val_LNout45_loss: 5.1277 - val_LNout46_loss: 4.9732 - val_LNout47_loss: 4.4301 - val_LNout48_loss: 5.2603 - val_LNout49_loss: 5.5834 - val_LNout50_loss: 4.5742 - val_LNout51_loss: 5.2942 - val_LNout52_loss: 5.0086 - val_LNout53_loss: 5.0888 - val_LNout54_loss: 5.3287 - val_LNout55_loss: 4.3245 - val_LNout56_loss: 4.5998 - val_LNout57_loss: 5.4156 - val_LNout58_loss: 5.2402 - val_LNout59_loss: 5.0602 - val_LNout60_loss: 4.7714 - val_LNout61_loss: 4.5683 - val_LNout62_loss: 5.3247 - val_LNout63_loss: 4.6493 - val_LNout64_loss: 5.0953 - val_LNout65_loss: 4.9888 - val_LNout66_loss: 4.4475 - val_LNout67_loss: 4.6640 - val_LNout68_loss: 4.3180 - val_LNout69_loss: 5.0280 - val_LNout70_loss: 4.4087 - val_LNout71_loss: 4.9640 - val_LNout72_loss: 4.7028 - val_LNout73_loss: 4.4475 - val_LNout74_loss: 4.9105 - val_LNout75_loss: 5.5653 - val_LNout76_loss: 5.0978 - val_LNout77_loss: 5.4542 - val_LNout78_loss: 4.7740 - val_LNout79_loss: 5.6929 - val_LNout80_loss: 4.7191 - val_LNout81_loss: 5.3360 - val_LNout82_loss: 5.2780 - val_LNout83_loss: 4.6703 - val_LNout84_loss: 4.7616 - val_LNout85_loss: 4.5484 - val_LNout86_loss: 4.9669 - val_LNout87_loss: 5.6506 - val_LNout88_loss: 5.3086 - val_LNout89_loss: 4.6692 - val_LNout90_loss: 5.6643 - val_LNout91_loss: 4.8277 - val_LNout92_loss: 4.2412 - val_LNout93_loss: 4.6809 - val_LNout94_loss: 5.8817 - val_LNout95_loss: 4.8149 - val_LNout96_loss: 5.4639 - val_LNout97_loss: 5.6373 - val_LNout98_loss: 4.8563 - val_LNout99_loss: 4.5480 - val_LNout100_loss: 5.5313 - val_LNout101_loss: 4.9194 - val_LNout102_loss: 4.5854 - val_LNout103_loss: 5.1129 - val_LNout104_loss: 5.1933 - val_LNout105_loss: 5.3067 - val_LNout106_loss: 4.6088 - val_LNout107_loss: 4.7010 - val_LNout108_loss: 3.7077 - val_LNout109_loss: 4.9595 - val_LNout110_loss: 4.2420 - val_LNout111_loss: 5.4650 - val_LNout112_loss: 3.8693 - val_LNout113_loss: 5.4731 - val_LNout114_loss: 4.7084 - val_LNout115_loss: 4.4076 - val_LNout116_loss: 3.5903 - val_LNout117_loss: 5.2082 - val_LNout118_loss: 4.4653 - val_LNout119_loss: 4.5461 - val_LNout120_loss: 4.6936 - val_LNout121_loss: 5.1838 - val_LNout122_loss: 5.6366 - val_LNout123_loss: 5.9725 - val_LNout124_loss: 5.0505 - val_LNout125_loss: 4.5430 - val_LNout126_loss: 4.5653 - val_LNout127_loss: 5.6596 - val_LNout128_loss: 5.3005 - val_LNout129_loss: 6.2134 - val_LNout130_loss: 5.1906 - val_LNout131_loss: 3.9700 - val_LNout132_loss: 4.5290 - val_LNout133_loss: 5.3981 - val_LNout134_loss: 5.4285 - val_LNout135_loss: 4.2834 - val_LNout136_loss: 4.3676 - val_LNout137_loss: 4.6291 - val_LNout138_loss: 5.6511 - val_LNout139_loss: 5.9228 - val_LNout140_loss: 5.9705 - val_LNout141_loss: 5.4212 - val_LNout142_loss: 5.0571 - val_LNout143_loss: 4.8491 - val_LNout144_loss: 4.2008 - val_LNout145_loss: 5.0774 - val_LNout146_loss: 4.9782 - val_LNout147_loss: 5.2327 - val_LNout148_loss: 4.8465 - val_LNout149_loss: 5.4135 - val_LNout150_loss: 5.4521 - val_LNout151_loss: 4.5246 - val_LNout152_loss: 5.2225 - val_LNout153_loss: 4.9392 - val_LNout154_loss: 4.8825 - val_LNout155_loss: 5.0203 - val_LNout156_loss: 5.8293 - val_LNout157_loss: 5.0995 - val_LNout158_loss: 5.8092 - val_LNout159_loss: 5.0301 - val_LNout160_loss: 5.0460 - val_LNout161_loss: 6.1653 - val_LNout162_loss: 4.9187 - val_LNout163_loss: 5.6029 - val_LNout164_loss: 4.9397 - val_LNout165_loss: 3.8394 - val_LNout166_loss: 4.9682 - val_LNout167_loss: 4.1222 - val_LNout168_loss: 4.8046 - val_LNout169_loss: 5.2693 - val_LNout170_loss: 5.5636 - val_LNout171_loss: 4.2642 - val_LNout172_loss: 5.7515 - val_LNout173_loss: 5.6433 - val_LNout174_loss: 4.6069 - val_LNout175_loss: 5.0172 - val_LNout176_loss: 5.6567 - val_LNout177_loss: 4.3101 - val_LNout178_loss: 5.1013 - val_LNout179_loss: 6.1964 - val_LNout180_loss: 4.9163 - val_LNout181_loss: 5.1251 - val_LNout182_loss: 4.9431 - val_LNout183_loss: 5.2112 - val_LNout184_loss: 4.7671 - val_LNout185_loss: 4.3363 - val_LNout186_loss: 5.1921 - val_LNout187_loss: 4.9907 - val_LNout188_loss: 5.1771 - val_LNout189_loss: 4.9556 - val_LNout190_loss: 4.7539 - val_LNout191_loss: 4.5911 - val_LNout192_loss: 4.9787 - val_LNout193_loss: 4.5400 - val_LNout194_loss: 5.6029 - val_LNout195_loss: 5.3601 - val_LNout196_loss: 4.6998 - val_LNout197_loss: 5.0302 - val_LNout198_loss: 5.6182 - val_LNout199_loss: 4.9310 - val_LNout200_loss: 4.8511 - val_LNout201_loss: 5.9803 - val_LNout202_loss: 5.5936 - val_LNout203_loss: 4.8584 - val_LNout204_loss: 4.7880 - val_LNout205_loss: 4.7986 - val_LNout206_loss: 4.3125 - val_LNout207_loss: 5.2315 - val_LNout208_loss: 4.5300 - val_LNout209_loss: 4.6389 - val_LNout210_loss: 4.3229 - val_LNout211_loss: 4.9978 - val_LNout212_loss: 5.3193 - val_LNout213_loss: 5.5061 - val_LNout214_loss: 4.0730 - val_LNout215_loss: 3.7913 - val_LNout216_loss: 5.3367 - val_LNout217_loss: 4.4675 - val_LNout218_loss: 5.1343 - val_LNout219_loss: 5.4742 - val_LNout220_loss: 5.1364 - val_LNout221_loss: 4.7256 - val_LNout222_loss: 4.9230 - val_LNout223_loss: 4.2321 - val_LNout224_loss: 4.6524 - val_LNout225_loss: 5.7718 - val_LNout226_loss: 4.9065 - val_LNout227_loss: 5.1265 - val_LNout228_loss: 4.4939 - val_LNout229_loss: 5.2473 - val_LNout230_loss: 5.3618 - val_LNout231_loss: 4.4978 - val_LNout232_loss: 5.8561 - val_LNout233_loss: 5.2648 - val_LNout234_loss: 4.1623 - val_LNout235_loss: 4.8452 - val_LNout236_loss: 5.2278 - val_LNout237_loss: 5.3018 - val_LNout238_loss: 4.2958 - val_LNout239_loss: 5.5911 - val_LNout240_loss: 4.4731 - val_LNout241_loss: 4.7556 - val_LNout242_loss: 4.0221 - val_LNout243_loss: 4.8076 - val_LNout244_loss: 4.9188 - val_LNout245_loss: 4.9625 - val_LNout246_loss: 4.4528 - val_LNout247_loss: 4.9818 - val_LNout248_loss: 5.7317 - val_LNout249_loss: 4.3388 - val_LNout250_loss: 5.9160 - val_LNout251_loss: 3.9519 - val_LNout252_loss: 5.9133 - val_LNout253_loss: 5.3231 - val_LNout254_loss: 4.7611 - val_LNout255_loss: 4.8326 - val_LNout256_loss: 5.4642 - val_LNout257_loss: 4.9704 - val_LNout258_loss: 5.5796 - val_LNout259_loss: 4.9014 - val_LNout260_loss: 4.2535 - val_LNout261_loss: 4.4993 - val_LNout262_loss: 5.6983 - val_LNout263_loss: 3.8253 - val_LNout264_loss: 5.6071 - val_LNout265_loss: 5.7680 - val_LNout266_loss: 5.2645 - val_LNout267_loss: 5.0534 - val_LNout268_loss: 4.9047 - val_LNout269_loss: 5.3977 - val_LNout270_loss: 5.3508 - val_LNout271_loss: 5.3457 - val_LNout272_loss: 4.7061 - val_LNout273_loss: 4.5119 - val_LNout274_loss: 4.8466 - val_LNout275_loss: 4.5210 - val_LNout276_loss: 4.3544 - val_LNout277_loss: 5.7697 - val_LNout278_loss: 4.6200 - val_LNout279_loss: 4.3849 - val_LNout280_loss: 4.0843 - val_LNout281_loss: 5.8903 - val_LNout282_loss: 4.6494 - val_LNout283_loss: 5.1258 - val_LNout284_loss: 4.8565 - val_LNout285_loss: 4.2399 - val_LNout286_loss: 5.3595 - val_LNout287_loss: 4.8423 - val_LNout288_loss: 4.8545 - val_LNout289_loss: 4.4798 - val_LNout290_loss: 5.2274 - val_LNout291_loss: 5.1578 - val_LNout292_loss: 5.4573 - val_LNout293_loss: 4.5189 - val_LNout294_loss: 5.9543 - val_LNout295_loss: 4.8115 - val_LNout296_loss: 4.4180 - val_LNout297_loss: 4.8673 - val_LNout298_loss: 4.4523 - val_LNout299_loss: 5.7806 - val_LNout300_loss: 4.3888 - val_LNout301_loss: 5.5553 - val_LNout302_loss: 5.4098 - val_LNout303_loss: 3.7523 - val_LNout304_loss: 5.7657 - val_LNout305_loss: 5.2147 - val_LNout306_loss: 4.8679 - val_LNout307_loss: 5.2009 - val_LNout308_loss: 5.9853 - val_LNout309_loss: 5.2020 - val_LNout310_loss: 5.2047 - val_LNout311_loss: 4.8547 - val_LNout312_loss: 5.2351 - val_LNout313_loss: 4.6823 - val_LNout314_loss: 4.4863 - val_LNout315_loss: 4.4993 - val_LNout316_loss: 4.2528 - val_LNout317_loss: 5.5779 - val_LNout318_loss: 4.9417 - val_LNout319_loss: 4.2748 - val_LNout320_loss: 4.8022 - val_LNout321_loss: 4.6862 - val_LNout322_loss: 4.4150 - val_LNout323_loss: 5.4431 - val_LNout324_loss: 5.6041 - val_LNout325_loss: 4.6630 - val_LNout326_loss: 4.1514 - val_LNout327_loss: 5.0398 - val_LNout328_loss: 4.8460 - val_LNout329_loss: 5.6750 - val_LNout330_loss: 3.9122 - val_LNout331_loss: 5.2871 - val_LNout332_loss: 4.8326 - val_LNout333_loss: 5.7434 - val_LNout334_loss: 4.6585 - val_LNout335_loss: 4.1005 - val_LNout336_loss: 5.1374 - val_LNout337_loss: 5.9598 - val_LNout338_loss: 3.8959 - val_LNout339_loss: 4.1005 - val_LNout340_loss: 5.0248 - val_LNout341_loss: 5.5564 - val_LNout342_loss: 5.6058 - val_LNout343_loss: 4.1740 - val_LNout344_loss: 4.2139 - val_LNout345_loss: 4.5649 - val_LNout346_loss: 5.1705 - val_LNout347_loss: 5.3254 - val_LNout348_loss: 3.9933 - val_LNout349_loss: 4.0769 - val_LNout350_loss: 4.5547 - val_LNout351_loss: 4.7889 - val_LNout352_loss: 4.6754 - val_LNout353_loss: 4.8175 - val_LNout354_loss: 5.0756 - val_LNout355_loss: 5.5878 - val_LNout356_loss: 4.9103 - val_LNout357_loss: 5.4984 - val_LNout358_loss: 4.7184 - val_LNout359_loss: 4.1961 - val_error_feed_forward_output1_accuracy: 0.0213 - val_LNout0_accuracy: 0.0000e+00 - val_LNout1_accuracy: 0.0071 - val_LNout2_accuracy: 0.0021 - val_LNout3_accuracy: 0.0000e+00 - val_LNout4_accuracy: 0.0000e+00 - val_LNout5_accuracy: 0.0000e+00 - val_LNout6_accuracy: 0.7649 - val_LNout7_accuracy: 0.0000e+00 - val_LNout8_accuracy: 0.0000e+00 - val_LNout9_accuracy: 0.0000e+00 - val_LNout10_accuracy: 0.0000e+00 - val_LNout11_accuracy: 0.0000e+00 - val_LNout12_accuracy: 0.0000e+00 - val_LNout13_accuracy: 7.1023e-04 - val_LNout14_accuracy: 0.0000e+00 - val_LNout15_accuracy: 0.0000e+00 - val_LNout16_accuracy: 0.0000e+00 - val_LNout17_accuracy: 7.1023e-04 - val_LNout18_accuracy: 0.0000e+00 - val_LNout19_accuracy: 0.0000e+00 - val_LNout20_accuracy: 0.0000e+00 - val_LNout21_accuracy: 0.0000e+00 - val_LNout22_accuracy: 0.0000e+00 - val_LNout23_accuracy: 0.0000e+00 - val_LNout24_accuracy: 0.0000e+00 - val_LNout25_accuracy: 0.0000e+00 - val_LNout26_accuracy: 0.0000e+00 - val_LNout27_accuracy: 0.0000e+00 - val_LNout28_accuracy: 0.0000e+00 - val_LNout29_accuracy: 0.0000e+00 - val_LNout30_accuracy: 0.0000e+00 - val_LNout31_accuracy: 0.0000e+00 - val_LNout32_accuracy: 0.0000e+00 - val_LNout33_accuracy: 0.0000e+00 - val_LNout34_accuracy: 0.0000e+00 - val_LNout35_accuracy: 0.0000e+00 - val_LNout36_accuracy: 0.0000e+00 - val_LNout37_accuracy: 0.0000e+00 - val_LNout38_accuracy: 0.0000e+00 - val_LNout39_accuracy: 0.0000e+00 - val_LNout40_accuracy: 0.0000e+00 - val_LNout41_accuracy: 0.0000e+00 - val_LNout42_accuracy: 0.0000e+00 - val_LNout43_accuracy: 0.0000e+00 - val_LNout44_accuracy: 0.0000e+00 - val_LNout45_accuracy: 0.0000e+00 - val_LNout46_accuracy: 0.0000e+00 - val_LNout47_accuracy: 0.0000e+00 - val_LNout48_accuracy: 0.0000e+00 - val_LNout49_accuracy: 0.0000e+00 - val_LNout50_accuracy: 0.0000e+00 - val_LNout51_accuracy: 0.0000e+00 - val_LNout52_accuracy: 0.0000e+00 - val_LNout53_accuracy: 0.0000e+00 - val_LNout54_accuracy: 0.0000e+00 - val_LNout55_accuracy: 0.0000e+00 - val_LNout56_accuracy: 0.0000e+00 - val_LNout57_accuracy: 0.0000e+00 - val_LNout58_accuracy: 0.0000e+00 - val_LNout59_accuracy: 0.0000e+00 - val_LNout60_accuracy: 0.0000e+00 - val_LNout61_accuracy: 0.0000e+00 - val_LNout62_accuracy: 7.1023e-04 - val_LNout63_accuracy: 0.0000e+00 - val_LNout64_accuracy: 7.1023e-04 - val_LNout65_accuracy: 7.1023e-04 - val_LNout66_accuracy: 0.0000e+00 - val_LNout67_accuracy: 0.0000e+00 - val_LNout68_accuracy: 0.0000e+00 - val_LNout69_accuracy: 0.0000e+00 - val_LNout70_accuracy: 0.0000e+00 - val_LNout71_accuracy: 0.0000e+00 - val_LNout72_accuracy: 0.0000e+00 - val_LNout73_accuracy: 0.0000e+00 - val_LNout74_accuracy: 7.1023e-04 - val_LNout75_accuracy: 7.1023e-04 - val_LNout76_accuracy: 7.1023e-04 - val_LNout77_accuracy: 0.0000e+00 - val_LNout78_accuracy: 0.0000e+00 - val_LNout79_accuracy: 0.0000e+00 - val_LNout80_accuracy: 0.0000e+00 - val_LNout81_accuracy: 0.0000e+00 - val_LNout82_accuracy: 0.0000e+00 - val_LNout83_accuracy: 0.0000e+00 - val_LNout84_accuracy: 0.0000e+00 - val_LNout85_accuracy: 0.0000e+00 - val_LNout86_accuracy: 0.0000e+00 - val_LNout87_accuracy: 0.0000e+00 - val_LNout88_accuracy: 0.0000e+00 - val_LNout89_accuracy: 0.0000e+00 - val_LNout90_accuracy: 0.0000e+00 - val_LNout91_accuracy: 0.0000e+00 - val_LNout92_accuracy: 0.0000e+00 - val_LNout93_accuracy: 0.0000e+00 - val_LNout94_accuracy: 0.0000e+00 - val_LNout95_accuracy: 0.0000e+00 - val_LNout96_accuracy: 0.0000e+00 - val_LNout97_accuracy: 0.0000e+00 - val_LNout98_accuracy: 0.0000e+00 - val_LNout99_accuracy: 0.0000e+00 - val_LNout100_accuracy: 0.0000e+00 - val_LNout101_accuracy: 0.0000e+00 - val_LNout102_accuracy: 0.0000e+00 - val_LNout103_accuracy: 0.0000e+00 - val_LNout104_accuracy: 0.0000e+00 - val_LNout105_accuracy: 0.0000e+00 - val_LNout106_accuracy: 0.0000e+00 - val_LNout107_accuracy: 0.0000e+00 - val_LNout108_accuracy: 0.7273 - val_LNout109_accuracy: 0.0000e+00 - val_LNout110_accuracy: 0.0000e+00 - val_LNout111_accuracy: 0.0000e+00 - val_LNout112_accuracy: 0.8672 - val_LNout113_accuracy: 0.0000e+00 - val_LNout114_accuracy: 0.0000e+00 - val_LNout115_accuracy: 0.0000e+00 - val_LNout116_accuracy: 1.0000 - val_LNout117_accuracy: 0.0000e+00 - val_LNout118_accuracy: 0.0000e+00 - val_LNout119_accuracy: 0.0000e+00 - val_LNout120_accuracy: 0.0000e+00 - val_LNout121_accuracy: 0.0000e+00 - val_LNout122_accuracy: 0.0000e+00 - val_LNout123_accuracy: 0.0000e+00 - val_LNout124_accuracy: 0.0000e+00 - val_LNout125_accuracy: 0.0000e+00 - val_LNout126_accuracy: 0.0000e+00 - val_LNout127_accuracy: 0.0021 - val_LNout128_accuracy: 0.0000e+00 - val_LNout129_accuracy: 0.0000e+00 - val_LNout130_accuracy: 0.0000e+00 - val_LNout131_accuracy: 0.6186 - val_LNout132_accuracy: 0.0000e+00 - val_LNout133_accuracy: 0.0000e+00 - val_LNout134_accuracy: 0.0000e+00 - val_LNout135_accuracy: 0.0000e+00 - val_LNout136_accuracy: 0.0000e+00 - val_LNout137_accuracy: 0.0000e+00 - val_LNout138_accuracy: 0.0000e+00 - val_LNout139_accuracy: 0.0000e+00 - val_LNout140_accuracy: 0.0000e+00 - val_LNout141_accuracy: 0.0000e+00 - val_LNout142_accuracy: 0.0000e+00 - val_LNout143_accuracy: 0.0000e+00 - val_LNout144_accuracy: 0.0000e+00 - val_LNout145_accuracy: 0.0000e+00 - val_LNout146_accuracy: 0.0000e+00 - val_LNout147_accuracy: 0.0000e+00 - val_LNout148_accuracy: 0.0000e+00 - val_LNout149_accuracy: 0.0000e+00 - val_LNout150_accuracy: 0.0000e+00 - val_LNout151_accuracy: 0.0000e+00 - val_LNout152_accuracy: 0.0000e+00 - val_LNout153_accuracy: 0.0000e+00 - val_LNout154_accuracy: 0.0000e+00 - val_LNout155_accuracy: 0.0000e+00 - val_LNout156_accuracy: 0.0000e+00 - val_LNout157_accuracy: 0.0000e+00 - val_LNout158_accuracy: 0.0000e+00 - val_LNout159_accuracy: 0.0000e+00 - val_LNout160_accuracy: 0.0000e+00 - val_LNout161_accuracy: 0.0000e+00 - val_LNout162_accuracy: 0.0000e+00 - val_LNout163_accuracy: 0.0000e+00 - val_LNout164_accuracy: 0.0000e+00 - val_LNout165_accuracy: 1.0000 - val_LNout166_accuracy: 0.0000e+00 - val_LNout167_accuracy: 0.0000e+00 - val_LNout168_accuracy: 0.0000e+00 - val_LNout169_accuracy: 0.0000e+00 - val_LNout170_accuracy: 0.0000e+00 - val_LNout171_accuracy: 0.0000e+00 - val_LNout172_accuracy: 0.0000e+00 - val_LNout173_accuracy: 0.0000e+00 - val_LNout174_accuracy: 0.0000e+00 - val_LNout175_accuracy: 0.0000e+00 - val_LNout176_accuracy: 0.0000e+00 - val_LNout177_accuracy: 0.0000e+00 - val_LNout178_accuracy: 0.0000e+00 - val_LNout179_accuracy: 0.0000e+00 - val_LNout180_accuracy: 0.0000e+00 - val_LNout181_accuracy: 0.0000e+00 - val_LNout182_accuracy: 0.0000e+00 - val_LNout183_accuracy: 0.0000e+00 - val_LNout184_accuracy: 0.0000e+00 - val_LNout185_accuracy: 0.0000e+00 - val_LNout186_accuracy: 0.0000e+00 - val_LNout187_accuracy: 0.0000e+00 - val_LNout188_accuracy: 0.0000e+00 - val_LNout189_accuracy: 0.0000e+00 - val_LNout190_accuracy: 0.0000e+00 - val_LNout191_accuracy: 0.0000e+00 - val_LNout192_accuracy: 0.0000e+00 - val_LNout193_accuracy: 0.0000e+00 - val_LNout194_accuracy: 0.0000e+00 - val_LNout195_accuracy: 0.0000e+00 - val_LNout196_accuracy: 0.0000e+00 - val_LNout197_accuracy: 0.0000e+00 - val_LNout198_accuracy: 0.0000e+00 - val_LNout199_accuracy: 0.0000e+00 - val_LNout200_accuracy: 0.0000e+00 - val_LNout201_accuracy: 0.0000e+00 - val_LNout202_accuracy: 0.0000e+00 - val_LNout203_accuracy: 0.0000e+00 - val_LNout204_accuracy: 0.0000e+00 - val_LNout205_accuracy: 0.0000e+00 - val_LNout206_accuracy: 0.0000e+00 - val_LNout207_accuracy: 0.0000e+00 - val_LNout208_accuracy: 0.0000e+00 - val_LNout209_accuracy: 0.0000e+00 - val_LNout210_accuracy: 0.0000e+00 - val_LNout211_accuracy: 0.0000e+00 - val_LNout212_accuracy: 0.0000e+00 - val_LNout213_accuracy: 0.0000e+00 - val_LNout214_accuracy: 0.0000e+00 - val_LNout215_accuracy: 0.0050 - val_LNout216_accuracy: 0.0000e+00 - val_LNout217_accuracy: 0.0000e+00 - val_LNout218_accuracy: 0.0000e+00 - val_LNout219_accuracy: 0.0000e+00 - val_LNout220_accuracy: 0.0000e+00 - val_LNout221_accuracy: 0.0000e+00 - val_LNout222_accuracy: 0.0000e+00 - val_LNout223_accuracy: 0.3047 - val_LNout224_accuracy: 0.0000e+00 - val_LNout225_accuracy: 0.0000e+00 - val_LNout226_accuracy: 0.0000e+00 - val_LNout227_accuracy: 0.0000e+00 - val_LNout228_accuracy: 0.0000e+00 - val_LNout229_accuracy: 0.0000e+00 - val_LNout230_accuracy: 0.0000e+00 - val_LNout231_accuracy: 0.0000e+00 - val_LNout232_accuracy: 0.0000e+00 - val_LNout233_accuracy: 0.0000e+00 - val_LNout234_accuracy: 0.0000e+00 - val_LNout235_accuracy: 0.0000e+00 - val_LNout236_accuracy: 0.0000e+00 - val_LNout237_accuracy: 0.0000e+00 - val_LNout238_accuracy: 0.0000e+00 - val_LNout239_accuracy: 0.0000e+00 - val_LNout240_accuracy: 7.1023e-04 - val_LNout241_accuracy: 0.0000e+00 - val_LNout242_accuracy: 0.0000e+00 - val_LNout243_accuracy: 0.0000e+00 - val_LNout244_accuracy: 0.0000e+00 - val_LNout245_accuracy: 0.0000e+00 - val_LNout246_accuracy: 0.0000e+00 - val_LNout247_accuracy: 0.0000e+00 - val_LNout248_accuracy: 0.0000e+00 - val_LNout249_accuracy: 0.0000e+00 - val_LNout250_accuracy: 0.0000e+00 - val_LNout251_accuracy: 0.9986 - val_LNout252_accuracy: 0.0000e+00 - val_LNout253_accuracy: 0.0000e+00 - val_LNout254_accuracy: 0.0000e+00 - val_LNout255_accuracy: 0.0000e+00 - val_LNout256_accuracy: 0.0000e+00 - val_LNout257_accuracy: 0.0000e+00 - val_LNout258_accuracy: 0.0000e+00 - val_LNout259_accuracy: 0.0000e+00 - val_LNout260_accuracy: 0.0000e+00 - val_LNout261_accuracy: 0.0000e+00 - val_LNout262_accuracy: 0.0000e+00 - val_LNout263_accuracy: 0.8942 - val_LNout264_accuracy: 0.0000e+00 - val_LNout265_accuracy: 0.0000e+00 - val_LNout266_accuracy: 0.0000e+00 - val_LNout267_accuracy: 0.0000e+00 - val_LNout268_accuracy: 0.0000e+00 - val_LNout269_accuracy: 0.0000e+00 - val_LNout270_accuracy: 0.0000e+00 - val_LNout271_accuracy: 0.0000e+00 - val_LNout272_accuracy: 0.0000e+00 - val_LNout273_accuracy: 0.0000e+00 - val_LNout274_accuracy: 0.0000e+00 - val_LNout275_accuracy: 0.0000e+00 - val_LNout276_accuracy: 0.0000e+00 - val_LNout277_accuracy: 0.0000e+00 - val_LNout278_accuracy: 0.0000e+00 - val_LNout279_accuracy: 0.0000e+00 - val_LNout280_accuracy: 0.0902 - val_LNout281_accuracy: 0.0000e+00 - val_LNout282_accuracy: 0.0000e+00 - val_LNout283_accuracy: 0.0000e+00 - val_LNout284_accuracy: 0.0000e+00 - val_LNout285_accuracy: 0.0000e+00 - val_LNout286_accuracy: 0.0000e+00 - val_LNout287_accuracy: 0.0000e+00 - val_LNout288_accuracy: 0.0000e+00 - val_LNout289_accuracy: 0.0000e+00 - val_LNout290_accuracy: 0.0000e+00 - val_LNout291_accuracy: 0.0000e+00 - val_LNout292_accuracy: 0.0000e+00 - val_LNout293_accuracy: 0.0000e+00 - val_LNout294_accuracy: 0.0000e+00 - val_LNout295_accuracy: 0.0000e+00 - val_LNout296_accuracy: 0.0000e+00 - val_LNout297_accuracy: 0.0000e+00 - val_LNout298_accuracy: 0.0000e+00 - val_LNout299_accuracy: 0.0000e+00 - val_LNout300_accuracy: 7.1023e-04 - val_LNout301_accuracy: 0.0000e+00 - val_LNout302_accuracy: 0.0000e+00 - val_LNout303_accuracy: 0.0256 - val_LNout304_accuracy: 0.0000e+00 - val_LNout305_accuracy: 0.0000e+00 - val_LNout306_accuracy: 0.0000e+00 - val_LNout307_accuracy: 0.0000e+00 - val_LNout308_accuracy: 0.0000e+00 - val_LNout309_accuracy: 0.0000e+00 - val_LNout310_accuracy: 0.0000e+00 - val_LNout311_accuracy: 0.0000e+00 - val_LNout312_accuracy: 0.0000e+00 - val_LNout313_accuracy: 0.0000e+00 - val_LNout314_accuracy: 0.0000e+00 - val_LNout315_accuracy: 0.0000e+00 - val_LNout316_accuracy: 0.0000e+00 - val_LNout317_accuracy: 0.0000e+00 - val_LNout318_accuracy: 0.0000e+00 - val_LNout319_accuracy: 0.0000e+00 - val_LNout320_accuracy: 0.0000e+00 - val_LNout321_accuracy: 0.0000e+00 - val_LNout322_accuracy: 0.0000e+00 - val_LNout323_accuracy: 0.0000e+00 - val_LNout324_accuracy: 0.0000e+00 - val_LNout325_accuracy: 0.0000e+00 - val_LNout326_accuracy: 0.0000e+00 - val_LNout327_accuracy: 0.0000e+00 - val_LNout328_accuracy: 0.0000e+00 - val_LNout329_accuracy: 0.0000e+00 - val_LNout330_accuracy: 0.0000e+00 - val_LNout331_accuracy: 0.0000e+00 - val_LNout332_accuracy: 0.0000e+00 - val_LNout333_accuracy: 0.0000e+00 - val_LNout334_accuracy: 0.0000e+00 - val_LNout335_accuracy: 0.1733 - val_LNout336_accuracy: 0.0000e+00 - val_LNout337_accuracy: 0.0000e+00 - val_LNout338_accuracy: 0.3139 - val_LNout339_accuracy: 0.0000e+00 - val_LNout340_accuracy: 0.0000e+00 - val_LNout341_accuracy: 0.0000e+00 - val_LNout342_accuracy: 0.0000e+00 - val_LNout343_accuracy: 0.0000e+00 - val_LNout344_accuracy: 0.0000e+00 - val_LNout345_accuracy: 0.0000e+00 - val_LNout346_accuracy: 0.0000e+00 - val_LNout347_accuracy: 0.0000e+00 - val_LNout348_accuracy: 0.1371 - val_LNout349_accuracy: 0.0000e+00 - val_LNout350_accuracy: 0.0000e+00 - val_LNout351_accuracy: 0.0000e+00 - val_LNout352_accuracy: 0.0000e+00 - val_LNout353_accuracy: 0.0000e+00 - val_LNout354_accuracy: 0.0000e+00 - val_LNout355_accuracy: 0.0000e+00 - val_LNout356_accuracy: 0.0000e+00 - val_LNout357_accuracy: 0.0000e+00 - val_LNout358_accuracy: 0.0000e+00 - val_LNout359_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "105/105 - 327s - loss: 178764.4531 - error_feed_forward_output1_loss: 0.3651 - LNout0_loss: 5.2531 - LNout1_loss: 5.1507 - LNout2_loss: 4.3558 - LNout3_loss: 4.9244 - LNout4_loss: 4.7860 - LNout5_loss: 4.6093 - LNout6_loss: 3.9211 - LNout7_loss: 5.0234 - LNout8_loss: 4.3606 - LNout9_loss: 4.5403 - LNout10_loss: 5.6438 - LNout11_loss: 5.3184 - LNout12_loss: 5.4635 - LNout13_loss: 5.0948 - LNout14_loss: 5.7446 - LNout15_loss: 5.2664 - LNout16_loss: 5.1234 - LNout17_loss: 5.3710 - LNout18_loss: 5.8459 - LNout19_loss: 4.4675 - LNout20_loss: 4.5674 - LNout21_loss: 5.2673 - LNout22_loss: 5.1923 - LNout23_loss: 4.4956 - LNout24_loss: 5.9075 - LNout25_loss: 5.2290 - LNout26_loss: 4.9844 - LNout27_loss: 4.8887 - LNout28_loss: 4.3656 - LNout29_loss: 5.6025 - LNout30_loss: 4.3514 - LNout31_loss: 3.9924 - LNout32_loss: 5.4418 - LNout33_loss: 5.1002 - LNout34_loss: 5.3830 - LNout35_loss: 4.9342 - LNout36_loss: 5.8831 - LNout37_loss: 5.2287 - LNout38_loss: 5.6077 - LNout39_loss: 4.7507 - LNout40_loss: 4.7193 - LNout41_loss: 5.5173 - LNout42_loss: 5.0718 - LNout43_loss: 5.3279 - LNout44_loss: 4.7998 - LNout45_loss: 5.0943 - LNout46_loss: 4.9944 - LNout47_loss: 4.5311 - LNout48_loss: 5.3159 - LNout49_loss: 5.4607 - LNout50_loss: 4.4855 - LNout51_loss: 5.2795 - LNout52_loss: 4.9800 - LNout53_loss: 5.0244 - LNout54_loss: 5.3244 - LNout55_loss: 4.3351 - LNout56_loss: 4.6239 - LNout57_loss: 5.4400 - LNout58_loss: 5.2727 - LNout59_loss: 5.0311 - LNout60_loss: 4.7895 - LNout61_loss: 4.6002 - LNout62_loss: 5.3190 - LNout63_loss: 4.6708 - LNout64_loss: 5.1653 - LNout65_loss: 5.0131 - LNout66_loss: 4.4970 - LNout67_loss: 4.5969 - LNout68_loss: 4.3673 - LNout69_loss: 5.0247 - LNout70_loss: 4.3913 - LNout71_loss: 5.0405 - LNout72_loss: 4.6940 - LNout73_loss: 4.4466 - LNout74_loss: 4.8206 - LNout75_loss: 5.5670 - LNout76_loss: 5.1051 - LNout77_loss: 5.4765 - LNout78_loss: 4.7724 - LNout79_loss: 5.7767 - LNout80_loss: 4.6907 - LNout81_loss: 5.2945 - LNout82_loss: 5.3576 - LNout83_loss: 4.7197 - LNout84_loss: 4.7351 - LNout85_loss: 4.5449 - LNout86_loss: 4.9444 - LNout87_loss: 5.5906 - LNout88_loss: 5.2743 - LNout89_loss: 4.6463 - LNout90_loss: 5.6364 - LNout91_loss: 4.7471 - LNout92_loss: 4.3137 - LNout93_loss: 4.6837 - LNout94_loss: 5.8506 - LNout95_loss: 4.9149 - LNout96_loss: 5.4344 - LNout97_loss: 5.6639 - LNout98_loss: 4.8536 - LNout99_loss: 4.5204 - LNout100_loss: 5.4466 - LNout101_loss: 4.9105 - LNout102_loss: 4.5818 - LNout103_loss: 5.0763 - LNout104_loss: 5.1865 - LNout105_loss: 5.3297 - LNout106_loss: 4.6230 - LNout107_loss: 4.6114 - LNout108_loss: 3.7972 - LNout109_loss: 5.0007 - LNout110_loss: 4.3382 - LNout111_loss: 5.4825 - LNout112_loss: 3.8862 - LNout113_loss: 5.4157 - LNout114_loss: 4.7391 - LNout115_loss: 4.4318 - LNout116_loss: 3.6342 - LNout117_loss: 5.2104 - LNout118_loss: 4.4931 - LNout119_loss: 4.5950 - LNout120_loss: 4.6597 - LNout121_loss: 5.1542 - LNout122_loss: 5.6078 - LNout123_loss: 5.9135 - LNout124_loss: 5.0525 - LNout125_loss: 4.5720 - LNout126_loss: 4.5410 - LNout127_loss: 5.5972 - LNout128_loss: 5.2473 - LNout129_loss: 6.2936 - LNout130_loss: 5.1269 - LNout131_loss: 4.0584 - LNout132_loss: 4.5423 - LNout133_loss: 5.2944 - LNout134_loss: 5.4983 - LNout135_loss: 4.2562 - LNout136_loss: 4.4293 - LNout137_loss: 4.6395 - LNout138_loss: 5.6515 - LNout139_loss: 5.8994 - LNout140_loss: 5.8379 - LNout141_loss: 5.3969 - LNout142_loss: 4.9832 - LNout143_loss: 4.8712 - LNout144_loss: 4.1750 - LNout145_loss: 5.0092 - LNout146_loss: 5.0033 - LNout147_loss: 5.2121 - LNout148_loss: 4.8828 - LNout149_loss: 5.4000 - LNout150_loss: 5.3290 - LNout151_loss: 4.5457 - LNout152_loss: 5.1834 - LNout153_loss: 4.9149 - LNout154_loss: 4.9196 - LNout155_loss: 4.9937 - LNout156_loss: 5.7990 - LNout157_loss: 5.0876 - LNout158_loss: 5.6769 - LNout159_loss: 5.0804 - LNout160_loss: 5.1204 - LNout161_loss: 6.1905 - LNout162_loss: 4.8362 - LNout163_loss: 5.7140 - LNout164_loss: 4.8824 - LNout165_loss: 3.9063 - LNout166_loss: 4.8643 - LNout167_loss: 4.1843 - LNout168_loss: 4.8823 - LNout169_loss: 5.2348 - LNout170_loss: 5.5131 - LNout171_loss: 4.2374 - LNout172_loss: 5.8477 - LNout173_loss: 5.5571 - LNout174_loss: 4.6691 - LNout175_loss: 5.0470 - LNout176_loss: 5.6293 - LNout177_loss: 4.4045 - LNout178_loss: 4.9935 - LNout179_loss: 6.2087 - LNout180_loss: 4.8499 - LNout181_loss: 5.1153 - LNout182_loss: 4.9141 - LNout183_loss: 5.2113 - LNout184_loss: 4.7109 - LNout185_loss: 4.4323 - LNout186_loss: 5.1890 - LNout187_loss: 5.0133 - LNout188_loss: 5.2308 - LNout189_loss: 4.9038 - LNout190_loss: 4.8166 - LNout191_loss: 4.6742 - LNout192_loss: 4.9966 - LNout193_loss: 4.3986 - LNout194_loss: 5.5635 - LNout195_loss: 5.3365 - LNout196_loss: 4.6908 - LNout197_loss: 5.0625 - LNout198_loss: 5.6344 - LNout199_loss: 4.9258 - LNout200_loss: 4.8231 - LNout201_loss: 5.8578 - LNout202_loss: 5.5264 - LNout203_loss: 4.8234 - LNout204_loss: 4.8333 - LNout205_loss: 4.7566 - LNout206_loss: 4.3500 - LNout207_loss: 5.1960 - LNout208_loss: 4.5658 - LNout209_loss: 4.5802 - LNout210_loss: 4.3325 - LNout211_loss: 4.9572 - LNout212_loss: 5.2242 - LNout213_loss: 5.4449 - LNout214_loss: 4.0691 - LNout215_loss: 3.8456 - LNout216_loss: 5.4282 - LNout217_loss: 4.5060 - LNout218_loss: 5.1184 - LNout219_loss: 5.4146 - LNout220_loss: 5.1257 - LNout221_loss: 4.6922 - LNout222_loss: 4.9562 - LNout223_loss: 4.2235 - LNout224_loss: 4.7541 - LNout225_loss: 5.7492 - LNout226_loss: 4.8443 - LNout227_loss: 5.0895 - LNout228_loss: 4.5693 - LNout229_loss: 5.2967 - LNout230_loss: 5.3583 - LNout231_loss: 4.4989 - LNout232_loss: 5.8685 - LNout233_loss: 5.2646 - LNout234_loss: 4.1845 - LNout235_loss: 4.7979 - LNout236_loss: 5.2310 - LNout237_loss: 5.2071 - LNout238_loss: 4.3194 - LNout239_loss: 5.5693 - LNout240_loss: 4.4604 - LNout241_loss: 4.7843 - LNout242_loss: 4.0797 - LNout243_loss: 4.7926 - LNout244_loss: 4.9327 - LNout245_loss: 4.9686 - LNout246_loss: 4.5113 - LNout247_loss: 4.9556 - LNout248_loss: 5.8367 - LNout249_loss: 4.3120 - LNout250_loss: 5.9220 - LNout251_loss: 3.8220 - LNout252_loss: 5.8521 - LNout253_loss: 5.3491 - LNout254_loss: 4.7657 - LNout255_loss: 4.8140 - LNout256_loss: 5.3883 - LNout257_loss: 4.9559 - LNout258_loss: 5.5325 - LNout259_loss: 4.8993 - LNout260_loss: 4.1960 - LNout261_loss: 4.5081 - LNout262_loss: 5.6207 - LNout263_loss: 3.9371 - LNout264_loss: 5.5807 - LNout265_loss: 5.8316 - LNout266_loss: 5.2525 - LNout267_loss: 4.9987 - LNout268_loss: 4.9300 - LNout269_loss: 5.3156 - LNout270_loss: 5.2544 - LNout271_loss: 5.3678 - LNout272_loss: 4.6988 - LNout273_loss: 4.5228 - LNout274_loss: 4.7937 - LNout275_loss: 4.5075 - LNout276_loss: 4.3562 - LNout277_loss: 5.7701 - LNout278_loss: 4.6777 - LNout279_loss: 4.3467 - LNout280_loss: 4.0833 - LNout281_loss: 5.8405 - LNout282_loss: 4.6492 - LNout283_loss: 5.0968 - LNout284_loss: 4.8625 - LNout285_loss: 4.3294 - LNout286_loss: 5.3402 - LNout287_loss: 4.9374 - LNout288_loss: 4.8430 - LNout289_loss: 4.5494 - LNout290_loss: 5.1992 - LNout291_loss: 5.1576 - LNout292_loss: 5.3721 - LNout293_loss: 4.4846 - LNout294_loss: 6.0357 - LNout295_loss: 4.8166 - LNout296_loss: 4.3962 - LNout297_loss: 4.9150 - LNout298_loss: 4.5566 - LNout299_loss: 5.8119 - LNout300_loss: 4.3565 - LNout301_loss: 5.4217 - LNout302_loss: 5.3313 - LNout303_loss: 3.7811 - LNout304_loss: 5.8069 - LNout305_loss: 5.2074 - LNout306_loss: 4.9124 - LNout307_loss: 5.1531 - LNout308_loss: 5.9836 - LNout309_loss: 5.1285 - LNout310_loss: 5.2076 - LNout311_loss: 4.7983 - LNout312_loss: 5.1987 - LNout313_loss: 4.6840 - LNout314_loss: 4.4884 - LNout315_loss: 4.5247 - LNout316_loss: 4.3247 - LNout317_loss: 5.5295 - LNout318_loss: 4.9520 - LNout319_loss: 4.3307 - LNout320_loss: 4.8487 - LNout321_loss: 4.7097 - LNout322_loss: 4.4251 - LNout323_loss: 5.4522 - LNout324_loss: 5.5448 - LNout325_loss: 4.6653 - LNout326_loss: 4.1945 - LNout327_loss: 5.0448 - LNout328_loss: 4.8319 - LNout329_loss: 5.6501 - LNout330_loss: 3.9397 - LNout331_loss: 5.2294 - LNout332_loss: 4.8021 - LNout333_loss: 5.7118 - LNout334_loss: 4.6850 - LNout335_loss: 4.1917 - LNout336_loss: 5.1234 - LNout337_loss: 5.8806 - LNout338_loss: 3.9240 - LNout339_loss: 4.1502 - LNout340_loss: 4.9752 - LNout341_loss: 5.4341 - LNout342_loss: 5.6175 - LNout343_loss: 4.1698 - LNout344_loss: 4.2123 - LNout345_loss: 4.5168 - LNout346_loss: 5.0821 - LNout347_loss: 5.3207 - LNout348_loss: 4.0144 - LNout349_loss: 4.1515 - LNout350_loss: 4.6366 - LNout351_loss: 4.8656 - LNout352_loss: 4.6169 - LNout353_loss: 4.8751 - LNout354_loss: 5.0628 - LNout355_loss: 5.5852 - LNout356_loss: 4.8218 - LNout357_loss: 5.4957 - LNout358_loss: 4.7159 - LNout359_loss: 4.0737 - error_feed_forward_output1_accuracy: 0.0281 - LNout0_accuracy: 2.2321e-04 - LNout1_accuracy: 0.0050 - LNout2_accuracy: 0.0378 - LNout3_accuracy: 0.0000e+00 - LNout4_accuracy: 7.4405e-04 - LNout5_accuracy: 4.4643e-04 - LNout6_accuracy: 0.4507 - LNout7_accuracy: 0.0000e+00 - LNout8_accuracy: 0.0275 - LNout9_accuracy: 1.4881e-04 - LNout10_accuracy: 0.0000e+00 - LNout11_accuracy: 0.0000e+00 - LNout12_accuracy: 7.4405e-05 - LNout13_accuracy: 5.2083e-04 - LNout14_accuracy: 7.4405e-05 - LNout15_accuracy: 0.0000e+00 - LNout16_accuracy: 0.0000e+00 - LNout17_accuracy: 0.0000e+00 - LNout18_accuracy: 0.0000e+00 - LNout19_accuracy: 0.0000e+00 - LNout20_accuracy: 5.2083e-04 - LNout21_accuracy: 0.0000e+00 - LNout22_accuracy: 2.2321e-04 - LNout23_accuracy: 3.7202e-04 - LNout24_accuracy: 0.0000e+00 - LNout25_accuracy: 1.4881e-04 - LNout26_accuracy: 1.4881e-04 - LNout27_accuracy: 7.4405e-05 - LNout28_accuracy: 0.0022 - LNout29_accuracy: 2.2321e-04 - LNout30_accuracy: 7.4405e-05 - LNout31_accuracy: 0.0717 - LNout32_accuracy: 0.0000e+00 - LNout33_accuracy: 0.0000e+00 - LNout34_accuracy: 0.0000e+00 - LNout35_accuracy: 0.0000e+00 - LNout36_accuracy: 7.4405e-05 - LNout37_accuracy: 0.0000e+00 - LNout38_accuracy: 0.0000e+00 - LNout39_accuracy: 0.0000e+00 - LNout40_accuracy: 0.0000e+00 - LNout41_accuracy: 0.0000e+00 - LNout42_accuracy: 0.0000e+00 - LNout43_accuracy: 0.0000e+00 - LNout44_accuracy: 0.0000e+00 - LNout45_accuracy: 0.0000e+00 - LNout46_accuracy: 0.0000e+00 - LNout47_accuracy: 0.0000e+00 - LNout48_accuracy: 0.0000e+00 - LNout49_accuracy: 0.0000e+00 - LNout50_accuracy: 7.4405e-04 - LNout51_accuracy: 0.0000e+00 - LNout52_accuracy: 0.0000e+00 - LNout53_accuracy: 0.0000e+00 - LNout54_accuracy: 0.0000e+00 - LNout55_accuracy: 0.0138 - LNout56_accuracy: 0.0000e+00 - LNout57_accuracy: 0.0000e+00 - LNout58_accuracy: 0.0000e+00 - LNout59_accuracy: 0.0000e+00 - LNout60_accuracy: 2.2321e-04 - LNout61_accuracy: 0.0655 - LNout62_accuracy: 0.0022 - LNout63_accuracy: 0.0000e+00 - LNout64_accuracy: 1.4881e-04 - LNout65_accuracy: 0.0016 - LNout66_accuracy: 7.4405e-05 - LNout67_accuracy: 1.4881e-04 - LNout68_accuracy: 0.0000e+00 - LNout69_accuracy: 0.0000e+00 - LNout70_accuracy: 6.6964e-04 - LNout71_accuracy: 0.0000e+00 - LNout72_accuracy: 0.0000e+00 - LNout73_accuracy: 0.0013 - LNout74_accuracy: 1.4881e-04 - LNout75_accuracy: 2.9762e-04 - LNout76_accuracy: 2.9762e-04 - LNout77_accuracy: 0.0000e+00 - LNout78_accuracy: 0.0000e+00 - LNout79_accuracy: 7.4405e-05 - LNout80_accuracy: 7.4405e-05 - LNout81_accuracy: 0.0000e+00 - LNout82_accuracy: 0.0000e+00 - LNout83_accuracy: 7.4405e-05 - LNout84_accuracy: 0.0000e+00 - LNout85_accuracy: 7.4405e-05 - LNout86_accuracy: 0.0000e+00 - LNout87_accuracy: 0.0000e+00 - LNout88_accuracy: 0.0000e+00 - LNout89_accuracy: 0.0000e+00 - LNout90_accuracy: 0.0000e+00 - LNout91_accuracy: 0.0000e+00 - LNout92_accuracy: 0.0097 - LNout93_accuracy: 0.0000e+00 - LNout94_accuracy: 0.0000e+00 - LNout95_accuracy: 0.0000e+00 - LNout96_accuracy: 0.0000e+00 - LNout97_accuracy: 0.0000e+00 - LNout98_accuracy: 0.0000e+00 - LNout99_accuracy: 0.0011 - LNout100_accuracy: 0.0000e+00 - LNout101_accuracy: 0.0000e+00 - LNout102_accuracy: 0.0000e+00 - LNout103_accuracy: 0.0000e+00 - LNout104_accuracy: 0.0000e+00 - LNout105_accuracy: 0.0000e+00 - LNout106_accuracy: 0.0000e+00 - LNout107_accuracy: 0.0000e+00 - LNout108_accuracy: 0.3017 - LNout109_accuracy: 0.0000e+00 - LNout110_accuracy: 2.2321e-04 - LNout111_accuracy: 0.0000e+00 - LNout112_accuracy: 0.7321 - LNout113_accuracy: 0.0000e+00 - LNout114_accuracy: 0.0000e+00 - LNout115_accuracy: 0.0017 - LNout116_accuracy: 0.9940 - LNout117_accuracy: 0.0000e+00 - LNout118_accuracy: 7.4405e-05 - LNout119_accuracy: 0.0000e+00 - LNout120_accuracy: 5.2083e-04 - LNout121_accuracy: 0.0000e+00 - LNout122_accuracy: 0.0016 - LNout123_accuracy: 1.4881e-04 - LNout124_accuracy: 6.6964e-04 - LNout125_accuracy: 7.4405e-05 - LNout126_accuracy: 0.0000e+00 - LNout127_accuracy: 0.0010 - LNout128_accuracy: 2.9762e-04 - LNout129_accuracy: 0.0000e+00 - LNout130_accuracy: 0.0000e+00 - LNout131_accuracy: 0.6226 - LNout132_accuracy: 0.0000e+00 - LNout133_accuracy: 0.0000e+00 - LNout134_accuracy: 0.0000e+00 - LNout135_accuracy: 0.0000e+00 - LNout136_accuracy: 0.0000e+00 - LNout137_accuracy: 0.0000e+00 - LNout138_accuracy: 0.0000e+00 - LNout139_accuracy: 1.4881e-04 - LNout140_accuracy: 0.0000e+00 - LNout141_accuracy: 0.0000e+00 - LNout142_accuracy: 0.0000e+00 - LNout143_accuracy: 0.0000e+00 - LNout144_accuracy: 0.0127 - LNout145_accuracy: 0.0000e+00 - LNout146_accuracy: 0.0000e+00 - LNout147_accuracy: 0.0000e+00 - LNout148_accuracy: 0.0000e+00 - LNout149_accuracy: 0.0000e+00 - LNout150_accuracy: 0.0000e+00 - LNout151_accuracy: 4.4643e-04 - LNout152_accuracy: 0.0000e+00 - LNout153_accuracy: 0.0000e+00 - LNout154_accuracy: 0.0000e+00 - LNout155_accuracy: 0.0000e+00 - LNout156_accuracy: 0.0000e+00 - LNout157_accuracy: 0.0000e+00 - LNout158_accuracy: 0.0000e+00 - LNout159_accuracy: 0.0000e+00 - LNout160_accuracy: 0.0000e+00 - LNout161_accuracy: 0.0000e+00 - LNout162_accuracy: 0.0000e+00 - LNout163_accuracy: 0.0000e+00 - LNout164_accuracy: 0.0000e+00 - LNout165_accuracy: 0.8448 - LNout166_accuracy: 0.0000e+00 - LNout167_accuracy: 7.4405e-05 - LNout168_accuracy: 0.0000e+00 - LNout169_accuracy: 0.0000e+00 - LNout170_accuracy: 0.0000e+00 - LNout171_accuracy: 0.0386 - LNout172_accuracy: 0.0000e+00 - LNout173_accuracy: 0.0000e+00 - LNout174_accuracy: 0.0000e+00 - LNout175_accuracy: 0.0000e+00 - LNout176_accuracy: 0.0000e+00 - LNout177_accuracy: 2.2321e-04 - LNout178_accuracy: 0.0000e+00 - LNout179_accuracy: 0.0000e+00 - LNout180_accuracy: 7.4405e-05 - LNout181_accuracy: 0.0000e+00 - LNout182_accuracy: 3.7202e-04 - LNout183_accuracy: 5.9524e-04 - LNout184_accuracy: 0.0000e+00 - LNout185_accuracy: 5.9524e-04 - LNout186_accuracy: 0.0000e+00 - LNout187_accuracy: 0.0000e+00 - LNout188_accuracy: 0.0000e+00 - LNout189_accuracy: 0.0000e+00 - LNout190_accuracy: 0.0000e+00 - LNout191_accuracy: 0.0000e+00 - LNout192_accuracy: 0.0000e+00 - LNout193_accuracy: 0.0044 - LNout194_accuracy: 0.0000e+00 - LNout195_accuracy: 0.0000e+00 - LNout196_accuracy: 0.0000e+00 - LNout197_accuracy: 0.0000e+00 - LNout198_accuracy: 0.0000e+00 - LNout199_accuracy: 0.0000e+00 - LNout200_accuracy: 0.0000e+00 - LNout201_accuracy: 0.0000e+00 - LNout202_accuracy: 0.0000e+00 - LNout203_accuracy: 0.0000e+00 - LNout204_accuracy: 0.0000e+00 - LNout205_accuracy: 0.0000e+00 - LNout206_accuracy: 3.7202e-04 - LNout207_accuracy: 0.0000e+00 - LNout208_accuracy: 0.0000e+00 - LNout209_accuracy: 0.0000e+00 - LNout210_accuracy: 0.0200 - LNout211_accuracy: 0.0000e+00 - LNout212_accuracy: 0.0000e+00 - LNout213_accuracy: 0.0000e+00 - LNout214_accuracy: 0.0132 - LNout215_accuracy: 0.1944 - LNout216_accuracy: 0.0000e+00 - LNout217_accuracy: 4.4643e-04 - LNout218_accuracy: 0.0000e+00 - LNout219_accuracy: 0.0000e+00 - LNout220_accuracy: 0.0000e+00 - LNout221_accuracy: 0.0000e+00 - LNout222_accuracy: 0.0000e+00 - LNout223_accuracy: 0.2788 - LNout224_accuracy: 0.0000e+00 - LNout225_accuracy: 0.0000e+00 - LNout226_accuracy: 0.0000e+00 - LNout227_accuracy: 0.0000e+00 - LNout228_accuracy: 0.0000e+00 - LNout229_accuracy: 0.0000e+00 - LNout230_accuracy: 0.0000e+00 - LNout231_accuracy: 0.0074 - LNout232_accuracy: 0.0000e+00 - LNout233_accuracy: 0.0000e+00 - LNout234_accuracy: 1.4881e-04 - LNout235_accuracy: 0.0000e+00 - LNout236_accuracy: 0.0000e+00 - LNout237_accuracy: 0.0000e+00 - LNout238_accuracy: 0.0172 - LNout239_accuracy: 0.0000e+00 - LNout240_accuracy: 5.9524e-04 - LNout241_accuracy: 0.0000e+00 - LNout242_accuracy: 7.4405e-05 - LNout243_accuracy: 0.0000e+00 - LNout244_accuracy: 7.4405e-05 - LNout245_accuracy: 0.0000e+00 - LNout246_accuracy: 0.0000e+00 - LNout247_accuracy: 0.0000e+00 - LNout248_accuracy: 0.0000e+00 - LNout249_accuracy: 2.9762e-04 - LNout250_accuracy: 0.0000e+00 - LNout251_accuracy: 0.9036 - LNout252_accuracy: 0.0000e+00 - LNout253_accuracy: 0.0000e+00 - LNout254_accuracy: 0.0000e+00 - LNout255_accuracy: 0.0000e+00 - LNout256_accuracy: 0.0000e+00 - LNout257_accuracy: 0.0000e+00 - LNout258_accuracy: 0.0000e+00 - LNout259_accuracy: 0.0000e+00 - LNout260_accuracy: 0.0339 - LNout261_accuracy: 0.0000e+00 - LNout262_accuracy: 0.0000e+00 - LNout263_accuracy: 0.3525 - LNout264_accuracy: 0.0000e+00 - LNout265_accuracy: 0.0000e+00 - LNout266_accuracy: 0.0000e+00 - LNout267_accuracy: 0.0000e+00 - LNout268_accuracy: 0.0000e+00 - LNout269_accuracy: 0.0000e+00 - LNout270_accuracy: 0.0000e+00 - LNout271_accuracy: 0.0000e+00 - LNout272_accuracy: 0.0000e+00 - LNout273_accuracy: 0.0000e+00 - LNout274_accuracy: 0.0000e+00 - LNout275_accuracy: 0.0000e+00 - LNout276_accuracy: 5.9524e-04 - LNout277_accuracy: 0.0000e+00 - LNout278_accuracy: 0.0000e+00 - LNout279_accuracy: 0.0065 - LNout280_accuracy: 0.1577 - LNout281_accuracy: 0.0000e+00 - LNout282_accuracy: 0.0000e+00 - LNout283_accuracy: 0.0000e+00 - LNout284_accuracy: 0.0000e+00 - LNout285_accuracy: 8.1845e-04 - LNout286_accuracy: 0.0000e+00 - LNout287_accuracy: 0.0000e+00 - LNout288_accuracy: 0.0000e+00 - LNout289_accuracy: 0.0000e+00 - LNout290_accuracy: 0.0000e+00 - LNout291_accuracy: 0.0000e+00 - LNout292_accuracy: 0.0000e+00 - LNout293_accuracy: 2.2321e-04 - LNout294_accuracy: 0.0000e+00 - LNout295_accuracy: 0.0000e+00 - LNout296_accuracy: 0.0000e+00 - LNout297_accuracy: 0.0000e+00 - LNout298_accuracy: 2.2321e-04 - LNout299_accuracy: 0.0000e+00 - LNout300_accuracy: 0.0101 - LNout301_accuracy: 0.0000e+00 - LNout302_accuracy: 1.4881e-04 - LNout303_accuracy: 0.2147 - LNout304_accuracy: 7.4405e-05 - LNout305_accuracy: 0.0000e+00 - LNout306_accuracy: 0.0000e+00 - LNout307_accuracy: 0.0000e+00 - LNout308_accuracy: 0.0000e+00 - LNout309_accuracy: 0.0000e+00 - LNout310_accuracy: 0.0000e+00 - LNout311_accuracy: 0.0000e+00 - LNout312_accuracy: 0.0000e+00 - LNout313_accuracy: 0.0000e+00 - LNout314_accuracy: 0.0100 - LNout315_accuracy: 0.0000e+00 - LNout316_accuracy: 0.0046 - LNout317_accuracy: 0.0000e+00 - LNout318_accuracy: 0.0000e+00 - LNout319_accuracy: 0.0012 - LNout320_accuracy: 0.0000e+00 - LNout321_accuracy: 0.0000e+00 - LNout322_accuracy: 0.0000e+00 - LNout323_accuracy: 0.0000e+00 - LNout324_accuracy: 0.0000e+00 - LNout325_accuracy: 0.0000e+00 - LNout326_accuracy: 0.0522 - LNout327_accuracy: 0.0000e+00 - LNout328_accuracy: 0.0000e+00 - LNout329_accuracy: 0.0000e+00 - LNout330_accuracy: 0.0243 - LNout331_accuracy: 0.0000e+00 - LNout332_accuracy: 0.0000e+00 - LNout333_accuracy: 0.0000e+00 - LNout334_accuracy: 0.0000e+00 - LNout335_accuracy: 0.0853 - LNout336_accuracy: 0.0000e+00 - LNout337_accuracy: 0.0000e+00 - LNout338_accuracy: 0.3254 - LNout339_accuracy: 0.0691 - LNout340_accuracy: 0.0000e+00 - LNout341_accuracy: 0.0000e+00 - LNout342_accuracy: 0.0000e+00 - LNout343_accuracy: 0.0128 - LNout344_accuracy: 0.0077 - LNout345_accuracy: 2.2321e-04 - LNout346_accuracy: 0.0000e+00 - LNout347_accuracy: 0.0000e+00 - LNout348_accuracy: 0.3667 - LNout349_accuracy: 0.0344 - LNout350_accuracy: 0.0000e+00 - LNout351_accuracy: 0.0000e+00 - LNout352_accuracy: 0.0000e+00 - LNout353_accuracy: 0.0000e+00 - LNout354_accuracy: 0.0000e+00 - LNout355_accuracy: 0.0000e+00 - LNout356_accuracy: 0.0000e+00 - LNout357_accuracy: 0.0000e+00 - LNout358_accuracy: 0.0000e+00 - LNout359_accuracy: 0.0228 - val_loss: 178165.9375 - val_error_feed_forward_output1_loss: 0.3448 - val_LNout0_loss: 5.2852 - val_LNout1_loss: 5.1876 - val_LNout2_loss: 4.2655 - val_LNout3_loss: 4.9099 - val_LNout4_loss: 4.7676 - val_LNout5_loss: 4.5662 - val_LNout6_loss: 3.8120 - val_LNout7_loss: 5.0409 - val_LNout8_loss: 4.2920 - val_LNout9_loss: 4.4533 - val_LNout10_loss: 5.6841 - val_LNout11_loss: 5.2352 - val_LNout12_loss: 5.4833 - val_LNout13_loss: 4.9489 - val_LNout14_loss: 5.7357 - val_LNout15_loss: 5.2580 - val_LNout16_loss: 5.1851 - val_LNout17_loss: 5.4331 - val_LNout18_loss: 5.8583 - val_LNout19_loss: 4.3058 - val_LNout20_loss: 4.4639 - val_LNout21_loss: 5.3482 - val_LNout22_loss: 5.2724 - val_LNout23_loss: 4.4395 - val_LNout24_loss: 5.9940 - val_LNout25_loss: 5.1905 - val_LNout26_loss: 5.0566 - val_LNout27_loss: 4.8488 - val_LNout28_loss: 4.2926 - val_LNout29_loss: 5.6890 - val_LNout30_loss: 4.4386 - val_LNout31_loss: 3.9307 - val_LNout32_loss: 5.4354 - val_LNout33_loss: 5.2121 - val_LNout34_loss: 5.4504 - val_LNout35_loss: 4.9306 - val_LNout36_loss: 5.9483 - val_LNout37_loss: 5.2055 - val_LNout38_loss: 5.6812 - val_LNout39_loss: 4.6606 - val_LNout40_loss: 4.7431 - val_LNout41_loss: 5.7211 - val_LNout42_loss: 5.0225 - val_LNout43_loss: 5.3077 - val_LNout44_loss: 4.6965 - val_LNout45_loss: 5.2202 - val_LNout46_loss: 4.9850 - val_LNout47_loss: 4.4948 - val_LNout48_loss: 5.3140 - val_LNout49_loss: 5.5282 - val_LNout50_loss: 4.4569 - val_LNout51_loss: 5.3051 - val_LNout52_loss: 4.9642 - val_LNout53_loss: 4.9521 - val_LNout54_loss: 5.2877 - val_LNout55_loss: 4.1902 - val_LNout56_loss: 4.6005 - val_LNout57_loss: 5.4899 - val_LNout58_loss: 5.2360 - val_LNout59_loss: 5.0657 - val_LNout60_loss: 4.7474 - val_LNout61_loss: 4.5528 - val_LNout62_loss: 5.3533 - val_LNout63_loss: 4.5967 - val_LNout64_loss: 5.1424 - val_LNout65_loss: 5.0075 - val_LNout66_loss: 4.3511 - val_LNout67_loss: 4.5024 - val_LNout68_loss: 4.2883 - val_LNout69_loss: 5.0353 - val_LNout70_loss: 4.3623 - val_LNout71_loss: 5.0766 - val_LNout72_loss: 4.5807 - val_LNout73_loss: 4.3408 - val_LNout74_loss: 4.7391 - val_LNout75_loss: 5.5088 - val_LNout76_loss: 5.1192 - val_LNout77_loss: 5.5662 - val_LNout78_loss: 4.6935 - val_LNout79_loss: 5.8796 - val_LNout80_loss: 4.6486 - val_LNout81_loss: 5.3657 - val_LNout82_loss: 5.4313 - val_LNout83_loss: 4.7352 - val_LNout84_loss: 4.5996 - val_LNout85_loss: 4.4798 - val_LNout86_loss: 4.8630 - val_LNout87_loss: 5.6523 - val_LNout88_loss: 5.2008 - val_LNout89_loss: 4.5788 - val_LNout90_loss: 5.6114 - val_LNout91_loss: 4.7585 - val_LNout92_loss: 4.1047 - val_LNout93_loss: 4.6395 - val_LNout94_loss: 5.7325 - val_LNout95_loss: 4.8834 - val_LNout96_loss: 5.4512 - val_LNout97_loss: 5.5859 - val_LNout98_loss: 4.9249 - val_LNout99_loss: 4.4510 - val_LNout100_loss: 5.5390 - val_LNout101_loss: 4.8896 - val_LNout102_loss: 4.4936 - val_LNout103_loss: 5.0947 - val_LNout104_loss: 5.1956 - val_LNout105_loss: 5.3967 - val_LNout106_loss: 4.5789 - val_LNout107_loss: 4.5800 - val_LNout108_loss: 3.7298 - val_LNout109_loss: 5.0043 - val_LNout110_loss: 4.3068 - val_LNout111_loss: 5.5980 - val_LNout112_loss: 3.7366 - val_LNout113_loss: 5.5769 - val_LNout114_loss: 4.6693 - val_LNout115_loss: 4.4171 - val_LNout116_loss: 3.5339 - val_LNout117_loss: 5.1152 - val_LNout118_loss: 4.5445 - val_LNout119_loss: 4.4960 - val_LNout120_loss: 4.6731 - val_LNout121_loss: 5.2311 - val_LNout122_loss: 5.7567 - val_LNout123_loss: 5.9765 - val_LNout124_loss: 5.1366 - val_LNout125_loss: 4.4280 - val_LNout126_loss: 4.4417 - val_LNout127_loss: 5.7382 - val_LNout128_loss: 5.2880 - val_LNout129_loss: 6.3457 - val_LNout130_loss: 5.1352 - val_LNout131_loss: 3.9561 - val_LNout132_loss: 4.5367 - val_LNout133_loss: 5.3494 - val_LNout134_loss: 5.4974 - val_LNout135_loss: 4.0978 - val_LNout136_loss: 4.3600 - val_LNout137_loss: 4.5991 - val_LNout138_loss: 5.6167 - val_LNout139_loss: 6.0212 - val_LNout140_loss: 5.8988 - val_LNout141_loss: 5.3394 - val_LNout142_loss: 5.0754 - val_LNout143_loss: 4.8701 - val_LNout144_loss: 4.1164 - val_LNout145_loss: 4.9782 - val_LNout146_loss: 4.9383 - val_LNout147_loss: 5.2092 - val_LNout148_loss: 4.8976 - val_LNout149_loss: 5.5059 - val_LNout150_loss: 5.3055 - val_LNout151_loss: 4.5887 - val_LNout152_loss: 5.1597 - val_LNout153_loss: 4.9468 - val_LNout154_loss: 4.9152 - val_LNout155_loss: 4.9946 - val_LNout156_loss: 5.8543 - val_LNout157_loss: 5.0775 - val_LNout158_loss: 5.6910 - val_LNout159_loss: 5.1202 - val_LNout160_loss: 5.1163 - val_LNout161_loss: 6.2717 - val_LNout162_loss: 4.7623 - val_LNout163_loss: 5.7947 - val_LNout164_loss: 4.8637 - val_LNout165_loss: 3.7493 - val_LNout166_loss: 4.9067 - val_LNout167_loss: 4.1477 - val_LNout168_loss: 4.9828 - val_LNout169_loss: 5.2824 - val_LNout170_loss: 5.4886 - val_LNout171_loss: 4.1406 - val_LNout172_loss: 5.9114 - val_LNout173_loss: 5.6342 - val_LNout174_loss: 4.6543 - val_LNout175_loss: 5.1079 - val_LNout176_loss: 5.7161 - val_LNout177_loss: 4.2803 - val_LNout178_loss: 5.0140 - val_LNout179_loss: 6.3236 - val_LNout180_loss: 4.8617 - val_LNout181_loss: 4.9986 - val_LNout182_loss: 4.9163 - val_LNout183_loss: 5.3291 - val_LNout184_loss: 4.7919 - val_LNout185_loss: 4.3584 - val_LNout186_loss: 5.1997 - val_LNout187_loss: 4.9991 - val_LNout188_loss: 5.2366 - val_LNout189_loss: 4.8574 - val_LNout190_loss: 4.7231 - val_LNout191_loss: 4.5172 - val_LNout192_loss: 4.9946 - val_LNout193_loss: 4.3670 - val_LNout194_loss: 5.5259 - val_LNout195_loss: 5.3795 - val_LNout196_loss: 4.5537 - val_LNout197_loss: 5.0186 - val_LNout198_loss: 5.6965 - val_LNout199_loss: 4.9639 - val_LNout200_loss: 4.9490 - val_LNout201_loss: 5.9396 - val_LNout202_loss: 5.5750 - val_LNout203_loss: 4.7565 - val_LNout204_loss: 4.7960 - val_LNout205_loss: 4.7374 - val_LNout206_loss: 4.1637 - val_LNout207_loss: 5.0901 - val_LNout208_loss: 4.5119 - val_LNout209_loss: 4.4486 - val_LNout210_loss: 4.3139 - val_LNout211_loss: 5.0173 - val_LNout212_loss: 5.1502 - val_LNout213_loss: 5.4876 - val_LNout214_loss: 4.0200 - val_LNout215_loss: 3.7642 - val_LNout216_loss: 5.4433 - val_LNout217_loss: 4.5112 - val_LNout218_loss: 5.1489 - val_LNout219_loss: 5.3822 - val_LNout220_loss: 5.0691 - val_LNout221_loss: 4.6246 - val_LNout222_loss: 4.9229 - val_LNout223_loss: 4.1093 - val_LNout224_loss: 4.6178 - val_LNout225_loss: 5.7485 - val_LNout226_loss: 4.7571 - val_LNout227_loss: 5.0456 - val_LNout228_loss: 4.4874 - val_LNout229_loss: 5.4219 - val_LNout230_loss: 5.3008 - val_LNout231_loss: 4.4275 - val_LNout232_loss: 6.0265 - val_LNout233_loss: 5.2291 - val_LNout234_loss: 4.0893 - val_LNout235_loss: 4.6425 - val_LNout236_loss: 5.3027 - val_LNout237_loss: 5.2098 - val_LNout238_loss: 4.2443 - val_LNout239_loss: 5.5715 - val_LNout240_loss: 4.4692 - val_LNout241_loss: 4.6618 - val_LNout242_loss: 3.9830 - val_LNout243_loss: 4.7284 - val_LNout244_loss: 4.8551 - val_LNout245_loss: 4.9903 - val_LNout246_loss: 4.4850 - val_LNout247_loss: 4.8945 - val_LNout248_loss: 5.9949 - val_LNout249_loss: 4.2146 - val_LNout250_loss: 5.9184 - val_LNout251_loss: 3.7114 - val_LNout252_loss: 5.8023 - val_LNout253_loss: 5.3418 - val_LNout254_loss: 4.6391 - val_LNout255_loss: 4.6952 - val_LNout256_loss: 5.3595 - val_LNout257_loss: 4.8838 - val_LNout258_loss: 5.5593 - val_LNout259_loss: 5.0052 - val_LNout260_loss: 4.1129 - val_LNout261_loss: 4.4341 - val_LNout262_loss: 5.7015 - val_LNout263_loss: 3.7926 - val_LNout264_loss: 5.6864 - val_LNout265_loss: 5.8948 - val_LNout266_loss: 5.3160 - val_LNout267_loss: 4.8946 - val_LNout268_loss: 4.9524 - val_LNout269_loss: 5.2368 - val_LNout270_loss: 5.2580 - val_LNout271_loss: 5.5064 - val_LNout272_loss: 4.6197 - val_LNout273_loss: 4.4620 - val_LNout274_loss: 4.7754 - val_LNout275_loss: 4.5258 - val_LNout276_loss: 4.2982 - val_LNout277_loss: 5.6886 - val_LNout278_loss: 4.6436 - val_LNout279_loss: 4.2598 - val_LNout280_loss: 4.0161 - val_LNout281_loss: 5.8157 - val_LNout282_loss: 4.5904 - val_LNout283_loss: 5.0149 - val_LNout284_loss: 4.8797 - val_LNout285_loss: 4.3040 - val_LNout286_loss: 5.3762 - val_LNout287_loss: 4.8281 - val_LNout288_loss: 4.7553 - val_LNout289_loss: 4.5513 - val_LNout290_loss: 5.2489 - val_LNout291_loss: 5.0932 - val_LNout292_loss: 5.3948 - val_LNout293_loss: 4.3918 - val_LNout294_loss: 6.1672 - val_LNout295_loss: 4.8949 - val_LNout296_loss: 4.4021 - val_LNout297_loss: 4.9451 - val_LNout298_loss: 4.5049 - val_LNout299_loss: 5.9061 - val_LNout300_loss: 4.3245 - val_LNout301_loss: 5.4972 - val_LNout302_loss: 5.2992 - val_LNout303_loss: 3.7126 - val_LNout304_loss: 5.7471 - val_LNout305_loss: 5.1983 - val_LNout306_loss: 4.9259 - val_LNout307_loss: 5.1651 - val_LNout308_loss: 6.0913 - val_LNout309_loss: 5.0376 - val_LNout310_loss: 5.1990 - val_LNout311_loss: 4.8154 - val_LNout312_loss: 5.2322 - val_LNout313_loss: 4.6083 - val_LNout314_loss: 4.5079 - val_LNout315_loss: 4.4994 - val_LNout316_loss: 4.3122 - val_LNout317_loss: 5.5457 - val_LNout318_loss: 4.9751 - val_LNout319_loss: 4.3429 - val_LNout320_loss: 4.7732 - val_LNout321_loss: 4.6454 - val_LNout322_loss: 4.3950 - val_LNout323_loss: 5.4922 - val_LNout324_loss: 5.5343 - val_LNout325_loss: 4.4894 - val_LNout326_loss: 4.1008 - val_LNout327_loss: 5.0165 - val_LNout328_loss: 4.7609 - val_LNout329_loss: 5.8223 - val_LNout330_loss: 3.8406 - val_LNout331_loss: 5.2157 - val_LNout332_loss: 4.6900 - val_LNout333_loss: 5.8233 - val_LNout334_loss: 4.6685 - val_LNout335_loss: 4.0957 - val_LNout336_loss: 5.1098 - val_LNout337_loss: 5.9328 - val_LNout338_loss: 3.7999 - val_LNout339_loss: 4.1975 - val_LNout340_loss: 4.9891 - val_LNout341_loss: 5.3820 - val_LNout342_loss: 5.6433 - val_LNout343_loss: 4.0442 - val_LNout344_loss: 4.1315 - val_LNout345_loss: 4.5491 - val_LNout346_loss: 5.0741 - val_LNout347_loss: 5.3215 - val_LNout348_loss: 3.7937 - val_LNout349_loss: 4.0355 - val_LNout350_loss: 4.4766 - val_LNout351_loss: 4.8360 - val_LNout352_loss: 4.5572 - val_LNout353_loss: 4.8434 - val_LNout354_loss: 5.0715 - val_LNout355_loss: 5.6113 - val_LNout356_loss: 4.7807 - val_LNout357_loss: 5.5069 - val_LNout358_loss: 4.6487 - val_LNout359_loss: 4.0099 - val_error_feed_forward_output1_accuracy: 0.0241 - val_LNout0_accuracy: 0.0000e+00 - val_LNout1_accuracy: 0.0000e+00 - val_LNout2_accuracy: 0.0646 - val_LNout3_accuracy: 0.0000e+00 - val_LNout4_accuracy: 7.1023e-04 - val_LNout5_accuracy: 7.1023e-04 - val_LNout6_accuracy: 0.1776 - val_LNout7_accuracy: 0.0000e+00 - val_LNout8_accuracy: 0.0000e+00 - val_LNout9_accuracy: 0.0000e+00 - val_LNout10_accuracy: 0.0000e+00 - val_LNout11_accuracy: 0.0000e+00 - val_LNout12_accuracy: 0.0000e+00 - val_LNout13_accuracy: 0.0014 - val_LNout14_accuracy: 0.0000e+00 - val_LNout15_accuracy: 0.0000e+00 - val_LNout16_accuracy: 0.0000e+00 - val_LNout17_accuracy: 0.0000e+00 - val_LNout18_accuracy: 0.0000e+00 - val_LNout19_accuracy: 0.0000e+00 - val_LNout20_accuracy: 0.0000e+00 - val_LNout21_accuracy: 0.0000e+00 - val_LNout22_accuracy: 0.0000e+00 - val_LNout23_accuracy: 0.0000e+00 - val_LNout24_accuracy: 0.0000e+00 - val_LNout25_accuracy: 0.0000e+00 - val_LNout26_accuracy: 0.0000e+00 - val_LNout27_accuracy: 0.0000e+00 - val_LNout28_accuracy: 0.0000e+00 - val_LNout29_accuracy: 0.0000e+00 - val_LNout30_accuracy: 0.0000e+00 - val_LNout31_accuracy: 0.0000e+00 - val_LNout32_accuracy: 0.0000e+00 - val_LNout33_accuracy: 0.0000e+00 - val_LNout34_accuracy: 0.0000e+00 - val_LNout35_accuracy: 0.0000e+00 - val_LNout36_accuracy: 0.0000e+00 - val_LNout37_accuracy: 0.0000e+00 - val_LNout38_accuracy: 0.0000e+00 - val_LNout39_accuracy: 0.0000e+00 - val_LNout40_accuracy: 0.0000e+00 - val_LNout41_accuracy: 0.0000e+00 - val_LNout42_accuracy: 0.0000e+00 - val_LNout43_accuracy: 0.0000e+00 - val_LNout44_accuracy: 0.0000e+00 - val_LNout45_accuracy: 0.0000e+00 - val_LNout46_accuracy: 0.0000e+00 - val_LNout47_accuracy: 0.0000e+00 - val_LNout48_accuracy: 0.0000e+00 - val_LNout49_accuracy: 0.0000e+00 - val_LNout50_accuracy: 0.0000e+00 - val_LNout51_accuracy: 0.0000e+00 - val_LNout52_accuracy: 0.0000e+00 - val_LNout53_accuracy: 0.0000e+00 - val_LNout54_accuracy: 0.0000e+00 - val_LNout55_accuracy: 0.0064 - val_LNout56_accuracy: 0.0000e+00 - val_LNout57_accuracy: 0.0000e+00 - val_LNout58_accuracy: 0.0000e+00 - val_LNout59_accuracy: 0.0000e+00 - val_LNout60_accuracy: 0.0000e+00 - val_LNout61_accuracy: 0.0625 - val_LNout62_accuracy: 0.0000e+00 - val_LNout63_accuracy: 0.0000e+00 - val_LNout64_accuracy: 0.0014 - val_LNout65_accuracy: 7.1023e-04 - val_LNout66_accuracy: 0.0000e+00 - val_LNout67_accuracy: 0.0000e+00 - val_LNout68_accuracy: 0.0000e+00 - val_LNout69_accuracy: 0.0000e+00 - val_LNout70_accuracy: 0.0000e+00 - val_LNout71_accuracy: 0.0000e+00 - val_LNout72_accuracy: 0.0000e+00 - val_LNout73_accuracy: 0.0000e+00 - val_LNout74_accuracy: 7.1023e-04 - val_LNout75_accuracy: 7.1023e-04 - val_LNout76_accuracy: 0.0000e+00 - val_LNout77_accuracy: 0.0000e+00 - val_LNout78_accuracy: 0.0000e+00 - val_LNout79_accuracy: 0.0000e+00 - val_LNout80_accuracy: 0.0000e+00 - val_LNout81_accuracy: 0.0000e+00 - val_LNout82_accuracy: 0.0000e+00 - val_LNout83_accuracy: 0.0000e+00 - val_LNout84_accuracy: 0.0000e+00 - val_LNout85_accuracy: 0.0000e+00 - val_LNout86_accuracy: 0.0000e+00 - val_LNout87_accuracy: 0.0000e+00 - val_LNout88_accuracy: 0.0000e+00 - val_LNout89_accuracy: 0.0000e+00 - val_LNout90_accuracy: 0.0000e+00 - val_LNout91_accuracy: 0.0000e+00 - val_LNout92_accuracy: 0.0000e+00 - val_LNout93_accuracy: 0.0000e+00 - val_LNout94_accuracy: 0.0000e+00 - val_LNout95_accuracy: 0.0000e+00 - val_LNout96_accuracy: 0.0000e+00 - val_LNout97_accuracy: 0.0000e+00 - val_LNout98_accuracy: 0.0000e+00 - val_LNout99_accuracy: 0.0000e+00 - val_LNout100_accuracy: 0.0000e+00 - val_LNout101_accuracy: 0.0000e+00 - val_LNout102_accuracy: 0.0000e+00 - val_LNout103_accuracy: 0.0000e+00 - val_LNout104_accuracy: 0.0000e+00 - val_LNout105_accuracy: 0.0000e+00 - val_LNout106_accuracy: 0.0000e+00 - val_LNout107_accuracy: 0.0000e+00 - val_LNout108_accuracy: 7.1023e-04 - val_LNout109_accuracy: 0.0000e+00 - val_LNout110_accuracy: 0.0000e+00 - val_LNout111_accuracy: 0.0000e+00 - val_LNout112_accuracy: 1.0000 - val_LNout113_accuracy: 0.0000e+00 - val_LNout114_accuracy: 0.0000e+00 - val_LNout115_accuracy: 0.0000e+00 - val_LNout116_accuracy: 1.0000 - val_LNout117_accuracy: 0.0000e+00 - val_LNout118_accuracy: 0.0000e+00 - val_LNout119_accuracy: 0.0000e+00 - val_LNout120_accuracy: 0.0000e+00 - val_LNout121_accuracy: 0.0000e+00 - val_LNout122_accuracy: 0.0014 - val_LNout123_accuracy: 0.0000e+00 - val_LNout124_accuracy: 0.0000e+00 - val_LNout125_accuracy: 0.0000e+00 - val_LNout126_accuracy: 0.0000e+00 - val_LNout127_accuracy: 0.0021 - val_LNout128_accuracy: 0.0000e+00 - val_LNout129_accuracy: 0.0000e+00 - val_LNout130_accuracy: 0.0000e+00 - val_LNout131_accuracy: 0.8991 - val_LNout132_accuracy: 0.0000e+00 - val_LNout133_accuracy: 0.0000e+00 - val_LNout134_accuracy: 0.0000e+00 - val_LNout135_accuracy: 0.0000e+00 - val_LNout136_accuracy: 0.0000e+00 - val_LNout137_accuracy: 0.0000e+00 - val_LNout138_accuracy: 0.0000e+00 - val_LNout139_accuracy: 0.0000e+00 - val_LNout140_accuracy: 0.0000e+00 - val_LNout141_accuracy: 0.0000e+00 - val_LNout142_accuracy: 0.0000e+00 - val_LNout143_accuracy: 0.0000e+00 - val_LNout144_accuracy: 0.0000e+00 - val_LNout145_accuracy: 0.0000e+00 - val_LNout146_accuracy: 0.0000e+00 - val_LNout147_accuracy: 0.0000e+00 - val_LNout148_accuracy: 0.0000e+00 - val_LNout149_accuracy: 0.0000e+00 - val_LNout150_accuracy: 0.0000e+00 - val_LNout151_accuracy: 0.0000e+00 - val_LNout152_accuracy: 0.0000e+00 - val_LNout153_accuracy: 0.0000e+00 - val_LNout154_accuracy: 0.0000e+00 - val_LNout155_accuracy: 0.0000e+00 - val_LNout156_accuracy: 0.0000e+00 - val_LNout157_accuracy: 0.0000e+00 - val_LNout158_accuracy: 0.0000e+00 - val_LNout159_accuracy: 0.0000e+00 - val_LNout160_accuracy: 0.0000e+00 - val_LNout161_accuracy: 0.0000e+00 - val_LNout162_accuracy: 0.0000e+00 - val_LNout163_accuracy: 0.0000e+00 - val_LNout164_accuracy: 0.0000e+00 - val_LNout165_accuracy: 1.0000 - val_LNout166_accuracy: 0.0000e+00 - val_LNout167_accuracy: 0.0000e+00 - val_LNout168_accuracy: 0.0000e+00 - val_LNout169_accuracy: 0.0000e+00 - val_LNout170_accuracy: 0.0000e+00 - val_LNout171_accuracy: 0.0178 - val_LNout172_accuracy: 0.0000e+00 - val_LNout173_accuracy: 0.0000e+00 - val_LNout174_accuracy: 0.0000e+00 - val_LNout175_accuracy: 0.0000e+00 - val_LNout176_accuracy: 0.0000e+00 - val_LNout177_accuracy: 0.0000e+00 - val_LNout178_accuracy: 0.0000e+00 - val_LNout179_accuracy: 0.0000e+00 - val_LNout180_accuracy: 0.0000e+00 - val_LNout181_accuracy: 0.0000e+00 - val_LNout182_accuracy: 0.0000e+00 - val_LNout183_accuracy: 0.0000e+00 - val_LNout184_accuracy: 0.0000e+00 - val_LNout185_accuracy: 0.0000e+00 - val_LNout186_accuracy: 0.0000e+00 - val_LNout187_accuracy: 0.0000e+00 - val_LNout188_accuracy: 0.0000e+00 - val_LNout189_accuracy: 0.0000e+00 - val_LNout190_accuracy: 0.0000e+00 - val_LNout191_accuracy: 0.0000e+00 - val_LNout192_accuracy: 0.0000e+00 - val_LNout193_accuracy: 0.0000e+00 - val_LNout194_accuracy: 0.0000e+00 - val_LNout195_accuracy: 0.0000e+00 - val_LNout196_accuracy: 0.0000e+00 - val_LNout197_accuracy: 0.0000e+00 - val_LNout198_accuracy: 0.0000e+00 - val_LNout199_accuracy: 0.0000e+00 - val_LNout200_accuracy: 0.0000e+00 - val_LNout201_accuracy: 0.0000e+00 - val_LNout202_accuracy: 0.0000e+00 - val_LNout203_accuracy: 0.0000e+00 - val_LNout204_accuracy: 0.0000e+00 - val_LNout205_accuracy: 0.0000e+00 - val_LNout206_accuracy: 0.0000e+00 - val_LNout207_accuracy: 0.0000e+00 - val_LNout208_accuracy: 0.0000e+00 - val_LNout209_accuracy: 0.0000e+00 - val_LNout210_accuracy: 0.0000e+00 - val_LNout211_accuracy: 0.0000e+00 - val_LNout212_accuracy: 0.0000e+00 - val_LNout213_accuracy: 0.0000e+00 - val_LNout214_accuracy: 0.0000e+00 - val_LNout215_accuracy: 0.0000e+00 - val_LNout216_accuracy: 0.0000e+00 - val_LNout217_accuracy: 0.0000e+00 - val_LNout218_accuracy: 0.0000e+00 - val_LNout219_accuracy: 0.0000e+00 - val_LNout220_accuracy: 0.0000e+00 - val_LNout221_accuracy: 0.0000e+00 - val_LNout222_accuracy: 0.0000e+00 - val_LNout223_accuracy: 0.4709 - val_LNout224_accuracy: 0.0000e+00 - val_LNout225_accuracy: 0.0000e+00 - val_LNout226_accuracy: 0.0000e+00 - val_LNout227_accuracy: 0.0000e+00 - val_LNout228_accuracy: 0.0000e+00 - val_LNout229_accuracy: 0.0000e+00 - val_LNout230_accuracy: 0.0000e+00 - val_LNout231_accuracy: 0.0000e+00 - val_LNout232_accuracy: 0.0000e+00 - val_LNout233_accuracy: 0.0000e+00 - val_LNout234_accuracy: 0.0000e+00 - val_LNout235_accuracy: 0.0000e+00 - val_LNout236_accuracy: 0.0000e+00 - val_LNout237_accuracy: 0.0000e+00 - val_LNout238_accuracy: 0.0000e+00 - val_LNout239_accuracy: 0.0000e+00 - val_LNout240_accuracy: 0.0000e+00 - val_LNout241_accuracy: 0.0000e+00 - val_LNout242_accuracy: 0.0000e+00 - val_LNout243_accuracy: 0.0000e+00 - val_LNout244_accuracy: 0.0000e+00 - val_LNout245_accuracy: 0.0000e+00 - val_LNout246_accuracy: 0.0000e+00 - val_LNout247_accuracy: 0.0000e+00 - val_LNout248_accuracy: 0.0000e+00 - val_LNout249_accuracy: 0.0000e+00 - val_LNout250_accuracy: 0.0000e+00 - val_LNout251_accuracy: 0.9986 - val_LNout252_accuracy: 0.0000e+00 - val_LNout253_accuracy: 0.0000e+00 - val_LNout254_accuracy: 0.0000e+00 - val_LNout255_accuracy: 0.0000e+00 - val_LNout256_accuracy: 0.0000e+00 - val_LNout257_accuracy: 0.0000e+00 - val_LNout258_accuracy: 0.0000e+00 - val_LNout259_accuracy: 0.0000e+00 - val_LNout260_accuracy: 0.0000e+00 - val_LNout261_accuracy: 0.0000e+00 - val_LNout262_accuracy: 0.0000e+00 - val_LNout263_accuracy: 0.9489 - val_LNout264_accuracy: 0.0000e+00 - val_LNout265_accuracy: 0.0000e+00 - val_LNout266_accuracy: 0.0000e+00 - val_LNout267_accuracy: 0.0000e+00 - val_LNout268_accuracy: 0.0000e+00 - val_LNout269_accuracy: 0.0000e+00 - val_LNout270_accuracy: 0.0000e+00 - val_LNout271_accuracy: 0.0000e+00 - val_LNout272_accuracy: 0.0000e+00 - val_LNout273_accuracy: 0.0000e+00 - val_LNout274_accuracy: 0.0000e+00 - val_LNout275_accuracy: 0.0000e+00 - val_LNout276_accuracy: 0.0000e+00 - val_LNout277_accuracy: 0.0000e+00 - val_LNout278_accuracy: 0.0000e+00 - val_LNout279_accuracy: 0.0000e+00 - val_LNout280_accuracy: 0.0696 - val_LNout281_accuracy: 0.0000e+00 - val_LNout282_accuracy: 0.0000e+00 - val_LNout283_accuracy: 0.0000e+00 - val_LNout284_accuracy: 0.0000e+00 - val_LNout285_accuracy: 0.0000e+00 - val_LNout286_accuracy: 0.0000e+00 - val_LNout287_accuracy: 0.0000e+00 - val_LNout288_accuracy: 0.0000e+00 - val_LNout289_accuracy: 0.0000e+00 - val_LNout290_accuracy: 0.0000e+00 - val_LNout291_accuracy: 0.0000e+00 - val_LNout292_accuracy: 0.0000e+00 - val_LNout293_accuracy: 0.0000e+00 - val_LNout294_accuracy: 0.0000e+00 - val_LNout295_accuracy: 0.0000e+00 - val_LNout296_accuracy: 0.0000e+00 - val_LNout297_accuracy: 0.0000e+00 - val_LNout298_accuracy: 0.0000e+00 - val_LNout299_accuracy: 0.0000e+00 - val_LNout300_accuracy: 7.1023e-04 - val_LNout301_accuracy: 0.0000e+00 - val_LNout302_accuracy: 0.0000e+00 - val_LNout303_accuracy: 0.0000e+00 - val_LNout304_accuracy: 0.0000e+00 - val_LNout305_accuracy: 0.0000e+00 - val_LNout306_accuracy: 0.0000e+00 - val_LNout307_accuracy: 0.0000e+00 - val_LNout308_accuracy: 0.0000e+00 - val_LNout309_accuracy: 0.0000e+00 - val_LNout310_accuracy: 0.0000e+00 - val_LNout311_accuracy: 0.0000e+00 - val_LNout312_accuracy: 0.0000e+00 - val_LNout313_accuracy: 0.0000e+00 - val_LNout314_accuracy: 0.0000e+00 - val_LNout315_accuracy: 0.0000e+00 - val_LNout316_accuracy: 0.0000e+00 - val_LNout317_accuracy: 0.0000e+00 - val_LNout318_accuracy: 0.0000e+00 - val_LNout319_accuracy: 0.0000e+00 - val_LNout320_accuracy: 0.0000e+00 - val_LNout321_accuracy: 0.0000e+00 - val_LNout322_accuracy: 0.0000e+00 - val_LNout323_accuracy: 0.0000e+00 - val_LNout324_accuracy: 0.0000e+00 - val_LNout325_accuracy: 0.0000e+00 - val_LNout326_accuracy: 0.0000e+00 - val_LNout327_accuracy: 0.0000e+00 - val_LNout328_accuracy: 0.0000e+00 - val_LNout329_accuracy: 0.0000e+00 - val_LNout330_accuracy: 0.0000e+00 - val_LNout331_accuracy: 0.0000e+00 - val_LNout332_accuracy: 0.0000e+00 - val_LNout333_accuracy: 0.0000e+00 - val_LNout334_accuracy: 0.0000e+00 - val_LNout335_accuracy: 0.0000e+00 - val_LNout336_accuracy: 0.0000e+00 - val_LNout337_accuracy: 0.0000e+00 - val_LNout338_accuracy: 0.3345 - val_LNout339_accuracy: 0.0000e+00 - val_LNout340_accuracy: 0.0000e+00 - val_LNout341_accuracy: 0.0000e+00 - val_LNout342_accuracy: 0.0000e+00 - val_LNout343_accuracy: 0.0114 - val_LNout344_accuracy: 0.0000e+00 - val_LNout345_accuracy: 0.0000e+00 - val_LNout346_accuracy: 0.0000e+00 - val_LNout347_accuracy: 0.0000e+00 - val_LNout348_accuracy: 0.8658 - val_LNout349_accuracy: 0.0000e+00 - val_LNout350_accuracy: 0.0000e+00 - val_LNout351_accuracy: 0.0000e+00 - val_LNout352_accuracy: 0.0000e+00 - val_LNout353_accuracy: 0.0000e+00 - val_LNout354_accuracy: 0.0000e+00 - val_LNout355_accuracy: 0.0000e+00 - val_LNout356_accuracy: 0.0000e+00 - val_LNout357_accuracy: 0.0000e+00 - val_LNout358_accuracy: 0.0000e+00 - val_LNout359_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history.history.keys:  dict_keys(['loss', 'error_feed_forward_output1_loss', 'LNout0_loss', 'LNout1_loss', 'LNout2_loss', 'LNout3_loss', 'LNout4_loss', 'LNout5_loss', 'LNout6_loss', 'LNout7_loss', 'LNout8_loss', 'LNout9_loss', 'LNout10_loss', 'LNout11_loss', 'LNout12_loss', 'LNout13_loss', 'LNout14_loss', 'LNout15_loss', 'LNout16_loss', 'LNout17_loss', 'LNout18_loss', 'LNout19_loss', 'LNout20_loss', 'LNout21_loss', 'LNout22_loss', 'LNout23_loss', 'LNout24_loss', 'LNout25_loss', 'LNout26_loss', 'LNout27_loss', 'LNout28_loss', 'LNout29_loss', 'LNout30_loss', 'LNout31_loss', 'LNout32_loss', 'LNout33_loss', 'LNout34_loss', 'LNout35_loss', 'LNout36_loss', 'LNout37_loss', 'LNout38_loss', 'LNout39_loss', 'LNout40_loss', 'LNout41_loss', 'LNout42_loss', 'LNout43_loss', 'LNout44_loss', 'LNout45_loss', 'LNout46_loss', 'LNout47_loss', 'LNout48_loss', 'LNout49_loss', 'LNout50_loss', 'LNout51_loss', 'LNout52_loss', 'LNout53_loss', 'LNout54_loss', 'LNout55_loss', 'LNout56_loss', 'LNout57_loss', 'LNout58_loss', 'LNout59_loss', 'LNout60_loss', 'LNout61_loss', 'LNout62_loss', 'LNout63_loss', 'LNout64_loss', 'LNout65_loss', 'LNout66_loss', 'LNout67_loss', 'LNout68_loss', 'LNout69_loss', 'LNout70_loss', 'LNout71_loss', 'LNout72_loss', 'LNout73_loss', 'LNout74_loss', 'LNout75_loss', 'LNout76_loss', 'LNout77_loss', 'LNout78_loss', 'LNout79_loss', 'LNout80_loss', 'LNout81_loss', 'LNout82_loss', 'LNout83_loss', 'LNout84_loss', 'LNout85_loss', 'LNout86_loss', 'LNout87_loss', 'LNout88_loss', 'LNout89_loss', 'LNout90_loss', 'LNout91_loss', 'LNout92_loss', 'LNout93_loss', 'LNout94_loss', 'LNout95_loss', 'LNout96_loss', 'LNout97_loss', 'LNout98_loss', 'LNout99_loss', 'LNout100_loss', 'LNout101_loss', 'LNout102_loss', 'LNout103_loss', 'LNout104_loss', 'LNout105_loss', 'LNout106_loss', 'LNout107_loss', 'LNout108_loss', 'LNout109_loss', 'LNout110_loss', 'LNout111_loss', 'LNout112_loss', 'LNout113_loss', 'LNout114_loss', 'LNout115_loss', 'LNout116_loss', 'LNout117_loss', 'LNout118_loss', 'LNout119_loss', 'LNout120_loss', 'LNout121_loss', 'LNout122_loss', 'LNout123_loss', 'LNout124_loss', 'LNout125_loss', 'LNout126_loss', 'LNout127_loss', 'LNout128_loss', 'LNout129_loss', 'LNout130_loss', 'LNout131_loss', 'LNout132_loss', 'LNout133_loss', 'LNout134_loss', 'LNout135_loss', 'LNout136_loss', 'LNout137_loss', 'LNout138_loss', 'LNout139_loss', 'LNout140_loss', 'LNout141_loss', 'LNout142_loss', 'LNout143_loss', 'LNout144_loss', 'LNout145_loss', 'LNout146_loss', 'LNout147_loss', 'LNout148_loss', 'LNout149_loss', 'LNout150_loss', 'LNout151_loss', 'LNout152_loss', 'LNout153_loss', 'LNout154_loss', 'LNout155_loss', 'LNout156_loss', 'LNout157_loss', 'LNout158_loss', 'LNout159_loss', 'LNout160_loss', 'LNout161_loss', 'LNout162_loss', 'LNout163_loss', 'LNout164_loss', 'LNout165_loss', 'LNout166_loss', 'LNout167_loss', 'LNout168_loss', 'LNout169_loss', 'LNout170_loss', 'LNout171_loss', 'LNout172_loss', 'LNout173_loss', 'LNout174_loss', 'LNout175_loss', 'LNout176_loss', 'LNout177_loss', 'LNout178_loss', 'LNout179_loss', 'LNout180_loss', 'LNout181_loss', 'LNout182_loss', 'LNout183_loss', 'LNout184_loss', 'LNout185_loss', 'LNout186_loss', 'LNout187_loss', 'LNout188_loss', 'LNout189_loss', 'LNout190_loss', 'LNout191_loss', 'LNout192_loss', 'LNout193_loss', 'LNout194_loss', 'LNout195_loss', 'LNout196_loss', 'LNout197_loss', 'LNout198_loss', 'LNout199_loss', 'LNout200_loss', 'LNout201_loss', 'LNout202_loss', 'LNout203_loss', 'LNout204_loss', 'LNout205_loss', 'LNout206_loss', 'LNout207_loss', 'LNout208_loss', 'LNout209_loss', 'LNout210_loss', 'LNout211_loss', 'LNout212_loss', 'LNout213_loss', 'LNout214_loss', 'LNout215_loss', 'LNout216_loss', 'LNout217_loss', 'LNout218_loss', 'LNout219_loss', 'LNout220_loss', 'LNout221_loss', 'LNout222_loss', 'LNout223_loss', 'LNout224_loss', 'LNout225_loss', 'LNout226_loss', 'LNout227_loss', 'LNout228_loss', 'LNout229_loss', 'LNout230_loss', 'LNout231_loss', 'LNout232_loss', 'LNout233_loss', 'LNout234_loss', 'LNout235_loss', 'LNout236_loss', 'LNout237_loss', 'LNout238_loss', 'LNout239_loss', 'LNout240_loss', 'LNout241_loss', 'LNout242_loss', 'LNout243_loss', 'LNout244_loss', 'LNout245_loss', 'LNout246_loss', 'LNout247_loss', 'LNout248_loss', 'LNout249_loss', 'LNout250_loss', 'LNout251_loss', 'LNout252_loss', 'LNout253_loss', 'LNout254_loss', 'LNout255_loss', 'LNout256_loss', 'LNout257_loss', 'LNout258_loss', 'LNout259_loss', 'LNout260_loss', 'LNout261_loss', 'LNout262_loss', 'LNout263_loss', 'LNout264_loss', 'LNout265_loss', 'LNout266_loss', 'LNout267_loss', 'LNout268_loss', 'LNout269_loss', 'LNout270_loss', 'LNout271_loss', 'LNout272_loss', 'LNout273_loss', 'LNout274_loss', 'LNout275_loss', 'LNout276_loss', 'LNout277_loss', 'LNout278_loss', 'LNout279_loss', 'LNout280_loss', 'LNout281_loss', 'LNout282_loss', 'LNout283_loss', 'LNout284_loss', 'LNout285_loss', 'LNout286_loss', 'LNout287_loss', 'LNout288_loss', 'LNout289_loss', 'LNout290_loss', 'LNout291_loss', 'LNout292_loss', 'LNout293_loss', 'LNout294_loss', 'LNout295_loss', 'LNout296_loss', 'LNout297_loss', 'LNout298_loss', 'LNout299_loss', 'LNout300_loss', 'LNout301_loss', 'LNout302_loss', 'LNout303_loss', 'LNout304_loss', 'LNout305_loss', 'LNout306_loss', 'LNout307_loss', 'LNout308_loss', 'LNout309_loss', 'LNout310_loss', 'LNout311_loss', 'LNout312_loss', 'LNout313_loss', 'LNout314_loss', 'LNout315_loss', 'LNout316_loss', 'LNout317_loss', 'LNout318_loss', 'LNout319_loss', 'LNout320_loss', 'LNout321_loss', 'LNout322_loss', 'LNout323_loss', 'LNout324_loss', 'LNout325_loss', 'LNout326_loss', 'LNout327_loss', 'LNout328_loss', 'LNout329_loss', 'LNout330_loss', 'LNout331_loss', 'LNout332_loss', 'LNout333_loss', 'LNout334_loss', 'LNout335_loss', 'LNout336_loss', 'LNout337_loss', 'LNout338_loss', 'LNout339_loss', 'LNout340_loss', 'LNout341_loss', 'LNout342_loss', 'LNout343_loss', 'LNout344_loss', 'LNout345_loss', 'LNout346_loss', 'LNout347_loss', 'LNout348_loss', 'LNout349_loss', 'LNout350_loss', 'LNout351_loss', 'LNout352_loss', 'LNout353_loss', 'LNout354_loss', 'LNout355_loss', 'LNout356_loss', 'LNout357_loss', 'LNout358_loss', 'LNout359_loss', 'error_feed_forward_output1_accuracy', 'LNout0_accuracy', 'LNout1_accuracy', 'LNout2_accuracy', 'LNout3_accuracy', 'LNout4_accuracy', 'LNout5_accuracy', 'LNout6_accuracy', 'LNout7_accuracy', 'LNout8_accuracy', 'LNout9_accuracy', 'LNout10_accuracy', 'LNout11_accuracy', 'LNout12_accuracy', 'LNout13_accuracy', 'LNout14_accuracy', 'LNout15_accuracy', 'LNout16_accuracy', 'LNout17_accuracy', 'LNout18_accuracy', 'LNout19_accuracy', 'LNout20_accuracy', 'LNout21_accuracy', 'LNout22_accuracy', 'LNout23_accuracy', 'LNout24_accuracy', 'LNout25_accuracy', 'LNout26_accuracy', 'LNout27_accuracy', 'LNout28_accuracy', 'LNout29_accuracy', 'LNout30_accuracy', 'LNout31_accuracy', 'LNout32_accuracy', 'LNout33_accuracy', 'LNout34_accuracy', 'LNout35_accuracy', 'LNout36_accuracy', 'LNout37_accuracy', 'LNout38_accuracy', 'LNout39_accuracy', 'LNout40_accuracy', 'LNout41_accuracy', 'LNout42_accuracy', 'LNout43_accuracy', 'LNout44_accuracy', 'LNout45_accuracy', 'LNout46_accuracy', 'LNout47_accuracy', 'LNout48_accuracy', 'LNout49_accuracy', 'LNout50_accuracy', 'LNout51_accuracy', 'LNout52_accuracy', 'LNout53_accuracy', 'LNout54_accuracy', 'LNout55_accuracy', 'LNout56_accuracy', 'LNout57_accuracy', 'LNout58_accuracy', 'LNout59_accuracy', 'LNout60_accuracy', 'LNout61_accuracy', 'LNout62_accuracy', 'LNout63_accuracy', 'LNout64_accuracy', 'LNout65_accuracy', 'LNout66_accuracy', 'LNout67_accuracy', 'LNout68_accuracy', 'LNout69_accuracy', 'LNout70_accuracy', 'LNout71_accuracy', 'LNout72_accuracy', 'LNout73_accuracy', 'LNout74_accuracy', 'LNout75_accuracy', 'LNout76_accuracy', 'LNout77_accuracy', 'LNout78_accuracy', 'LNout79_accuracy', 'LNout80_accuracy', 'LNout81_accuracy', 'LNout82_accuracy', 'LNout83_accuracy', 'LNout84_accuracy', 'LNout85_accuracy', 'LNout86_accuracy', 'LNout87_accuracy', 'LNout88_accuracy', 'LNout89_accuracy', 'LNout90_accuracy', 'LNout91_accuracy', 'LNout92_accuracy', 'LNout93_accuracy', 'LNout94_accuracy', 'LNout95_accuracy', 'LNout96_accuracy', 'LNout97_accuracy', 'LNout98_accuracy', 'LNout99_accuracy', 'LNout100_accuracy', 'LNout101_accuracy', 'LNout102_accuracy', 'LNout103_accuracy', 'LNout104_accuracy', 'LNout105_accuracy', 'LNout106_accuracy', 'LNout107_accuracy', 'LNout108_accuracy', 'LNout109_accuracy', 'LNout110_accuracy', 'LNout111_accuracy', 'LNout112_accuracy', 'LNout113_accuracy', 'LNout114_accuracy', 'LNout115_accuracy', 'LNout116_accuracy', 'LNout117_accuracy', 'LNout118_accuracy', 'LNout119_accuracy', 'LNout120_accuracy', 'LNout121_accuracy', 'LNout122_accuracy', 'LNout123_accuracy', 'LNout124_accuracy', 'LNout125_accuracy', 'LNout126_accuracy', 'LNout127_accuracy', 'LNout128_accuracy', 'LNout129_accuracy', 'LNout130_accuracy', 'LNout131_accuracy', 'LNout132_accuracy', 'LNout133_accuracy', 'LNout134_accuracy', 'LNout135_accuracy', 'LNout136_accuracy', 'LNout137_accuracy', 'LNout138_accuracy', 'LNout139_accuracy', 'LNout140_accuracy', 'LNout141_accuracy', 'LNout142_accuracy', 'LNout143_accuracy', 'LNout144_accuracy', 'LNout145_accuracy', 'LNout146_accuracy', 'LNout147_accuracy', 'LNout148_accuracy', 'LNout149_accuracy', 'LNout150_accuracy', 'LNout151_accuracy', 'LNout152_accuracy', 'LNout153_accuracy', 'LNout154_accuracy', 'LNout155_accuracy', 'LNout156_accuracy', 'LNout157_accuracy', 'LNout158_accuracy', 'LNout159_accuracy', 'LNout160_accuracy', 'LNout161_accuracy', 'LNout162_accuracy', 'LNout163_accuracy', 'LNout164_accuracy', 'LNout165_accuracy', 'LNout166_accuracy', 'LNout167_accuracy', 'LNout168_accuracy', 'LNout169_accuracy', 'LNout170_accuracy', 'LNout171_accuracy', 'LNout172_accuracy', 'LNout173_accuracy', 'LNout174_accuracy', 'LNout175_accuracy', 'LNout176_accuracy', 'LNout177_accuracy', 'LNout178_accuracy', 'LNout179_accuracy', 'LNout180_accuracy', 'LNout181_accuracy', 'LNout182_accuracy', 'LNout183_accuracy', 'LNout184_accuracy', 'LNout185_accuracy', 'LNout186_accuracy', 'LNout187_accuracy', 'LNout188_accuracy', 'LNout189_accuracy', 'LNout190_accuracy', 'LNout191_accuracy', 'LNout192_accuracy', 'LNout193_accuracy', 'LNout194_accuracy', 'LNout195_accuracy', 'LNout196_accuracy', 'LNout197_accuracy', 'LNout198_accuracy', 'LNout199_accuracy', 'LNout200_accuracy', 'LNout201_accuracy', 'LNout202_accuracy', 'LNout203_accuracy', 'LNout204_accuracy', 'LNout205_accuracy', 'LNout206_accuracy', 'LNout207_accuracy', 'LNout208_accuracy', 'LNout209_accuracy', 'LNout210_accuracy', 'LNout211_accuracy', 'LNout212_accuracy', 'LNout213_accuracy', 'LNout214_accuracy', 'LNout215_accuracy', 'LNout216_accuracy', 'LNout217_accuracy', 'LNout218_accuracy', 'LNout219_accuracy', 'LNout220_accuracy', 'LNout221_accuracy', 'LNout222_accuracy', 'LNout223_accuracy', 'LNout224_accuracy', 'LNout225_accuracy', 'LNout226_accuracy', 'LNout227_accuracy', 'LNout228_accuracy', 'LNout229_accuracy', 'LNout230_accuracy', 'LNout231_accuracy', 'LNout232_accuracy', 'LNout233_accuracy', 'LNout234_accuracy', 'LNout235_accuracy', 'LNout236_accuracy', 'LNout237_accuracy', 'LNout238_accuracy', 'LNout239_accuracy', 'LNout240_accuracy', 'LNout241_accuracy', 'LNout242_accuracy', 'LNout243_accuracy', 'LNout244_accuracy', 'LNout245_accuracy', 'LNout246_accuracy', 'LNout247_accuracy', 'LNout248_accuracy', 'LNout249_accuracy', 'LNout250_accuracy', 'LNout251_accuracy', 'LNout252_accuracy', 'LNout253_accuracy', 'LNout254_accuracy', 'LNout255_accuracy', 'LNout256_accuracy', 'LNout257_accuracy', 'LNout258_accuracy', 'LNout259_accuracy', 'LNout260_accuracy', 'LNout261_accuracy', 'LNout262_accuracy', 'LNout263_accuracy', 'LNout264_accuracy', 'LNout265_accuracy', 'LNout266_accuracy', 'LNout267_accuracy', 'LNout268_accuracy', 'LNout269_accuracy', 'LNout270_accuracy', 'LNout271_accuracy', 'LNout272_accuracy', 'LNout273_accuracy', 'LNout274_accuracy', 'LNout275_accuracy', 'LNout276_accuracy', 'LNout277_accuracy', 'LNout278_accuracy', 'LNout279_accuracy', 'LNout280_accuracy', 'LNout281_accuracy', 'LNout282_accuracy', 'LNout283_accuracy', 'LNout284_accuracy', 'LNout285_accuracy', 'LNout286_accuracy', 'LNout287_accuracy', 'LNout288_accuracy', 'LNout289_accuracy', 'LNout290_accuracy', 'LNout291_accuracy', 'LNout292_accuracy', 'LNout293_accuracy', 'LNout294_accuracy', 'LNout295_accuracy', 'LNout296_accuracy', 'LNout297_accuracy', 'LNout298_accuracy', 'LNout299_accuracy', 'LNout300_accuracy', 'LNout301_accuracy', 'LNout302_accuracy', 'LNout303_accuracy', 'LNout304_accuracy', 'LNout305_accuracy', 'LNout306_accuracy', 'LNout307_accuracy', 'LNout308_accuracy', 'LNout309_accuracy', 'LNout310_accuracy', 'LNout311_accuracy', 'LNout312_accuracy', 'LNout313_accuracy', 'LNout314_accuracy', 'LNout315_accuracy', 'LNout316_accuracy', 'LNout317_accuracy', 'LNout318_accuracy', 'LNout319_accuracy', 'LNout320_accuracy', 'LNout321_accuracy', 'LNout322_accuracy', 'LNout323_accuracy', 'LNout324_accuracy', 'LNout325_accuracy', 'LNout326_accuracy', 'LNout327_accuracy', 'LNout328_accuracy', 'LNout329_accuracy', 'LNout330_accuracy', 'LNout331_accuracy', 'LNout332_accuracy', 'LNout333_accuracy', 'LNout334_accuracy', 'LNout335_accuracy', 'LNout336_accuracy', 'LNout337_accuracy', 'LNout338_accuracy', 'LNout339_accuracy', 'LNout340_accuracy', 'LNout341_accuracy', 'LNout342_accuracy', 'LNout343_accuracy', 'LNout344_accuracy', 'LNout345_accuracy', 'LNout346_accuracy', 'LNout347_accuracy', 'LNout348_accuracy', 'LNout349_accuracy', 'LNout350_accuracy', 'LNout351_accuracy', 'LNout352_accuracy', 'LNout353_accuracy', 'LNout354_accuracy', 'LNout355_accuracy', 'LNout356_accuracy', 'LNout357_accuracy', 'LNout358_accuracy', 'LNout359_accuracy', 'val_loss', 'val_error_feed_forward_output1_loss', 'val_LNout0_loss', 'val_LNout1_loss', 'val_LNout2_loss', 'val_LNout3_loss', 'val_LNout4_loss', 'val_LNout5_loss', 'val_LNout6_loss', 'val_LNout7_loss', 'val_LNout8_loss', 'val_LNout9_loss', 'val_LNout10_loss', 'val_LNout11_loss', 'val_LNout12_loss', 'val_LNout13_loss', 'val_LNout14_loss', 'val_LNout15_loss', 'val_LNout16_loss', 'val_LNout17_loss', 'val_LNout18_loss', 'val_LNout19_loss', 'val_LNout20_loss', 'val_LNout21_loss', 'val_LNout22_loss', 'val_LNout23_loss', 'val_LNout24_loss', 'val_LNout25_loss', 'val_LNout26_loss', 'val_LNout27_loss', 'val_LNout28_loss', 'val_LNout29_loss', 'val_LNout30_loss', 'val_LNout31_loss', 'val_LNout32_loss', 'val_LNout33_loss', 'val_LNout34_loss', 'val_LNout35_loss', 'val_LNout36_loss', 'val_LNout37_loss', 'val_LNout38_loss', 'val_LNout39_loss', 'val_LNout40_loss', 'val_LNout41_loss', 'val_LNout42_loss', 'val_LNout43_loss', 'val_LNout44_loss', 'val_LNout45_loss', 'val_LNout46_loss', 'val_LNout47_loss', 'val_LNout48_loss', 'val_LNout49_loss', 'val_LNout50_loss', 'val_LNout51_loss', 'val_LNout52_loss', 'val_LNout53_loss', 'val_LNout54_loss', 'val_LNout55_loss', 'val_LNout56_loss', 'val_LNout57_loss', 'val_LNout58_loss', 'val_LNout59_loss', 'val_LNout60_loss', 'val_LNout61_loss', 'val_LNout62_loss', 'val_LNout63_loss', 'val_LNout64_loss', 'val_LNout65_loss', 'val_LNout66_loss', 'val_LNout67_loss', 'val_LNout68_loss', 'val_LNout69_loss', 'val_LNout70_loss', 'val_LNout71_loss', 'val_LNout72_loss', 'val_LNout73_loss', 'val_LNout74_loss', 'val_LNout75_loss', 'val_LNout76_loss', 'val_LNout77_loss', 'val_LNout78_loss', 'val_LNout79_loss', 'val_LNout80_loss', 'val_LNout81_loss', 'val_LNout82_loss', 'val_LNout83_loss', 'val_LNout84_loss', 'val_LNout85_loss', 'val_LNout86_loss', 'val_LNout87_loss', 'val_LNout88_loss', 'val_LNout89_loss', 'val_LNout90_loss', 'val_LNout91_loss', 'val_LNout92_loss', 'val_LNout93_loss', 'val_LNout94_loss', 'val_LNout95_loss', 'val_LNout96_loss', 'val_LNout97_loss', 'val_LNout98_loss', 'val_LNout99_loss', 'val_LNout100_loss', 'val_LNout101_loss', 'val_LNout102_loss', 'val_LNout103_loss', 'val_LNout104_loss', 'val_LNout105_loss', 'val_LNout106_loss', 'val_LNout107_loss', 'val_LNout108_loss', 'val_LNout109_loss', 'val_LNout110_loss', 'val_LNout111_loss', 'val_LNout112_loss', 'val_LNout113_loss', 'val_LNout114_loss', 'val_LNout115_loss', 'val_LNout116_loss', 'val_LNout117_loss', 'val_LNout118_loss', 'val_LNout119_loss', 'val_LNout120_loss', 'val_LNout121_loss', 'val_LNout122_loss', 'val_LNout123_loss', 'val_LNout124_loss', 'val_LNout125_loss', 'val_LNout126_loss', 'val_LNout127_loss', 'val_LNout128_loss', 'val_LNout129_loss', 'val_LNout130_loss', 'val_LNout131_loss', 'val_LNout132_loss', 'val_LNout133_loss', 'val_LNout134_loss', 'val_LNout135_loss', 'val_LNout136_loss', 'val_LNout137_loss', 'val_LNout138_loss', 'val_LNout139_loss', 'val_LNout140_loss', 'val_LNout141_loss', 'val_LNout142_loss', 'val_LNout143_loss', 'val_LNout144_loss', 'val_LNout145_loss', 'val_LNout146_loss', 'val_LNout147_loss', 'val_LNout148_loss', 'val_LNout149_loss', 'val_LNout150_loss', 'val_LNout151_loss', 'val_LNout152_loss', 'val_LNout153_loss', 'val_LNout154_loss', 'val_LNout155_loss', 'val_LNout156_loss', 'val_LNout157_loss', 'val_LNout158_loss', 'val_LNout159_loss', 'val_LNout160_loss', 'val_LNout161_loss', 'val_LNout162_loss', 'val_LNout163_loss', 'val_LNout164_loss', 'val_LNout165_loss', 'val_LNout166_loss', 'val_LNout167_loss', 'val_LNout168_loss', 'val_LNout169_loss', 'val_LNout170_loss', 'val_LNout171_loss', 'val_LNout172_loss', 'val_LNout173_loss', 'val_LNout174_loss', 'val_LNout175_loss', 'val_LNout176_loss', 'val_LNout177_loss', 'val_LNout178_loss', 'val_LNout179_loss', 'val_LNout180_loss', 'val_LNout181_loss', 'val_LNout182_loss', 'val_LNout183_loss', 'val_LNout184_loss', 'val_LNout185_loss', 'val_LNout186_loss', 'val_LNout187_loss', 'val_LNout188_loss', 'val_LNout189_loss', 'val_LNout190_loss', 'val_LNout191_loss', 'val_LNout192_loss', 'val_LNout193_loss', 'val_LNout194_loss', 'val_LNout195_loss', 'val_LNout196_loss', 'val_LNout197_loss', 'val_LNout198_loss', 'val_LNout199_loss', 'val_LNout200_loss', 'val_LNout201_loss', 'val_LNout202_loss', 'val_LNout203_loss', 'val_LNout204_loss', 'val_LNout205_loss', 'val_LNout206_loss', 'val_LNout207_loss', 'val_LNout208_loss', 'val_LNout209_loss', 'val_LNout210_loss', 'val_LNout211_loss', 'val_LNout212_loss', 'val_LNout213_loss', 'val_LNout214_loss', 'val_LNout215_loss', 'val_LNout216_loss', 'val_LNout217_loss', 'val_LNout218_loss', 'val_LNout219_loss', 'val_LNout220_loss', 'val_LNout221_loss', 'val_LNout222_loss', 'val_LNout223_loss', 'val_LNout224_loss', 'val_LNout225_loss', 'val_LNout226_loss', 'val_LNout227_loss', 'val_LNout228_loss', 'val_LNout229_loss', 'val_LNout230_loss', 'val_LNout231_loss', 'val_LNout232_loss', 'val_LNout233_loss', 'val_LNout234_loss', 'val_LNout235_loss', 'val_LNout236_loss', 'val_LNout237_loss', 'val_LNout238_loss', 'val_LNout239_loss', 'val_LNout240_loss', 'val_LNout241_loss', 'val_LNout242_loss', 'val_LNout243_loss', 'val_LNout244_loss', 'val_LNout245_loss', 'val_LNout246_loss', 'val_LNout247_loss', 'val_LNout248_loss', 'val_LNout249_loss', 'val_LNout250_loss', 'val_LNout251_loss', 'val_LNout252_loss', 'val_LNout253_loss', 'val_LNout254_loss', 'val_LNout255_loss', 'val_LNout256_loss', 'val_LNout257_loss', 'val_LNout258_loss', 'val_LNout259_loss', 'val_LNout260_loss', 'val_LNout261_loss', 'val_LNout262_loss', 'val_LNout263_loss', 'val_LNout264_loss', 'val_LNout265_loss', 'val_LNout266_loss', 'val_LNout267_loss', 'val_LNout268_loss', 'val_LNout269_loss', 'val_LNout270_loss', 'val_LNout271_loss', 'val_LNout272_loss', 'val_LNout273_loss', 'val_LNout274_loss', 'val_LNout275_loss', 'val_LNout276_loss', 'val_LNout277_loss', 'val_LNout278_loss', 'val_LNout279_loss', 'val_LNout280_loss', 'val_LNout281_loss', 'val_LNout282_loss', 'val_LNout283_loss', 'val_LNout284_loss', 'val_LNout285_loss', 'val_LNout286_loss', 'val_LNout287_loss', 'val_LNout288_loss', 'val_LNout289_loss', 'val_LNout290_loss', 'val_LNout291_loss', 'val_LNout292_loss', 'val_LNout293_loss', 'val_LNout294_loss', 'val_LNout295_loss', 'val_LNout296_loss', 'val_LNout297_loss', 'val_LNout298_loss', 'val_LNout299_loss', 'val_LNout300_loss', 'val_LNout301_loss', 'val_LNout302_loss', 'val_LNout303_loss', 'val_LNout304_loss', 'val_LNout305_loss', 'val_LNout306_loss', 'val_LNout307_loss', 'val_LNout308_loss', 'val_LNout309_loss', 'val_LNout310_loss', 'val_LNout311_loss', 'val_LNout312_loss', 'val_LNout313_loss', 'val_LNout314_loss', 'val_LNout315_loss', 'val_LNout316_loss', 'val_LNout317_loss', 'val_LNout318_loss', 'val_LNout319_loss', 'val_LNout320_loss', 'val_LNout321_loss', 'val_LNout322_loss', 'val_LNout323_loss', 'val_LNout324_loss', 'val_LNout325_loss', 'val_LNout326_loss', 'val_LNout327_loss', 'val_LNout328_loss', 'val_LNout329_loss', 'val_LNout330_loss', 'val_LNout331_loss', 'val_LNout332_loss', 'val_LNout333_loss', 'val_LNout334_loss', 'val_LNout335_loss', 'val_LNout336_loss', 'val_LNout337_loss', 'val_LNout338_loss', 'val_LNout339_loss', 'val_LNout340_loss', 'val_LNout341_loss', 'val_LNout342_loss', 'val_LNout343_loss', 'val_LNout344_loss', 'val_LNout345_loss', 'val_LNout346_loss', 'val_LNout347_loss', 'val_LNout348_loss', 'val_LNout349_loss', 'val_LNout350_loss', 'val_LNout351_loss', 'val_LNout352_loss', 'val_LNout353_loss', 'val_LNout354_loss', 'val_LNout355_loss', 'val_LNout356_loss', 'val_LNout357_loss', 'val_LNout358_loss', 'val_LNout359_loss', 'val_error_feed_forward_output1_accuracy', 'val_LNout0_accuracy', 'val_LNout1_accuracy', 'val_LNout2_accuracy', 'val_LNout3_accuracy', 'val_LNout4_accuracy', 'val_LNout5_accuracy', 'val_LNout6_accuracy', 'val_LNout7_accuracy', 'val_LNout8_accuracy', 'val_LNout9_accuracy', 'val_LNout10_accuracy', 'val_LNout11_accuracy', 'val_LNout12_accuracy', 'val_LNout13_accuracy', 'val_LNout14_accuracy', 'val_LNout15_accuracy', 'val_LNout16_accuracy', 'val_LNout17_accuracy', 'val_LNout18_accuracy', 'val_LNout19_accuracy', 'val_LNout20_accuracy', 'val_LNout21_accuracy', 'val_LNout22_accuracy', 'val_LNout23_accuracy', 'val_LNout24_accuracy', 'val_LNout25_accuracy', 'val_LNout26_accuracy', 'val_LNout27_accuracy', 'val_LNout28_accuracy', 'val_LNout29_accuracy', 'val_LNout30_accuracy', 'val_LNout31_accuracy', 'val_LNout32_accuracy', 'val_LNout33_accuracy', 'val_LNout34_accuracy', 'val_LNout35_accuracy', 'val_LNout36_accuracy', 'val_LNout37_accuracy', 'val_LNout38_accuracy', 'val_LNout39_accuracy', 'val_LNout40_accuracy', 'val_LNout41_accuracy', 'val_LNout42_accuracy', 'val_LNout43_accuracy', 'val_LNout44_accuracy', 'val_LNout45_accuracy', 'val_LNout46_accuracy', 'val_LNout47_accuracy', 'val_LNout48_accuracy', 'val_LNout49_accuracy', 'val_LNout50_accuracy', 'val_LNout51_accuracy', 'val_LNout52_accuracy', 'val_LNout53_accuracy', 'val_LNout54_accuracy', 'val_LNout55_accuracy', 'val_LNout56_accuracy', 'val_LNout57_accuracy', 'val_LNout58_accuracy', 'val_LNout59_accuracy', 'val_LNout60_accuracy', 'val_LNout61_accuracy', 'val_LNout62_accuracy', 'val_LNout63_accuracy', 'val_LNout64_accuracy', 'val_LNout65_accuracy', 'val_LNout66_accuracy', 'val_LNout67_accuracy', 'val_LNout68_accuracy', 'val_LNout69_accuracy', 'val_LNout70_accuracy', 'val_LNout71_accuracy', 'val_LNout72_accuracy', 'val_LNout73_accuracy', 'val_LNout74_accuracy', 'val_LNout75_accuracy', 'val_LNout76_accuracy', 'val_LNout77_accuracy', 'val_LNout78_accuracy', 'val_LNout79_accuracy', 'val_LNout80_accuracy', 'val_LNout81_accuracy', 'val_LNout82_accuracy', 'val_LNout83_accuracy', 'val_LNout84_accuracy', 'val_LNout85_accuracy', 'val_LNout86_accuracy', 'val_LNout87_accuracy', 'val_LNout88_accuracy', 'val_LNout89_accuracy', 'val_LNout90_accuracy', 'val_LNout91_accuracy', 'val_LNout92_accuracy', 'val_LNout93_accuracy', 'val_LNout94_accuracy', 'val_LNout95_accuracy', 'val_LNout96_accuracy', 'val_LNout97_accuracy', 'val_LNout98_accuracy', 'val_LNout99_accuracy', 'val_LNout100_accuracy', 'val_LNout101_accuracy', 'val_LNout102_accuracy', 'val_LNout103_accuracy', 'val_LNout104_accuracy', 'val_LNout105_accuracy', 'val_LNout106_accuracy', 'val_LNout107_accuracy', 'val_LNout108_accuracy', 'val_LNout109_accuracy', 'val_LNout110_accuracy', 'val_LNout111_accuracy', 'val_LNout112_accuracy', 'val_LNout113_accuracy', 'val_LNout114_accuracy', 'val_LNout115_accuracy', 'val_LNout116_accuracy', 'val_LNout117_accuracy', 'val_LNout118_accuracy', 'val_LNout119_accuracy', 'val_LNout120_accuracy', 'val_LNout121_accuracy', 'val_LNout122_accuracy', 'val_LNout123_accuracy', 'val_LNout124_accuracy', 'val_LNout125_accuracy', 'val_LNout126_accuracy', 'val_LNout127_accuracy', 'val_LNout128_accuracy', 'val_LNout129_accuracy', 'val_LNout130_accuracy', 'val_LNout131_accuracy', 'val_LNout132_accuracy', 'val_LNout133_accuracy', 'val_LNout134_accuracy', 'val_LNout135_accuracy', 'val_LNout136_accuracy', 'val_LNout137_accuracy', 'val_LNout138_accuracy', 'val_LNout139_accuracy', 'val_LNout140_accuracy', 'val_LNout141_accuracy', 'val_LNout142_accuracy', 'val_LNout143_accuracy', 'val_LNout144_accuracy', 'val_LNout145_accuracy', 'val_LNout146_accuracy', 'val_LNout147_accuracy', 'val_LNout148_accuracy', 'val_LNout149_accuracy', 'val_LNout150_accuracy', 'val_LNout151_accuracy', 'val_LNout152_accuracy', 'val_LNout153_accuracy', 'val_LNout154_accuracy', 'val_LNout155_accuracy', 'val_LNout156_accuracy', 'val_LNout157_accuracy', 'val_LNout158_accuracy', 'val_LNout159_accuracy', 'val_LNout160_accuracy', 'val_LNout161_accuracy', 'val_LNout162_accuracy', 'val_LNout163_accuracy', 'val_LNout164_accuracy', 'val_LNout165_accuracy', 'val_LNout166_accuracy', 'val_LNout167_accuracy', 'val_LNout168_accuracy', 'val_LNout169_accuracy', 'val_LNout170_accuracy', 'val_LNout171_accuracy', 'val_LNout172_accuracy', 'val_LNout173_accuracy', 'val_LNout174_accuracy', 'val_LNout175_accuracy', 'val_LNout176_accuracy', 'val_LNout177_accuracy', 'val_LNout178_accuracy', 'val_LNout179_accuracy', 'val_LNout180_accuracy', 'val_LNout181_accuracy', 'val_LNout182_accuracy', 'val_LNout183_accuracy', 'val_LNout184_accuracy', 'val_LNout185_accuracy', 'val_LNout186_accuracy', 'val_LNout187_accuracy', 'val_LNout188_accuracy', 'val_LNout189_accuracy', 'val_LNout190_accuracy', 'val_LNout191_accuracy', 'val_LNout192_accuracy', 'val_LNout193_accuracy', 'val_LNout194_accuracy', 'val_LNout195_accuracy', 'val_LNout196_accuracy', 'val_LNout197_accuracy', 'val_LNout198_accuracy', 'val_LNout199_accuracy', 'val_LNout200_accuracy', 'val_LNout201_accuracy', 'val_LNout202_accuracy', 'val_LNout203_accuracy', 'val_LNout204_accuracy', 'val_LNout205_accuracy', 'val_LNout206_accuracy', 'val_LNout207_accuracy', 'val_LNout208_accuracy', 'val_LNout209_accuracy', 'val_LNout210_accuracy', 'val_LNout211_accuracy', 'val_LNout212_accuracy', 'val_LNout213_accuracy', 'val_LNout214_accuracy', 'val_LNout215_accuracy', 'val_LNout216_accuracy', 'val_LNout217_accuracy', 'val_LNout218_accuracy', 'val_LNout219_accuracy', 'val_LNout220_accuracy', 'val_LNout221_accuracy', 'val_LNout222_accuracy', 'val_LNout223_accuracy', 'val_LNout224_accuracy', 'val_LNout225_accuracy', 'val_LNout226_accuracy', 'val_LNout227_accuracy', 'val_LNout228_accuracy', 'val_LNout229_accuracy', 'val_LNout230_accuracy', 'val_LNout231_accuracy', 'val_LNout232_accuracy', 'val_LNout233_accuracy', 'val_LNout234_accuracy', 'val_LNout235_accuracy', 'val_LNout236_accuracy', 'val_LNout237_accuracy', 'val_LNout238_accuracy', 'val_LNout239_accuracy', 'val_LNout240_accuracy', 'val_LNout241_accuracy', 'val_LNout242_accuracy', 'val_LNout243_accuracy', 'val_LNout244_accuracy', 'val_LNout245_accuracy', 'val_LNout246_accuracy', 'val_LNout247_accuracy', 'val_LNout248_accuracy', 'val_LNout249_accuracy', 'val_LNout250_accuracy', 'val_LNout251_accuracy', 'val_LNout252_accuracy', 'val_LNout253_accuracy', 'val_LNout254_accuracy', 'val_LNout255_accuracy', 'val_LNout256_accuracy', 'val_LNout257_accuracy', 'val_LNout258_accuracy', 'val_LNout259_accuracy', 'val_LNout260_accuracy', 'val_LNout261_accuracy', 'val_LNout262_accuracy', 'val_LNout263_accuracy', 'val_LNout264_accuracy', 'val_LNout265_accuracy', 'val_LNout266_accuracy', 'val_LNout267_accuracy', 'val_LNout268_accuracy', 'val_LNout269_accuracy', 'val_LNout270_accuracy', 'val_LNout271_accuracy', 'val_LNout272_accuracy', 'val_LNout273_accuracy', 'val_LNout274_accuracy', 'val_LNout275_accuracy', 'val_LNout276_accuracy', 'val_LNout277_accuracy', 'val_LNout278_accuracy', 'val_LNout279_accuracy', 'val_LNout280_accuracy', 'val_LNout281_accuracy', 'val_LNout282_accuracy', 'val_LNout283_accuracy', 'val_LNout284_accuracy', 'val_LNout285_accuracy', 'val_LNout286_accuracy', 'val_LNout287_accuracy', 'val_LNout288_accuracy', 'val_LNout289_accuracy', 'val_LNout290_accuracy', 'val_LNout291_accuracy', 'val_LNout292_accuracy', 'val_LNout293_accuracy', 'val_LNout294_accuracy', 'val_LNout295_accuracy', 'val_LNout296_accuracy', 'val_LNout297_accuracy', 'val_LNout298_accuracy', 'val_LNout299_accuracy', 'val_LNout300_accuracy', 'val_LNout301_accuracy', 'val_LNout302_accuracy', 'val_LNout303_accuracy', 'val_LNout304_accuracy', 'val_LNout305_accuracy', 'val_LNout306_accuracy', 'val_LNout307_accuracy', 'val_LNout308_accuracy', 'val_LNout309_accuracy', 'val_LNout310_accuracy', 'val_LNout311_accuracy', 'val_LNout312_accuracy', 'val_LNout313_accuracy', 'val_LNout314_accuracy', 'val_LNout315_accuracy', 'val_LNout316_accuracy', 'val_LNout317_accuracy', 'val_LNout318_accuracy', 'val_LNout319_accuracy', 'val_LNout320_accuracy', 'val_LNout321_accuracy', 'val_LNout322_accuracy', 'val_LNout323_accuracy', 'val_LNout324_accuracy', 'val_LNout325_accuracy', 'val_LNout326_accuracy', 'val_LNout327_accuracy', 'val_LNout328_accuracy', 'val_LNout329_accuracy', 'val_LNout330_accuracy', 'val_LNout331_accuracy', 'val_LNout332_accuracy', 'val_LNout333_accuracy', 'val_LNout334_accuracy', 'val_LNout335_accuracy', 'val_LNout336_accuracy', 'val_LNout337_accuracy', 'val_LNout338_accuracy', 'val_LNout339_accuracy', 'val_LNout340_accuracy', 'val_LNout341_accuracy', 'val_LNout342_accuracy', 'val_LNout343_accuracy', 'val_LNout344_accuracy', 'val_LNout345_accuracy', 'val_LNout346_accuracy', 'val_LNout347_accuracy', 'val_LNout348_accuracy', 'val_LNout349_accuracy', 'val_LNout350_accuracy', 'val_LNout351_accuracy', 'val_LNout352_accuracy', 'val_LNout353_accuracy', 'val_LNout354_accuracy', 'val_LNout355_accuracy', 'val_LNout356_accuracy', 'val_LNout357_accuracy', 'val_LNout358_accuracy', 'val_LNout359_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZzU1Znv8c8Dzb4j+9J0o7iisolgAyKuMUYTddwSFzBxMpNMEieTm9GbGTMm0ZjcZEwmuTch2ggmonFLNAkaFRBZZREQFQXpBhoQkH0T6O7n/nFOWUXbQCFdXV3d3/frVS+rzm+pcwT66ef8nt/5mbsjIiJS0xpluwMiIlI/KcCIiEhGKMCIiEhGKMCIiEhGKMCIiEhGKMCIiEhGKMCI1AFm9oiZ/TDNfUvN7KLjPY9IpinAiIhIRijAiIhIRijAiKQpTk19x8yWmtkeM3vYzLqa2RQz22VmL5tZh5T9rzSzt8xsu5lNN7PTUrYNNLNF8bgngOZVvusKM1scj51tZmd9yj5/xcxWmtlWM3vOzHrEdjOz/zazTWa2I46pf9x2uZm9Hfu2zsz+7VP9D5MGTwFG5NhcA1wMnAx8DpgC3A10Ivx7+gaAmZ0MTAa+BXQG/gY8b2ZNzawp8CfgUaAj8GQ8L/HYQUAx8I/ACcBvgefMrNmxdNTMxgD3A9cB3YHVwONx8yXAqDiO9sD1wJa47WHgH929DdAfmHos3yuSoAAjcmz+x903uvs64DVgnru/4e77gWeBgXG/64G/uvtL7n4Q+D9AC+A8YBjQBHjQ3Q+6+1PA/JTv+ArwW3ef5+4V7j4R2B+POxZfBIrdfVHs313AcDMrAA4CbYBTAXP3d9x9QzzuIHC6mbV1923uvugYv1cEUIAROVYbU97vq+Zz6/i+ByFjAMDdK4G1QM+4bZ0futLs6pT3fYBvx+mx7Wa2HegdjzsWVfuwm5Cl9HT3qcCvgF8DG81svJm1jbteA1wOrDazV81s+DF+rwigACOSKesJgQII1zwIQWIdsAHoGdsS8lPerwV+5O7tU14t3X3ycfahFWHKbR2Au//S3QcDZxCmyr4T2+e7+1VAF8JU3h+P8XtFAAUYkUz5I/BZM7vQzJoA3yZMc80G5gDlwDfMLM/MrgaGphz7O+CrZnZuvBjfysw+a2ZtjrEPjwFjzWxAvH5zH2FKr9TMzonnbwLsAT4CKuI1oi+aWbs4tbcTqDiO/w/SgCnAiGSAu78LfAn4H+BDQkHA59z9gLsfAK4GbgO2Ea7XPJNy7ALCdZhfxe0r477H2odXgP8AniZkTScCN8TNbQmBbBthGm0L4ToRwM1AqZntBL4axyFyzEwPHBMRkUxQBiMiIhmhACMiIhmhACMiIhmhACMiIhmRl+0O1BWdOnXygoKCbHdDRCSnLFy48EN371zdNgWYqKCggAULFmS7GyIiOcXMVh9um6bIREQkIxRgREQkIxRgREQkIxRgREQkIxRgREQkIzIWYMysOD6OdVlK2wAzmxsfBbvAzIambLsrPtr1XTO7NKV9sJm9Gbf9MrHEuZk1M7MnYvu8+BClxDG3mtmK+Lo1U2MUEZHDy2QG8whwWZW2nwD/5e4DgP+MnzGz0wmrvJ4Rj/m/ZtY4HvP/gDuAfvGVOOftwDZ3Pwn4b+CBeK6OwD3AuYQl0O9JfU66iIjUjowFGHefAWyt2kxYJhygHeGBSABXAY+7+353LyEsTz7UzLoDbd19Tnz63yTg8ynHTIzvnwIujNnNpcBL7r7V3bcBL/HJQFdj3J37/vYOi9duz9RXiIjkpNq+0fJbwItm9n8Iwe282N4TmJuyX1lsOxjfV21PHLMWwN3LzWwH4Wl9H7dXc8whzOwOQnZEfn5+dbsc1eote5k8bw3jZ6xiUH57xo0o5LIzupHXWJe3RKRhq+2fgv8E3OnuvYE7gYdju1Wzrx+h/dMec2ij+3h3H+LuQzp3rnalg6Mq6NSKOXdfyH9deQZb9xzg64+9waifTOM3r77P9r0HPtU5RUTqg9oOMLeSfHLfkyQfE1tGeF55Qi/C9FlZfF+1/ZBjzCyPMOW29QjnypjWzfK49bwCpn57NA/dMoSCTq348ZTlDL9/Kt/705us3LQ7k18vIlIn1XaAWQ+cH9+PAVbE988BN8TKsELCxfzX3X0DsMvMhsXrK7cAf045JlEhdi0wNV6neRG4xMw6xIv7l8S2jGvUyLjo9K489pVhTPnmSK48uwd/XFDGRT9/ldsmvM6M9zajJ4iKSEORsUcmm9lkYDTQCdhIqOx6F/gF4drPR8A/u/vCuP//BsYB5cC33H1KbB9CqEhrAUwB/sXd3cyaA48CAwmZyw3uvioeMw64O3blR+4+4Wj9HTJkiGdiscstu/fz2Lw1TJq7ms279tOvS2vGFhXyhYE9adG08dFPICJSh5nZQncfUu02/UYdZCrAJBwor+Svb67n4ZklLFu3k/Ytm3DT0HxuHt6H7u1aZOx7RUQySQEmDZkOMAnuzoLV2yieWcKLb31AIzMuP7M740YUMqB3+4x/v4hITTpSgNHzYGqZmXFOQUfOKejI2q17mTSnlMdfX8tzS9arzFlE6hVlMFFtZTDV2b2/nKcXljFhVgmlW/bSo11zbh5ewI1De9O+ZdOs9ElEJB2aIktDNgNMQmWlM+3dTRTPKmHWyi00b9KIawb1YmxRISd1aZ3VvomIVEcBJg11IcCkWv7BTibMLOXZxes4UF7J+Sd3ZtyIQkb160Rc71NEJOsUYNJQ1wJMQtUy55O6tGZsUQFXD+ylMmcRyToFmDTU1QCTUF2Z841D87lFZc4ikkUKMGmo6wEmoWqZsyXKnIsKGJivpxKISO1SmXI9Um2Z8/y1PL9kPQPz2zOuqJDL+nejicqcRSTLlMFEuZLBVKdqmXP3ds25RWXOIlILNEWWhlwOMAmJMucJs0qZufLDlDLnAk7q0ibb3RORekgBJg31IcCkWv7BTh6ZVcozb6jMWUQyRwEmDfUtwCRs2b2fya+vYdKc1WxSmbOI1DAFmDTU1wCTcKC8kr+9uYGHZ5bw5rodtGvRhJvOVZmziBwfBZg01PcAk+DuLFy9jeJZJbywTGXOInJ8VKYsHzMzhhR0ZEhBR8q27WXSnNVMfn3Nx2XOY4sK+YzKnEWkBiiDiRpKBlOdPfvLeXpRGRNmlVLy4R66tW3OLef14aah+SpzFpEj0hRZGhpygEmorHSmv7eJ4pkqcxaR9BwpwGRsHsTMis1sk5ktS2l7wswWx1epmS2O7U3NbIKZvWlmS8xsdGxvk7L/YjP70MwejNtuM7PNKdu+nPI9t5rZivi6NVNjrG8aNTLGnNqV33/5XF781ig+P6AnTy4s46Kfz+CW4teZ/u4mKiv1C4mIpCdjGYyZjQJ2A5PcvX81238G7HD3e83sa8AQdx9rZl2AKcA57l5Z5ZiFwJ3uPsPMbovHfL3KPh2BBcAQwIGFwGB333ak/iqDqV7VMucTO7dibFEhVw/qScumuoQn0tBlJYNx9xnA1sN0yIDrgMmx6XTglXjcJmA7IUCkHtMP6AK8dpSvvhR4yd23xqDyEnDZpxxGg3dC62Z8fUw/Zn53DA9eP4BWzfL43p+WMfz+qfx4ynLWb9+X7S6KSB2VrVKhkcBGd18RPy8BrjKzPDMrBAYDvasccyPwhB+acl1jZkvN7CkzS+zfE1ibsk9ZbPsEM7vDzBaY2YLNmzcf75jqtaZ5jfj8wJ78+WtFPPXV4RSddALjZ7zPyJ9M4+uPLWLRmiMmiCLSAGVrjuNGktkLQDFwGmFqazUwGyivcswNwM0pn58HJrv7fjP7KjARGANUtw5KtfOA7j4eGA9hiuzYh9HwHK7M+S9LNzCgd3vGjVCZs4gEtR5gzCwPuJqQpQDg7uXAnSn7zAZWpHw+G8hz94Upx2xJOe3vgAfi+zJgdMq2XsD0GhuAfKxXh5bcfflpfPPCfjy9qIxHZpXyjclvfFzmfOM5+XRopTJnkYYqG79mXgQsd/eyRIOZtTSzVvH9xUC5u7+dckzVjAcz657y8Urgnfj+ReASM+tgZh2AS2KbZEirZnncMryAl//1fCbcdg79urbmJy+8y/Afv8Ldz77Jyk27st1FEcmCjGUwZjaZkEl0MrMy4B53f5gw1TW5yu5dgBfNrBJYx6FTYRAKAi6v0vYNM7uSMJW2FbgNwN23mtkPgPlxv3vdvdpiA6lZjRoZF5zahQtO7cK7H+zikdklPL2wjMfmrWHUyZ0ZV1TAqH6dadRIqzmLNAS60TJSmXJmbN1zgMmvr2Hi7FKVOYvUQ7qTPw0KMJl1oLySKcvCas5Ly8JqzjcODas592iv1ZxFcpUCTBoUYGqHu7NozTaKZ5YyZdkGzIzP9O/GuBGFDNJqziI5R6spS51hZgzu05HBfUKZ86NzVvOYypxF6iVlMJEymOzZs7+cZ+JqzqtSVnNWmbNI3acpsjQowGRfZaXz6nubKZ5VwmsrwmrOXxjYi3FFBfTrqtWcReoiTZFJTqiuzPmZRWVMfn0NI/t1YtyIQs5XmbNIzlAGEymDqZsSZc6T5pSyced++sYy52tU5ixSJ2iKLA0KMHVb1TLnts3zuPHcfG4ZXkBPlTmLZI0CTBoUYHJDdWXOl/XvxriiQgbltyc8CUJEaouuwUi9UV2Z8+TX1/DXpRs4u3d7xhUVcPmZ3VXmLFIHKIOJlMHkrr0Hynl60TomzCz5uMz55uF9uGmoypxFMk1TZGlQgMl9lZXOqys2UzxTZc4itUVTZNIgNGpkXHBKFy44pQvvbdzFhFmlKnMWySJlMJEymPpJZc4imaUpsjQowNRvBysq+dubGyieWcISlTmL1BgFmDQowDQMocx5O8WzSnhh2QcAKnMWOQ66BiMShTLnDgzu04F12/cxaU4pk+epzFkkE5TBRMpgGq6Py5xnlbBq8x66tm3GLcMLuHFoPh1V5ixyREfKYDL2a5qZFZvZJjNbltL2hJktjq9SM1sc25ua2QQze9PMlpjZ6JRjppvZuynHdYntzeL5VprZPDMrSDnmVjNbEV+3ZmqMUj+0bJrHzcP68PKd5zNh7Dmc3LUNP33xXYbf/wp3PbOU9zbuynYXRXJSJqfIHgF+BUxKNLj79Yn3ZvYzYEf8+JW4/cwYQKaY2TnuXhm3f9Hdq6YXtwPb3P0kM7sBeAC43sw6AvcAQwAHFprZc+6+rcZHKPXK4cuc14Yy56JCzj9ZZc4i6cpYBuPuM4Ct1W2zcCX1OmBybDodeCUetwnYTggQR3IVMDG+fwq4MJ73UuAld98ag8pLwGXHMRRpgE7u2ob7rz6TuXddyHcuPYX3Nu5i7CPzuejnr/LonFL27C/PdhdF6rxsXckcCWx09xXx8xLgKjPLM7NCYDDQO2X/CXF67D8sWebTE1gL4O7lhGzohNT2qCy2fYKZ3WFmC8xswebNm2tqbFKPdGjVlK9dcBIzvzuGX9wwgDbN8/iPP7/F8Ptf4f6/vcO67fuy3UWROitbVWQ3ksxeAIqB04AFwGpgNpD4FfGL7r7OzNoATwM3E6bdqpun8CO0f7LRfTwwHsJF/mMfhjQUTRo34qoBPbny7B4flzk/NDO8LjujG+NGFDAov4PKnEVS1HqAMbM84GpClgJ8nIHcmbLPbGBF3LYu/neXmT0GDCUEmDJCllMWz9mOMCVXBoxO+cpewPSMDUgalNQy5/Xb9zEpsZrzmxs4u1c7xo0o5DP9u9M0T2XOItn4V3ARsNzdyxINZtbSzFrF9xcD5e7+dpwy6xTbmwBXAImqtOeARIXYtcBUDzXXLwKXmFkHM+sAXBLbRGpUj/Yt+PfPnMqcu8bwg8/3Z9f+cr75+GJG/mQqv562kq17DmS7iyJZlbH7YMxsMiGT6ARsBO5x94fN7BFgrrv/JmXfAkIQqATWAbe7++oYdGYATYDGwMvAv7p7hZk1Bx4FBhIylxvcfVU83zjg7nj6H7n7hKP1V/fByPFKrOY8YVYpM97bTLO8Rlw9qCdjiwo5Was5Sz2lpWLSoAAjNWnFxl1MmB3KnD86WKkyZ6m3FGDSoAAjmbBtzwEmz1/DpNmr+WDnR/Tt1IqxRQVcPagXrZpppSbJfQowaVCAkUw6WFHJlGUf8PDMEpas3R5Wcx6azy3naTVnyW0KMGlQgJHasmjNNopnljAlsZqzypwlh2k1ZZE6ZFB+BwbdVH2Z89iiQi4/U2XOUj8og4mUwUi27D1QzjOL1lEcV3Pu0qYZtwzvw03n9tFqzlLnaYosDQowkm2Vlc6MFZspTilz/sLAUOZ8SjeVOUvdpCkykRzQqJEx+pQujD6lyyFlzo/PX8uIkzoxbkQBo0/uojJnyRnKYCJlMFIXVS1zLoxlzteozFnqCE2RpUEBRuqyRJlz8cwSFq/dTptEmfPwPvTq0DLb3ZMGTAEmDQowkitSy5zdncv6d2NcUSGD+6jMWWqfrsGI1CPVlTn/7c0POKtXO8apzFnqEGUwkTIYyVWJMucJs0p4X2XOUss0RZYGBRjJdZWVzmsrP6R4ZgmvqsxZaommyEQagEaNjPNP7sz5J3dm5aZdTJhVytMqc5YsUgYTKYOR+mj73gNMfn0tk+aUsmGHypyl5mmKLA0KMFKfHayo5IVlH1A8q4Q31qjMWWqOAkwaFGCkoVi0ZhsTZpXytzc3fFzmPLaokCEqc5ZPQQEmDQow0tCs376PR+eu5rF5a9ix7yBn9mzHuBEFfPbMHipzlrQdKcBk7G+RmRWb2SYzW5bS9oSZLY6vUjNbHNubmtkEM3vTzJaY2ejY3tLM/mpmy83sLTP7ccq5bjOzzSnn+3LKtlvNbEV83ZqpMYrksh7tW/Ddy05lzl1j+NEX+rP3QDl3PrGEEQ9M5VdTV7Bl9/5sd1FyXMYyGDMbBewGJrl7/2q2/wzY4e73mtnXgCHuPtbMugBTgHOA5sC57j7NzJoCrwD3ufsUM7stHvP1KuftCCwAhgAOLAQGu/u2I/VXGYw0dFXLnJvmNeILA3oydkQBp3Zrm+3uSR2VlTJld59hZgWH6ZAB1wFjYtPphOCBu28ys+2E4PE6MC22HzCzRUCvo3z1pcBL7r41ftdLwGXA5OMakEg9d7gy5ycWrKXopBMYV1TIBaeozFnSl62J1pHARndfET8vAa4yszwzKwQGA71TDzCz9sDniIEousbMlprZU2aW2L8nsDZln7LY9glmdoeZLTCzBZs3bz7+UYnUEyd1acOPvnAmc++6kO9ediqrNu/h9okLuPDnrzJxdil79pdnu4uSA7IVYG7k0IyimBAIFgAPArOBj/8Gm1le3P+X7r4qNj8PFLj7WcDLwMTE7tV8X7XzgO4+3t2HuPuQzp07H8dwROqn9i2b8k+jT2TG/7qA/7lxIO1bNuGe595i2P2v8KO/vs3arXuz3UWpw2r9TqsYLK4mZCkAuHs5cGfKPrOBFSmHjQdWuPuDKcdsSdn+O+CB+L4MGJ2yrRcwvWZ6L9IwNWnciM+d3YPPnd3j4zLn4lmlPDyzhEvP6Ma4ESpzlk/Kxq28FwHL3b0s0WBmLQkFB3vM7GKg3N3fjtt+CLQDvpx6EjPr7u4b4scrgXfi+xeB+8ysQ/x8CXBXxkYj0sAMyu/AoPwO3H35qR+v5jxl2Qcqc5ZPyGQV2WRCJtEJ2Ajc4+4Pm9kjwFx3/03KvgWEwFAJrANud/fVZtaLcD1lOZComfyVuz9kZvcTAks5sBX4J3dfHs83Drg77v8jd59wtP6qikzk09l3oIJn31hH8awSVm7aTec2zbhlWB9uOjefE1o3y3b3JMN0o2UaFGBEjo+789qKDymeVcL0d1Xm3FBoNWURyTgzY9TJnRl1cmdWbtrNI7NLeHrhOpU5N2DKYCJlMCI1b/veAzw+fy0TZ4fVnAtOaMnYokKuHazVnOsLTZGlQQFGJHMOVlTy4lsfUDyzhEVxNecbzunNLcML6N1RqznnMgWYNCjAiNSON1JWc650V5lzjlOASYMCjEjt2rBjH4/OWc1jr69h+16t5pyrFGDSoAAjkh3VlTnfHMucO6nMuc477gBjZt8EJgC7gIeAgcC/u/vfa7Kj2aQAI5Jd1ZU5f35AD8YWFXJad5U511U1UaY8zt1/YWaXAp2BsYSAU28CjIhk1+HKnP+4oIzzTgxlzmNOVZlzLkk3g1nq7meZ2S+A6e7+rJm94e4DM9/F2qEMRqTuqa7M+bbzCrh2SG9aq8y5TqiJKbIJhCXvC4GzgcaEQDP4iAfmEAUYkbrrE2XOzfK4/pze3HqeypyzrSYCTCNgALDK3bfHp0b2cvelNdvV7FGAEckNVcucLzk9lDmfU6Ay52yoiQBTBCyOqx1/CRgE/MLdV9dsV7NHAUYkt3yw4yMenVvKH+aFMuf+PdsyrqiQK85SmXNtqokAs5QwNXYW8CjwMHC1u59fkx3NJgUYkdy070AFf1q8juKZJaxQmXOtq4kAs8jdB5nZfwLr4rL7i9x9UE13NlsUYERym7szc+WHFM8sYZrKnGtNTZQp7zKzu4CbgZFm1hhoUlMdFBE5XmbGyH6dGdmvM+9v3s0js0p5amGZypyzKN0MphtwEzDf3V8zs3xgtLtPynQHa4syGJH6Z8fegzw+fw0TZ5eyXmXOGVEjS8WYWVfgnPjxdXffVEP9qxMUYETqr/KKSl58ayPFs0pYuHqbypxrUE1cg7kO+CkwHTBgJPAdd3+qBvuZVQowIg3D4rXbmTCrhL8uVZlzTaiJALMEuDiRtZhZZ+Bldz/7CMcUA1cAm9y9f2x7Ajgl7tIe2O7uA8ysKfBbYAhQCXzT3afHYwYDjwAtgL/FbW5mzYBJwGBgC3C9u5fGY24Fvhe/54fuPvFoY1SAEWlYqitzHnteIVec3Z1meY2z3b2ccaQAk26xeKMqU2Jb0jj2EeCy1AZ3v97dB7j7AOBp4Jm46Stx+5nAxcDP4s2dAP8PuAPoF1+Jc94ObHP3k4D/Bh4AiDeB3gOcCwwF7jGzDmmOU0QaiG7tmvOdS09lzr9fyP1Xn8n+g5V8+8klFP14Gr94eQUf7t6f7S7mvHQDzAtm9qKZ3WZmtwF/JWQTh+XuM4Ct1W2zkIdeB0yOTacDr8TjNgHbgSFm1h1o6+5zPKRak4DPx2OuAhKZyVPAhfG8lwIvuftWd98GvESVQCciktCiaWNuHJrP3+8cxaO3D+XMnm3575ff47wfT+U7Ty7h7fU7s93FnJVWGYW7f8fMrgGKCNdgxrv7s8fxvSOBje6+In5eAlxlZo8DvQnTXr0J02VlKceVEdZEI/53bexfuZntAE5Iba/mmEOY2R2E7Ij8/PzjGI6I5LrDlTk/ubCM4X1PYNyIUObcWGXOaUu7Ts/dnyZMa9WEG0lmLwDFwGnAAmA1MBsoJwSzT3Ql/vdw2450zKGN7uOB8RCuwaTTcRGp/07s3JoffL4//3bJKR+XOX9l0gL6xDLnf1CZc1qO+H/IzHZR/Q9nA9zdj/n2WDPLA64mZCkQTlQO3Jmyz2xgBbAN6JVyeC9gfXxfRshyyuI52xGm5MqA0VWOmX6s/RQRadeyCf94/oncPqLw4zLn/3r+bX7+9/e47pze3KYy5yM6YoBx9zYZ+M6LgOXu/vHUl5m1JFS07TGzi4Fyd387bttlZsOAecAtwP/Ew54DbgXmANcCU2N12YvAfSkX9i8B7srAOESkgchr3IjPntWdz57VnSWxzHni7FImzCrh4tO7Mq6okKGFHVXmXEXaN1oe84nNJhMyiU7ARuCeuIbZI8Bcd/9Nyr4FwIuEay7rgNsTKzWb2RCSZcpTgH+JgaQ5YeHNgYTM5QZ3XxWPGQfcHU//I3efcLT+qkxZRI5Fosz5sXlr2Lb3IGf0iKs5N7Ay5xq5k7++U4ARkU/jo4MV/OmNdRTPKuG9jbvp1Dqs5vzFYQ1jNWcFmDQcV4DZuQHadq/ZDolITnF3Zq3cQvGsEqYu30TTxo24Kq7mfHqP+ruac02spiyHs3cr/Pw06FAAJ14AJ46BgpHQon22eyYitcjMGNGvEyP6deL9zbuZOLuUJxc07DJnZTDRp85gPtoBiyfDqmlQOhMO7AZrBD0HQ98LQtDpdQ401tMNRBqaHXsP8sSCNUycvZp12/fVyzJnTZGloUauwZQfgLL5Idi8Pw3WLwKvhKatoWBEyG76XgCd+oGqTUQajPKKSv7+9kaKZ5awIK7mXF/KnBVg0pCRi/z7tkHJa8mAs60ktLftmcxu+o6GVp1q9ntFpM5KlDn/Ja7mfPHpXRlbVMi5OVrmrACThlqpIttakgw2Ja+G6TWAbmfFYHMB5A+HJs0z2w8RybqNOz/i0Tmr+cO81Wzbe5DTu7dl3IhCPpdjZc4KMGmo9TLlygpYvxhWTYX3p8PaeVB5EPKahyCTCDhd+0OjdNckFZFcU12Z85eG5fPFc/vQuU3dL3NWgElD1u+D2b8bVs8K2c2qabB5eWhv1TlMoyWm1Nr2yF4fRSRjqitzvnJAD8YWFXBGj3bZ7t5hKcCkIesBpqqd62HV9GTA2bM5tHc6JVkO3acImrXOajdFpOat2rybR2KZ876DFQzr25FxRYVceFrXOlfmrACThjoXYFJVVsKmt5LBZvVsKP8IGjWB3kOT2U2PgdAod+ZuReTIqpY553dMlDn3ok3zunHrgwJMGup0gKnq4Eewdi68PzUEnQ+Whvbm7aBwVLIcumNhdvspIjWiaplz62Z5XDcklDnnn5DdMmcFmDTkVICpas+HYTpt1bRQMLAzLlTdoSCZ3RSOghZ6crRIrkstc65w5+LTujJuRPbKnBVg0pDTASaVO2xZmcxuSl9Lri7QY1CyOq3XOZDXNNu9FZFPKVHm/Njra9i650DWypwVYNJQbwJMVRUHoWxB8v6bdQvC6gJNWsXVBWLBQKeTtbqASA766GAFf168juKZpby7cRedWjflS8P61FqZswJMGuptgKlq3/aQ1SQKBrauCu1teiSzm76joXXnbPZSRI6RuzP7/S0UzyzhlVosc1aASUODCTBVbVudzG5WTYePtof2rmfCiaNDwOlzHjRpkc1eisgxWJVYzQUv3TMAABW4SURBVHlhGXsPZLbMWQEmDQ02wKSqrIANi5PBZs3csLpA42bQZ3iyYKDrmVpdQCQH7Nh3kD/OX8sjs0szVuasAJMGBZhqHNgT7rlJFAxsfie0t+wEfc9PlkO365ndforIEZVXVPLS2xspnlXC/NKaLXPOSoAxs2LgCmCTu/ePbU8Ap8Rd2gPb3X2AmTUBHgIGER6CNsnd7zezNsBrKaftBfze3b9lZrcBPwXWxW2/cveH4vfcCnwvtv/Q3Scerb8KMGnYuSGlHHoa7NkU2judnMxuCkZAszZZ7aaIHN7Ssu1MmFXK80vWU+HORad1ZVxRIcP6froy52wFmFHAbkKw6F/N9p8BO9z9XjO7CbjS3W8ws5bA28Body+tcsxC4E53nxEDzBB3/3qVfToCC4AhgAMLgcHuvu1I/VWAOUbusPGtZLBZPRvK90GjvFACnchuegyExvXjwUoi9cnGnR/x+7mr+cO8NfTu0II/fa2oxgNMxv7lxyBQcJgOGXAdMCaxO9DKzPKAFsABYGeVY/oBXTg0o6nOpcBL7r41HvcScBkw+VMNRKpnBt36h9d5/xJXF5iXDDjT7oNpP4Jm7aBwZLIcumPfbPdcRICubZvz7UtO4WsXnMTGnR9l5CbNbP1qORLY6O4r4uengKuADUBLQpaytcoxNwJP+KEp1zUxU3ovHrMW6AmsTdmnLLZ9gpndAdwBkJ+ff3wjauiaNA/XZfqeDxd9H/ZsgZLpyYKB5X8J+7XvkyyHLhwFLTtmr88iQvMmjelzQquMnDtbAeZGDs0ohgIVQA+gA/Camb3s7qtS9rkBuDnl8/PAZHffb2ZfBSYSMqLqwnC184DuPh4YD2GK7FOORarT6gTof014ucOW92N2MxXefBoWPgJYmEJLBJze52p1AZF6pNYDTJwGuxoYnNJ8E/CCux8ENpnZLMI1lFXxmLOBPHdfmDjA3bekHP874IH4vgwYnbKtFzC9Zkchx8QMOp0UXkO/ElYXWLcwebPnzAfhtZ/F1QWKkgUDnU/V6gIiOSwbGcxFwHJ3L0tpWwOMMbPfE6bIhgEPpmyvmvFgZt3dfUP8eCUQa2h5EbjPzBIrO14C3FWzQ5Dj0rgJ5A8LrwvuCo+OLp2ZLIde8fewX5vuYVWBE8fE1QW6ZK/PInLMMhZgzGwyIZPoZGZlwD3u/jBhqqvqBfdfAxOAZYQprgnuvjRl+3XA5VWO+YaZXQmUA1uB2wDcfauZ/QCYH/e7t5rrOVKXNG8Hp342vAC2r0lmN++9AEviX5eu/WPAuQDyz4Om2V2mXESOTDdaRipTrqMqK2DDkmR12tp5UHEgrC6Qf26yHLrbWVpdQCQLdCd/GhRgcsSBPbB6TjLgbHortLc8AQrPTxYMtO+d3X6KNBBZuQ9GJCOatoJ+F4UXwK4PQhl0YkrtrWdC+wn9ksGmYAQ0b5u1Los0VMpgImUw9YA7bHonmd2UzgyrC1jjuLpAvNmzxyCtLiBSQzRFlgYFmHqofH+4ZpPIbtYvBjy5ukDf0cnVBVQOLfKpaIpMGqa8ZmG1gMJRwD2wd2vKYp3Tk6sLtMsPz745cUy4jqPVBURqhDKYSBlMA+Menub5/tQQdEpmwP6dhNUFBiRv9ux9bghUIlItTZGlQQGmgasoh/WLkjd7ls0Hr4AmLcMTPRPl0F1O03SaSAoFmDQowMghPtoZigQS66dtWRnaW3dL3uzZdzS06Za9PorUAboGI3KsmreFUy8PL4Dta5PVaSv+DksfD+1dzkiWQ/fR6gIiqZTBRMpgJG2VlfDB0mR2s2ZuXF2gabhmkyiH7na2VheQek9TZGlQgJFP7cBeWDM7+eybjctCe4uO8Rk5sWCgvZ45JPWPpshEMqlpSzjpovAC2LURSl5NFgy89Wxo73hiysPWRoZFPkXqMWUwkTIYyQh32Lw8ebNn6Uw4uDeuLjAkmd30HBweYyCSYzRFlgYFGKkV5Qeg7PVkdrP+DcChaZtwQ2giwznhRJVDS05QgEmDAoxkxd6t4SbPRMHA9jWhvV3vZDl04ejwCGqROkjXYETqqpYd4YzPh1didYFEOfTbz8EbjwIG3c9OZjf5w7S6gOQEZTCRMhipcyrKwxRaIrspmw+V5ZDXIq4uEMuhu5yu6TTJGk2RpUEBRuq8/btCkUCiYODD90J7665hOq1vXF2gbffs9VEanKxMkZlZMXAFsMnd+8e2J4BT4i7tge3uPsDMmgAPAYNinya5+/3xmOlAd2BfPO4Sd99kZs2AScBgYAtwvbuXxmNuBb4X9/+hu0/M1DhFak2zNnDKZ8ILYEdZfNjaVFj5Mix9IrR3Pi3lYWtF4SFtIlmQsQzGzEYBuwnBon81238G7HD3e83sJuBKd7/BzFoCbwOj3b00Bph/c/cFVY7/Z+Asd/+qmd0AfMHdrzezjsACYAjgwEJgsLtvO1J/lcFITqushI1vJrOb1XOgYj80ahKu2fQdHYJO9wHQqHG2eyv1SFYyGHefYWYFh+mQAdcBYxK7A63MLA9oARwAdh7lK64Cvh/fPwX8Kp73UuAld98av+sl4DJg8qcdi0id16hRKATofjaM+BYc3AerZyeffTP1B+HVokMsh46rQ3fok+2eSz2WrSqykcBGd18RPz9FCBgbgJbAnYkAEU0wswrgacKUlwM9gbUA7l5uZjuAE1Lbo7LY9glmdgdwB0B+vpbxkHqkSQs46cLwAti9CVa9mlKh9ufQ3rFv8mbPgpHQon32+iz1TrYCzI0cmlEMBSqAHkAH4DUze9ndVwFfdPd1ZtaGEGBuJlx7qa5sxo/Q/slG9/HAeAhTZJ9yLCJ1X+sucNY/hJc7bH43GWyWPA4LHgZrFFYUSGQ3vYZodQE5LrUeYOI02NWEi/MJNwEvuPtBYJOZzSJcQ1nl7usA3H2XmT1GCEaTCJlJb6AsnrMdsDW2j045dy9geibHJJJTzKDLqeE17J/i6gLzk+XQM34Krz4QVhcoGJEsGOjUT+XQckyykcFcBCx397KUtjXAGDP7PWGKbBjwYAwc7d39w1hpdgXwcjzmOeBWYA5wLTDV3d3MXgTuM7MOcb9LgLsyPiqRXJXXNFSbFRTBmO/Bvm1hdYFEwcB7U8J+bXvBiaOT5dCtOmWx05ILMlmmPJmQSXQyszLgHnd/GLiBT15w/zUwAVhGmOKa4O5LzawV8GIMLo0JweV38ZiHgUfNbCUhc7kBwN23mtkPgPlxv3urXM8RkSNp0QFOvyq8ALaWJLObd56HN34f2rudlbzZs/cwaNI8e32WOkk3WkYqUxZJQ2VFWF0gkd2snRdXF2geVhdIFAx07a/ptAZCd/KnQQFG5FPYvwtKZyULBj58N7S36hzvvRkTVxfokb0+SkZpsUsRyYxmbeCUy8ILYMe6sLpAIuC8+WRo73xqMrvpUwTNWmety1J7lMFEymBEalhlZXh8dCLYrJkD5R+F1QV6nxsLBsZAD60ukMs0RZYGBRiRDDu4D9bMTRYMfPBmaG/e/tCHrXUszG4/5ZgowKRBAUaklu3eDCWvJgsGdq4L7R0Kk8GmcJRWF6jjFGDSoAAjkkXu8OGKkNmsmhYeS3Bgd1hdoMegZDl0r3O0ukAdowCTBgUYkTqk4mBYXSCR3axbCF4JTVuH1QUSBQOdTlY5dJYpwKRBAUakDtu3PawukCgY2FYS2tv2PPRha607Z6+PDZTKlEUkt7VoD6dfGV4A20qT2c3yv8LiP4T2bmcms5v84WFVackaZTCRMhiRHFVZAesXw6qp4dk3a+dB5cGwukD+8GTBQNf+4bk5UqM0RZYGBRiRemL/7pSHrU2FzctDe8tOySd79r0A2lX7mCg5RpoiE5GGo1lrOPmS8ALYuT6sLvD+tPDfZU+F9k6nJINNQVFYlUBqlDKYSBmMSAPgDhvfSpZDr54dVxfIg15Dk+XQPQZqdYE0aYosDQowIg3QwY9g7dxkwcCGJaG9ebtwk2eiYKBj3+z2sw7TFJmISHWaNI9lzqOB/4I9H6Ys1jk9PP8GoH2fZHZTOCo8M0eOShlMpAxGRA7hDltWJrObktfgwK64usDAZHbTa2h4KmgDpSmyNCjAiMgRVRyEsgXJmz3XLQSvgCatQpHAiWNC0Ol8SoNaXUABJg0KMCJyTD7aEbKaRDn01lWhvU33ZHbTdzS07pLNXmZcVq7BmFkxcAWwyd37x7YngFPiLu2B7e4+wMyaAA8Bg2KfJrn7/WbWEngSOBGoAJ5393+P57oN+CkQl2DlV+7+UNx2K/C92P5Dd5+YqXGKSAPVvB2cdkV4AWxbncxu3psCSx4L7V3PjM++uSA8VroBrS6QsQzGzEYBuwnBon81238G7HD3e83sJuBKd78hBpW3gdHAJuBcd59mZk2BV4D73H1KDDBD3P3rVc7bEVgADAEcWAgMdvdtR+qvMhgRqTGVFaEi7eOHrc0Nqws0bgb5w5L333Q7K+dXF8hKBuPuM8ys4DAdMuA6YExid6CVmeUBLYADwE533wtMi+c7YGaLgF5H+epLgZfcfWv8rpeAy4DJxzUgEZF0NWoMPQeF18hvw4E94Z6bRMHAy98Hvg8tT0gu1nniBdDuaD/ecku2ypRHAhvdfUX8/BRwFbABaAncmQgQCWbWHvgc8IuU5mtipvRePGYt0BNYm7JPWWz7BDO7A7gDID8//3jHJCJSvaatoN/F4QWwc0OyHHrVdFj2dGg/oV8oFjjxgvBYghxfXSBbAeZGDs0ohhKusfQAOgCvmdnL7r4KIGY2k4FfJtqA54HJ7r7fzL4KTCRkRNWVb1Q7D+ju44HxEKbIjntUIiLpaNsdBtwYXu6w6e1kdrNoErz+27i6wDnJ7KbHIGicW7cu1npvY7C4Ghic0nwT8IK7HwQ2mdkswjWURDAZD6xw9wcTB7j7lpTjfwc8EN+XEa7fJPQCptfgEEREao4ZdD0jvM77OpTvD9dsEtdvpt8P0++DZu2gcGTy+k3HvnW+HDob4fAiYLm7l6W0rQHGmNnvCVNkw4AHAczsh0A74MupJzGz7u6+IX68Engnvn8RuM/MErfaXgLclYmBiIjUuLxm0Pf88Lro+7BnC5S8mgw4y/8S9mufn8xuCs+Hlh2z2etqZbJMeTIhk+hkZmXAPe7+MHADn7zg/mtgArCMMMU1wd2Xmlkv4H8Dy4FFoTbg43Lkb5jZlUA5sBW4DcDdt5rZD4D58dz3Vr2eIyKSM1qdAP2vDi932PJ+Mti89SwsmghYWF0gkd30HhoCVZbpRstIZcoiknMqysOKAombPcsWxNUFWkKfomTA6XJaxqbTdCd/GhRgRCTnfbQDSmcmCwa2rAztrbslg03f0dCma419pVZTFhFpCJq3g1M/G14A29ckg817L8KSeHWiyxlxdegLIP88aNoyI91RBhMpgxGReq2yEj5Ykgw4a+ZCxQFo3BROvQL+YcKnOq0yGBGRhq5RfMxAj4Ew8l/hwN6wusCqaRkrCFCAERFpiJq2hH4XhVeG5PYqayIiUmcpwIiISEYowIiISEYowIiISEYowIiISEYowIiISEYowIiISEYowIiISEZoqZjIzDYDq4/jFJ2AD2uoO7mioY25oY0XNOaG4njG3MfdO1e3QQGmhpjZgsOtx1NfNbQxN7TxgsbcUGRqzJoiExGRjFCAERGRjFCAqTnjs92BLGhoY25o4wWNuaHIyJh1DUZERDJCGYyIiGSEAoyIiGSEAswxMLPLzOxdM1tpZv9ezXYzs1/G7UvNbFA2+lmT0hjzF+NYl5rZbDM7Oxv9rElHG3PKfueYWYWZXVub/cuEdMZsZqPNbLGZvWVmr9Z2H2taGn+325nZ82a2JI55bDb6WVPMrNjMNpnZssNsr/mfX+6uVxovoDHwPtAXaAosAU6vss/lwBTAgGHAvGz3uxbGfB7QIb7/TEMYc8p+U4G/Addmu9+18OfcHngbyI+fu2S737Uw5ruBB+L7zsBWoGm2+34cYx4FDAKWHWZ7jf/8UgaTvqHASndf5e4HgMeBq6rscxUwyYO5QHsz617bHa1BRx2zu892923x41ygVy33saal8+cM8C/A08Cm2uxchqQz5puAZ9x9DYC75/q40xmzA23MzIDWhABTXrvdrDnuPoMwhsOp8Z9fCjDp6wmsTflcFtuOdZ9ccqzjuZ3wG1AuO+qYzawn8AXgN7XYr0xK58/5ZKCDmU03s4Vmdkut9S4z0hnzr4DTgPXAm8A33b2ydrqXFTX+8yvvuLrTsFg1bVVrvNPZJ5ekPR4zu4AQYEZktEeZl86YHwS+6+4V4ZfbnJfOmPOAwcCFQAtgjpnNdff3Mt25DElnzJcCi4ExwInAS2b2mrvvzHTnsqTGf34pwKSvDOid8rkX4TebY90nl6Q1HjM7C3gI+Iy7b6mlvmVKOmMeAjweg0sn4HIzK3f3P9VOF2tcun+3P3T3PcAeM5sBnA3kaoBJZ8xjgR97uECx0sxKgFOB12uni7Wuxn9+aYosffOBfmZWaGZNgRuA56rs8xxwS6zGGAbscPcNtd3RGnTUMZtZPvAMcHMO/zab6qhjdvdCdy9w9wLgKeCfczi4QHp/t/8MjDSzPDNrCZwLvFPL/axJ6Yx5DSFjw8y6AqcAq2q1l7Wrxn9+KYNJk7uXm9nXgRcJFSjF7v6WmX01bv8NoaLocmAlsJfwG1DOSnPM/wmcAPzf+Bt9uefwSrRpjrleSWfM7v6Omb0ALAUqgYfcvdpy11yQ5p/zD4BHzOxNwvTRd909Z5fxN7PJwGigk5mVAfcATSBzP7+0VIyIiGSEpshERCQjFGBERCQjFGBERCQjFGBERCQjFGBERCQjFGBE6oG40vFfst0PkVQKMCIikhEKMCK1yMy+ZGavx+eq/NbMGpvZbjP7mZktMrNXzKxz3HeAmc2Nz+Z41sw6xPaTzOzl+JySRWZ2Yjx9azN7ysyWm9kfrJ4slCa5SwFGpJaY2WnA9UCRuw8AKoAvAq2ARe4+CHiVcIc1wCTC3eNnEVbzTbT/Afi1u59NeB5PYjmPgcC3gNMJzzkpyvigRI5AS8WI1J4LCSsSz4/JRQvC82QqgSfiPr8HnjGzdkB7d088OXIi8KSZtQF6uvuzAO7+EUA83+vuXhY/LwYKgJmZH5ZI9RRgRGqPARPd/a5DGs3+o8p+R1q/6UjTXvtT3legf9+SZZoiE6k9rwDXmlkXADPraGZ9CP8Or4373ATMdPcdwDYzGxnbbwZejc8iKTOzz8dzNIurG4vUOfoNR6SWuPvbZvY94O9m1gg4CHwN2AOcYWYLgR2E6zQAtwK/iQFkFcnVbW8Gfmtm98Zz/EMtDkMkbVpNWSTLzGy3u7fOdj9EapqmyEREJCOUwYiISEYogxERkYxQgBERkYxQgBERkYxQgBERkYxQgBERkYz4/yn4Cj14ArjeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (360,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-823d92b62b87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTestTranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-60201de7f9df>\u001b[0m in \u001b[0;36mtest_translate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0mplotTrainingLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mplotTrainingErrorTypeAcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[0mplotTrainingErrorLineAcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model training completed...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a5796f538dc5>\u001b[0m in \u001b[0;36mplotTrainingErrorLineAcc\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m360\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"LNout\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"_accuracy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m360\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#draw acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (360,) (2,) "
     ]
    }
   ],
   "source": [
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model para\n",
    "model_name = \"test_model1.h5\"\n",
    "x_test_model = \"x_test_500.npy\" \n",
    "y_test_mdodel1 = \"y_test0_500.npy\" \n",
    "y_test_mdodel2 = \"y_test1_500.npy\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "def loadmodel(model_name, x_test_model, y_test_mdodel1, y_test_mdodel2):\n",
    "    import numpy as np\n",
    "    #load model and dic ps. dic is not use\n",
    "    model, source_token_dict = load(model_name)\n",
    "    #print(model.summary())\n",
    "    #load\n",
    "    \n",
    "    '''\n",
    "    Para:\n",
    "        x_test_loaded  : answer model        (include type and lineblock)\n",
    "        y_test_loaded_0: predict model       (type)\n",
    "        y_test_loaded_1: predict model       (lineblock)\n",
    "        out1           : answer model output (type) \n",
    "        out2           : answer model output (lineblock)\n",
    "    '''\n",
    "    \n",
    "    x_test_loaded = loadTestTrainData(\"test_models/\" + x_test_model) \n",
    "    y_test_loaded_0 = loadTestTrainData(\"test_models/\" + y_test_mdodel1)\n",
    "    y_test_loaded_1 = loadTestTrainData(\"test_models/\" + y_test_mdodel2) \n",
    "    \n",
    "    ''' <-------dust switch\n",
    "    print(\"y_test_loaded_1 shape: \", y_test_loaded_1.shape)\n",
    "    print(\"x_test_loaded length: \", len(x_test_loaded))\n",
    "    #'''\n",
    "    \n",
    "    out1, out2 = tfr.decode(model,x_test_loaded, max_len = x.getsource_max_lan())\n",
    "    \n",
    "    #==============show org result================\n",
    "    ''' <-------dust switch\n",
    "    print(\"y_test_loaded_0 shape: \", y_test_loaded_0.shape)\n",
    "    print(\"y_test_loaded_1 shape: \", y_test_loaded_1.shape)\n",
    "    print(\"y_test_loaded_0[0] result:\", (y_test_loaded_0[0])) #Error_type #vs out1\n",
    "    print(\"y_test_loaded_1[0][1] result:\", (y_test_loaded_1[0][1])) #Line_Block #vs out2\n",
    "    print(\"out1 shape: \", (out1).shape)#prob upper then 0.5\n",
    "    print(\"out2[0] shape: \", (out2[0]).shape)#prob upper then 0.5\n",
    "    print(\"out2 length: \", len(out2))\n",
    "    print(\"out2[0] length: \", len(out2[0]))#prob lb\n",
    "    #'''\n",
    "    \n",
    "    #=============================================\n",
    "    test_ep = np.around(out1)\n",
    "    test_lb = np.around(out2)\n",
    "    ans_ep = np.around(y_test_loaded_0)\n",
    "    ans_lb = np.around(y_test_loaded_1)\n",
    "    #==============show toint result==============\n",
    "    \n",
    "    ''' <-------dust switch\n",
    "    print(test_ep[1])\n",
    "    print(test_lb[1])\n",
    "    print(ans_ep[1])\n",
    "    print([ans_lb[1]])\n",
    "    #'''\n",
    "    #=============================================\n",
    "    return test_ep, ans_ep, test_lb, ans_lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(pre_errortype, ans_errortype):\n",
    "    #print(\"inter :\", pre_errortype)\n",
    "    #print(\"inter: \", ans_errortype)\n",
    "    #ans_errortype = float(ans_errortype)\n",
    "    #ref = \"https://www.796t.com/post/Mjc4am8=.html\"\n",
    "    inter = [pre_value for pre_value, ans_value in zip(pre_errortype, ans_errortype) if (pre_value == ans_value == 1)]\n",
    "    inter_two = [pre_value for pre_value, ans_value in zip(pre_errortype, ans_errortype) if (pre_value == ans_value)]\n",
    "    #print(\"sort inter: \", inter)\n",
    "    return inter, inter_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new ans array contains only 1 in array \n",
    "def ans_typefilter(ans_errortype):\n",
    "    #print(\"org ans type: \", ans_errortype)\n",
    "    new_ans_errortype = [value for value in ans_errortype if value == 1]\n",
    "    #print(\"new ans errortype: \", new_ans_errortype)\n",
    "    return new_ans_errortype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new pre array contains only 1 in array \n",
    "def pre_typefilter(pre_errortype):\n",
    "    #print(\"org error type: \", pre_errortype)\n",
    "    new_pre_errortype = [value for value in pre_errortype if value == 1]\n",
    "    #print(\"new pre errortype: \", new_pre_errortype)\n",
    "    return new_pre_errortype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate error type score\n",
    "#Note:\n",
    "    #Predict score:\n",
    "        # score = inter of ans and pre divide by len of pre\n",
    "    #Recall score:\n",
    "        # score = inter of ans and pre divide by len of ans\n",
    "    #Accuracy score:\n",
    "        # score = inter_two of ans and pre divide by original len of pre\n",
    "def errortype_score(pre_errortype, ans_errortype):\n",
    "    #find pre length\n",
    "    new_pre_errortype = pre_typefilter(pre_errortype) #make new array for score\n",
    "    pre_length = len(new_pre_errortype) #get lenght from ans_type\n",
    "    #find ans length\n",
    "    new_ans_errortype = ans_typefilter(ans_errortype) #make new array for score\n",
    "    ans_length = len(new_ans_errortype) #get lenght from ans_type\n",
    "    #print(\"ans length: \", ans_length)\n",
    "    inter, inter_two = intersect(pre_errortype, ans_errortype) #get intersection\n",
    "    inter_length = len(inter)\n",
    "    inter_two_length = len(inter_two)\n",
    "    #print(\"inter_length: \", inter_length)\n",
    "    #print(\"pre length: \", pre_length) \n",
    "    \n",
    "    #calculate Predict score \n",
    "    if (inter_length == 0 and pre_length == 0):\n",
    "        pre_score = 1\n",
    "    elif (pre_length == 0):\n",
    "        pre_score = 0\n",
    "    else:\n",
    "        pre_score = inter_length/pre_length \n",
    "    #print(\"predict score: \", pre_score) #show pre score\n",
    "    \n",
    "    #calculate Recall score\n",
    "    if (inter_length == 0 and ans_length == 0):\n",
    "        rec_score = 1\n",
    "    elif (ans_length == 0):\n",
    "        rec_score = 0\n",
    "    else:\n",
    "        rec_score = inter_length/ans_length\n",
    "    #print(\"recall score: \", rec_score) #show ans score\n",
    "    \n",
    "    #calculate Accuarcy score\n",
    "    acc_score = inter_two_length/len(pre_errortype)\n",
    "    \n",
    "    return pre_score, rec_score , acc_score#return float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show total perdict score and recall score\n",
    "def errortype_totalscore(pre_errortype,ans_errortype):\n",
    "    #initial para\n",
    "    pre_total = 0.0\n",
    "    rec_total = 0.0\n",
    "    acc_total = 0.0\n",
    "    #get each score then get total and avg score\n",
    "    for i in range(len(pre_errortype)):\n",
    "        #print(\"Sample: \", i)\n",
    "        pre, rec, acc = errortype_score(pre_errortype[i], ans_errortype[i])\n",
    "        pre_total = pre_total + pre\n",
    "        rec_total = rec_total + rec\n",
    "        acc_total = acc_total + acc\n",
    "    print(\"pre_total: \", pre_total)\n",
    "    print(\"rec_total: \", rec_total)\n",
    "    print(\"acc_total: \", acc_total)\n",
    "    pre_avg_score = pre_total/len(pre_errortype)\n",
    "    rec_avg_socre = rec_total/len(ans_errortype)\n",
    "    acc_avg_score = acc_total/len(ans_errortype)\n",
    "    print(\"avg_pre: \", pre_avg_score)#pre_total/len(pre_errortype))\n",
    "    print(\"avg_rec: \", rec_avg_socre)#rec_total/len(ans_errortype))\n",
    "    print(\"avg_acc: \", acc_avg_score)#acc_total/len(ans_errortype))\n",
    "    return pre_avg_score, rec_avg_socre, acc_avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_line(pre_begin_index, pre_end_index, ans_begin_index, ans_end_index):\n",
    "    #Note:\n",
    "            #use softmax: [0.....150] total == 1\n",
    "            #begin: index[0]\n",
    "            #end: index[0]\n",
    "        \n",
    "    #get [value] first and get [value] again to get value\n",
    "    pre_begin_line = pre_begin_index[0]\n",
    "    pre_begin_line = pre_begin_line[0]\n",
    "    pre_end_line = pre_end_index[0]\n",
    "    pre_end_line = pre_end_line[0]\n",
    "    ans_begin_line = ans_begin_index[0]\n",
    "    ans_begin_line = ans_begin_line[0]\n",
    "    ans_end_line = ans_end_index[0]\n",
    "    ans_end_line = ans_end_line[0]\n",
    "    \n",
    "    return pre_begin_line, pre_end_line, ans_begin_line, ans_end_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_adjustment(line):\n",
    "    if(line > 1):\n",
    "        line = line-1\n",
    "        return line\n",
    "    else:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each pre and ans lineblocks and make them to pre begin/end ans begin/end array\n",
    "def make_lineblock(begin, end):\n",
    "    if (end < begin):\n",
    "        block = [value for value in range(end, begin+1)]\n",
    "        return block\n",
    "    else:\n",
    "        block = [value for value in range(begin, end+1)]\n",
    "        return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref = \"https://www.geeksforgeeks.org/python-intersection-two-lists/\"\n",
    "def lineblock_intersect(pre_block, ans_block):\n",
    "    inter = [value for value in pre_block if value in ans_block]\n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return index with int datatype \n",
    "def get_block_index(pre_begins, pre_ends, ans_begins, ans_ends):\n",
    "    pre_begin = int(pre_begins)\n",
    "    pre_end = int(pre_ends)\n",
    "    ans_begin = int(ans_begins)\n",
    "    ans_end = int(ans_ends)\n",
    "    #print(pre_begin, pre_end, ans_begin, ans_end)\n",
    "    return pre_begin, pre_end, ans_begin, ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new pre array contains only 1 in array \n",
    "def linefilter(line_block):\n",
    "    #print(\"org error type: \", pre_errortype)\n",
    "    new_line = [value for value in line_block if value == 1]\n",
    "    #print(\"new pre errortype: \", new_pre_errortype)\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Note:\n",
    "    #Predict score:\n",
    "        # score = inter of ans and pre divide by len of pre\n",
    "    #Recall score:\n",
    "        # score = inter of ans and pre divide by len of ans\n",
    "    #                 b   s   l\n",
    "    #line sturcture [84][36][150]\n",
    "def errorline_totalscore(pre_errorline, ans_errorline):\n",
    "    #get sample size\n",
    "    sample_size = len(pre_errorline[1])\n",
    "    #get block size\n",
    "    lineblock_size = len(pre_errorline)\n",
    "    total_pre = 0.0\n",
    "    total_rec = 0.0\n",
    "    total_sample_pre = 0.0\n",
    "    total_sample_rec = 0.0\n",
    "    for sample in range(sample_size):\n",
    "        sample_totalline_pre = 0.0\n",
    "        sample_totalline_rec = 0.0\n",
    "        for lineblock in range(0, lineblock_size, 2):\n",
    "            #give index blocks value\n",
    "            pre_begin = pre_errorline[lineblock][sample]\n",
    "            pre_end = pre_errorline[lineblock+1][sample]\n",
    "            ans_begin = ans_errorline[lineblock][sample]\n",
    "            ans_end = ans_errorline[lineblock+1][sample]\n",
    "            ''' <-------dust switch\n",
    "            #show error block value\n",
    "            print(\"pre_errorline begin[lineblock][sample]: \", lineblock, \" \", sample)\n",
    "            print(pre_begin)\n",
    "            print(\"pre_errorline begin[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "            print(pre_end)\n",
    "            print(\"ans_errorline begin [lineblock][sample]: \", lineblock, \" \", sample)\n",
    "            print(ans_begin)\n",
    "            print(\"ans_errorline end[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "            print(ans_end)\n",
    "            #'''\n",
    "            #get block start/end index\n",
    "            pre_begin_index = np.where(pre_begin == 1)\n",
    "            pre_end_index = np.where(pre_end == 1)\n",
    "            ans_begin_index = np.where(ans_begin == 1)\n",
    "            ans_end_index = np.where(ans_end == 1)\n",
    "            ''' <-------dust switch\n",
    "            #show start/end index\n",
    "            print(\"pre_begin_index: \", pre_begin_index)\n",
    "            print(\"pre_begin_index type: \", type(pre_begin_index))\n",
    "            print(\"pre_end_index: \", pre_end_index)\n",
    "            print(\"ans_begin_index: \", ans_begin_index)\n",
    "            print(\"ans_end_index: \", ans_end_index)\n",
    "            #'''\n",
    "            #give start value\n",
    "            #Note:\n",
    "                #use softmax: [0.....150] total == 1\n",
    "                #begin: index[0]\n",
    "                #end: index[0]\n",
    "\n",
    "            #get [value] first and get [value] again to get value\n",
    "            pre_begin_line, pre_end_line, ans_begin_line, ans_end_line = get_start_end_line(\n",
    "                                                                                            pre_begin_index, \n",
    "                                                                                            pre_end_index, \n",
    "                                                                                            ans_begin_index, \n",
    "                                                                                            ans_end_index\n",
    "                                                                                            )\n",
    "            ''' <-------dust switch\n",
    "            #show start/end index\n",
    "            print(\"pre_begin_line: \", pre_begin_line)\n",
    "            print(\"pre_end_line: \", pre_end_line)\n",
    "            print(\"ans_begin_line: \", ans_begin_line)\n",
    "            print(\"ans_end_line: \", ans_end_line)\n",
    "            #'''\n",
    "            ''' <-------dust switch\n",
    "            #'''\n",
    "            #make line downgrade for 1\n",
    "            #Note:\n",
    "            #before:  0 -1  1  2  3  4\n",
    "            #after:   0  1 <-----abs  \n",
    "                        #1  2  3  4\n",
    "                    #[0, 1, 2, 3, 4, 5]\n",
    "            pre_begin_line = line_adjustment(pre_begin_line)\n",
    "            pre_end_line = line_adjustment(pre_end_line)\n",
    "            ans_begin_line = line_adjustment(ans_begin_line)\n",
    "            ans_end_line = line_adjustment(ans_end_line)\n",
    "            ''' <-------dust switch\n",
    "            print(\"pre_begin_line: \", pre_begin_line)\n",
    "            print(\"pre_end_line: \", pre_end_line)\n",
    "            print(\"ans_begin_line: \",ans_begin_line)\n",
    "            print(\"ans_end_line: \", ans_end_line)\n",
    "            #'''\n",
    "            pre_lineblock = make_lineblock(pre_begin_line, pre_end_line)\n",
    "            ans_lineblock = make_lineblock(ans_begin_line, ans_end_line)\n",
    "            #print(\"pre_lineblock: \", pre_lineblock)\n",
    "            #print(\"ans_lineblock: \", ans_lineblock)\n",
    "            inter = lineblock_intersect(pre_lineblock, ans_lineblock)\n",
    "            #print(\"inter: \", inter)\n",
    "            inter_length = len(inter)\n",
    "            pre_length = len(pre_lineblock)\n",
    "            ans_length = len(ans_lineblock)\n",
    "            #print(\"inter length: \", inter_length)\n",
    "            #print(\"pre_length: \", pre_length)\n",
    "            #print(\"ans_length: \", ans_length)\n",
    "            #count pre_score\n",
    "            if (inter_length == 0 and pre_length == 0):\n",
    "                pre_score = 1\n",
    "            elif (pre_length == 0):\n",
    "                pre_score = 0\n",
    "            else :\n",
    "                pre_score = inter_length/pre_length\n",
    "            #count rec_score\n",
    "            if (inter_length == 0 and ans_length == 0):\n",
    "                rec_score = 1  \n",
    "                #\n",
    "            elif (ans_length == 0):\n",
    "                rec_score = 0\n",
    "            else:\n",
    "                rec_score = inter_length/ans_length\n",
    "            #print(\"Sample pre_score: \", pre_score)\n",
    "            #print(\"Sample rec_score: \", rec_score)\n",
    "            #cal each sample lineblock score\n",
    "            sample_totalline_pre += pre_score\n",
    "            sample_totalline_rec += rec_score\n",
    "            #print(\"Sample total_pre: \", sample_pre)\n",
    "            #print(\"Sample total_rec: \", sample_rec)\n",
    "        #cal total sample score\n",
    "        total_sample_pre += sample_totalline_pre/len(lineblock_size/2)\n",
    "        total_sample_rec += sample_totalline_rec/len(lineblock_size/2)\n",
    "    avg_pre = total_sample_pre/sample_size\n",
    "    avg_rec = total_sample_rec/sample_size\n",
    "    return avg_pre, avg_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_type_F_score(pre_score, rec_score):\n",
    "    f_one = (2*pre_score*rec_score)/(pre_score + rec_score)\n",
    "    f_two = (3*pre_score*rec_score)/((2*pre_score) + rec_score)\n",
    "    f_pointfive = (3*pre_score*rec_score)/(pre_score + (2*rec_score))\n",
    "    print(\"F_one: \", f_one)\n",
    "    print(\"F_two: \", f_two)\n",
    "    print(\"F_pointfive: \", f_pointfive)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_line_F_score(pre_score, rec_score):\n",
    "    f_one = (2*pre_score*rec_score)/(pre_score + rec_score)\n",
    "    f_two = (3*pre_score*rec_score)/((2*pre_score) + rec_score)\n",
    "    f_pointfive = (3*pre_score*rec_score)/(pre_score + (2*rec_score))\n",
    "    print(\"F_one: \", f_one)\n",
    "    print(\"F_two: \", f_two)\n",
    "    print(\"F_pointfive: \", f_pointfive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_errortype, ans_errortype, pre_errorline, ans_errorline = loadmodel(model_name, x_test_model, y_test_mdodel1, y_test_mdodel2)\n",
    "#print(len(pre_errortype))\n",
    "\n",
    "'''\n",
    "work : \n",
    "    fix 3 dim prob: \n",
    "        out2 only show two dim\n",
    "        score need three dim: out2: 2D, y_test_loaded_1: 3D\n",
    "    issue:\n",
    "        y_test_loaded_1: dim is not expected...\n",
    "    ==================================================================================\n",
    "    expect :\n",
    "    y_test_loaded_0 : [sample = 36][error type = 36] correct v\n",
    "    out1 :            [sample = 36][error type = 36] correct v\n",
    "    y_test_loaded_1 : [sample = 36][block = 84][begin and end :line total = 150]\n",
    "    out2 :            [sample = 36][block = 84][begin and end :line total = 150]\n",
    "    ==================================================================================\n",
    "    real : \n",
    "            out2 shape            = [36][150]    ------>dim should be [36][84][150]\n",
    "            y_test_loaded_1 shape = [84][36][150]------>dim should be [36][84][150]\n",
    "    ==================================================================================\n",
    "    ref in line work space\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all sample error type score\n",
    "avg_pre, avg_rec, avg_acc = errortype_totalscore(pre_errortype, ans_errortype)\n",
    "error_type_F_score(avg_pre, avg_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all sample error line score\n",
    "avg_pre, avg_rec = errorline_totalscore(pre_errorline, ans_errorline)\n",
    "error_line_F_score(avg_pre, avg_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work space...\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "#len(ans_errortype)\n",
    "sample_size = len(pre_errorline[1])\n",
    "lineblock_size = len(pre_errorline)\n",
    "'''\n",
    "Note:\n",
    "    pre_errorline[lineblock][sample]\n",
    "    ans_errorline[lineblock][sample]\n",
    "Para:\n",
    "'''\n",
    "\n",
    "\n",
    "#''' <-------dust switch\n",
    "print(\"sample size: \", sample_size)\n",
    "print(\"lineblock size: \", lineblock_size)\n",
    "#'''\n",
    "total_pre_score = 0.0\n",
    "total_rec_score = 0.0\n",
    "for sample in range(sample_size):\n",
    "    for lineblock in range(0, lineblock_size, 2):\n",
    "        #give index blocks value\n",
    "        pre_begin = pre_errorline[lineblock][sample]\n",
    "        pre_end = pre_errorline[lineblock+1][sample]\n",
    "        ans_begin = ans_errorline[lineblock][sample]\n",
    "        ans_end = ans_errorline[lineblock+1][sample]\n",
    "        \n",
    "        ''' <-------dust switch\n",
    "        #show error block value\n",
    "        print(\"pre_errorline begin[lineblock][sample]: \", lineblock, \" \", sample)\n",
    "        print(pre_begin)\n",
    "        print(\"pre_errorline begin[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "        print(pre_end)\n",
    "        print(\"ans_errorline begin [lineblock][sample]: \", lineblock, \" \", sample)\n",
    "        print(ans_begin)\n",
    "        print(\"ans_errorline end[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "        print(ans_end)\n",
    "        #'''\n",
    "        #get block start/end index\n",
    "        pre_begin_index = np.where(pre_begin == 1)\n",
    "        pre_end_index = np.where(pre_end == 1)\n",
    "        ans_begin_index = np.where(ans_begin == 1)\n",
    "        ans_end_index = np.where(ans_end == 1)\n",
    "        \n",
    "        ''' <-------dust switch\n",
    "        #show start/end index\n",
    "        print(\"pre_begin_index: \", pre_begin_index)\n",
    "        print(\"pre_begin_index type: \", type(pre_begin_index))\n",
    "        print(\"pre_end_index: \", pre_end_index)\n",
    "        print(\"ans_begin_index: \", ans_begin_index)\n",
    "        print(\"ans_end_index: \", ans_end_index)\n",
    "        #'''\n",
    "        \n",
    "        #give start value\n",
    "        #Note:\n",
    "            #use softmax: [0.....150] total == 1\n",
    "            #begin: index[0]\n",
    "            #end: index[0]\n",
    "        \n",
    "        #get [value] first and get [value] again to get value\n",
    "        #pre_begin_line, pre_end_line, ans_begin_line, ans_end_line = get_start_end_line(pre_begin_index, pre_end_index, ans_begin_index, ans_end_index)\n",
    "        \n",
    "        pre_begin_line = pre_begin_index[0]\n",
    "        #pre_begin_line = pre_begin_line[0]\n",
    "        pre_end_line = pre_end_index[0]\n",
    "        #pre_end_line = pre_end_line[0]\n",
    "        ans_begin_line = ans_begin_index[0]\n",
    "        ans_begin_line = ans_begin_line[0]\n",
    "        ans_end_line = ans_end_index[0]\n",
    "        ans_end_line = ans_end_line[0]\n",
    "        \n",
    "        #''' <-------dust switch\n",
    "        #show start/end index\n",
    "        print(\"pre_begin_line: \", pre_begin_line)\n",
    "        print(\"pre_end_line: \", pre_end_line)\n",
    "        print(\"ans_begin_line: \", ans_begin_line)\n",
    "        print(\"ans_end_line: \", ans_end_line)\n",
    "        #'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        #make intersection\n",
    "        \n",
    "        ''' <-------dust switch\n",
    "        print(\"inter_begin: \", inter_begin)\n",
    "        print(\"inter_all_begin: \", inter_all_begin)\n",
    "        #'''\n",
    "        \n",
    "        \n",
    "'''\n",
    "final decide: find index and build ----> [1 2 3 4 5 6] vs [2 3 4 5 6 7] to make intercestion\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note:\n",
    "    #Predict score:\n",
    "        # score = inter of ans and pre divide by len of pre\n",
    "    #Recall score:\n",
    "        # score = inter of ans and pre divide by len of ans\n",
    "    #Accuracy score:\n",
    "        # score = inter_two of ans and pre divide by original len of pre\n",
    "\n",
    "#case 1:  n  n vs  n  n \n",
    "#case 2:  n  n vs -1 -1\n",
    "#case 3: -1 -1 vs -1 -1\n",
    "#case 4: -1  n vs  n  n\n",
    "#case 5: -1  n vs -1  n\n",
    "pre_begin_line = 5\n",
    "pre_end_line = 7\n",
    "ans_begin_line = 3\n",
    "ans_end_line = 2\n",
    "pre_begin_line = line_adjustment(pre_begin_line)\n",
    "pre_end_line = line_adjustment(pre_end_line)\n",
    "ans_begin_line = line_adjustment(ans_begin_line)\n",
    "ans_end_line = line_adjustment(ans_end_line)\n",
    "\n",
    "print(\"pre_begin_line: \", pre_begin_line)\n",
    "print(\"pre_end_line: \", pre_end_line)\n",
    "print(\"ans_begin_line: \",ans_begin_line)\n",
    "print(\"ans_end_line: \", ans_end_line)\n",
    "pre_lineblock = make_lineblock(pre_begin_line, pre_end_line)\n",
    "ans_lineblock = make_lineblock(ans_begin_line, ans_end_line)\n",
    "print(\"pre_lineblock: \", pre_lineblock)\n",
    "print(\"ans_lineblock: \", ans_lineblock)\n",
    "inter = lineblock_intersect(pre_lineblock, ans_lineblock)\n",
    "print(\"inter: \", inter)\n",
    "inter_length = len(inter)\n",
    "pre_length = len(pre_lineblock)\n",
    "ans_length = len(ans_lineblock)\n",
    "print(\"inter length: \", inter_length)\n",
    "print(\"pre_length: \", pre_length)\n",
    "print(\"ans_length: \", ans_length)\n",
    "if (inter_length == 0 and pre_length == 0):\n",
    "    pre_score = 1\n",
    "    # issue 7.12ver \n",
    "elif (pre_length == 0):\n",
    "    pre_score = 0\n",
    "else :\n",
    "    pre_score = inter_length/pre_length\n",
    "#    \n",
    "if (inter_length == 0 and ans_length == 0):\n",
    "    rec_score = 1  \n",
    "    #\n",
    "elif (ans_length == 0):\n",
    "    rec_score = 0\n",
    "else:\n",
    "    rec_score = inter_length/ans_length\n",
    "#print(\"Sample pre_score: \", pre_score)\n",
    "total_pre += pre_score\n",
    "#print(\"Sample rec_score: \", rec_score)\n",
    "total_rec += rec_score\n",
    "#print(\"Sample total_pre: \", total_pre)\n",
    "#print(\"Sample total_rec: \", total_rec)\n",
    "avg_pre = total_pre/len(pre_begins)\n",
    "avg_rec = total_rec/len(ans_begins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "Input_Path = \"Trianing\\InputTxt\\Split-500\"\n",
    "Input_Path = (glob.glob(Input_Path+\"/**/*.txt\"))\n",
    "for i in range(10):\n",
    "    print(Input_Path[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y-JMBpEFcJWh",
    "psq7nyP0ca_Z"
   ],
   "name": "Main_Colab擴增版.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
