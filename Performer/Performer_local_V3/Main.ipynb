{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d94ffe56",
    "outputId": "1249cd6b-bff9-44bc-f32e-212835586c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\W.R_Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performer as tfr\n",
    "import nltk\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "9b54d338",
    "outputId": "805aecc7-247e-4761-a0f1-f2ac4bb1e054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef solve_cudnn_error():\\n    import tensorflow as tf\\n    gpus = tf.config.experimental.list_physical_devices(\\'GPU\\')\\n    if gpus:\\n        try:\\n            # Currently, memory growth needs to be the same across GPUs\\n            for gpu in gpus:\\n                tf.config.experimental.set_memory_growth(gpu, True)\\n            logical_gpus = tf.config.experimental.list_logical_devices(\\'GPU\\')\\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n        except RuntimeError as e:\\n            # Memory growth must be set before GPUs have been initialized\\n            print(e)\\n            \\nsolve_cudnn_error()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def solve_cudnn_error():\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "            \n",
    "solve_cudnn_error()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "22361e8f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(file_name):\n",
    "    errlist=[]\n",
    "    LBlist=[]\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "    #讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows: \n",
    "            RL=list(row.values())\n",
    "            #print(\"RL[0]: \", type(RL[0]), \"RL[1]: \", type(RL[1]))\n",
    "            RL[1:] = list(map(int, RL[1:]))\n",
    "            errs=RL[1:37]\n",
    "            LB=RL[37:]\n",
    "            errlist.append(errs)\n",
    "            LBlist.append(LB)\n",
    "    return errlist,LBlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b92cf71a"
   },
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1b88909b"
   },
   "outputs": [],
   "source": [
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "abb41dde"
   },
   "outputs": [],
   "source": [
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0db5e0b1"
   },
   "outputs": [],
   "source": [
    "def parseSentence(x):\t\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        #print(ord(x[i]))\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"ASCII\":\n",
    "            if inp==\"E\":\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  # nltk.word_tokenize(chrs) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e88de533"
   },
   "outputs": [],
   "source": [
    "def readcode(fname):\n",
    "    with open(fname,encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f1JYt9ELGe5Z"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plotTrainingLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def listdir_fullpath(d):\n",
    "    return [f for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ec120d95"
   },
   "outputs": [],
   "source": [
    "#save model for training\n",
    "class TestTranslate(unittest.TestCase):\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<CR>': 5,\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "    \n",
    "    def test_translate(self):\n",
    "        #print(\"i am here: \" )\n",
    "        #source_file=[]\n",
    "        #Set Para\n",
    "        max_javaline_length = 160 #Max number of lines\n",
    "        #set path\n",
    "        Input_Path = \"Trianing\\InputTxt\\Split-500\"\n",
    "        Output_Path = \"Trianing\\InputCSV\\Split-500\"\n",
    "        model_for_training_path = \"Model-for-training\\Split-500\"\n",
    "        Trained_model_Path = \"Trained_models\\Spilt-500\"\n",
    "        #get all txt file in input path\n",
    "        in_path = glob.glob(Input_Path+\"/**/**/*.txt\")\n",
    "        cases = listdir_fullpath(Input_Path)\n",
    "        source_max_len = 0\n",
    "        target_max_len = 0\n",
    "        token_num = 0\n",
    "        all_sample_num = 16644 #all sample number\n",
    "        block_num = 16644 #sample num e.g 10000 sample have 10*1000\n",
    "        \n",
    "        #''' <----dust switch, if you need to create npy model for traing open this\n",
    "        self.sl = 0\n",
    "        \n",
    "        import math\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version \n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"First loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            for i in range(dirs):\n",
    "                    Input_fullpath.append(in_path[loop*block_num + i])                    \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "                    #print(\"source_tokens length: \", len(source_tokens))\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            #get csv file     \n",
    "            out_path = Output_Path + \"/\" + \"test\" + str(loop)+\".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            \n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err,lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            dd = np.asarray(target_errors)\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]     \n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            \n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            \n",
    "            #output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens] \n",
    "            \n",
    "            self.sl = max(list(map(len, encode_tokens))+[self.sl])\n",
    "            source_max_len = self.sl\n",
    "\n",
    "        #padding here\n",
    "        print(\"source_max_len:\", source_max_len)\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version\n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"Second loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])\n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "\n",
    "            out_path = Output_Path + \"/\" + \"test\"+ str(loop)+ \".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err, lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "            #if len(source_tokens)>max_files: break\n",
    "            dd = np.asarray(target_errors)\n",
    "            #print(\"AAAA: \", dd.shape)\n",
    "            #print(\"aaaa: \" , type(target_errors[0][0]))\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]     \n",
    "            #print(\"source_token legth: \" , len(source_tokens))\n",
    "            #print(\"YYYY: \" , type(target_errors[0][0]))\n",
    "            #print(\"ZZZZ: \" , len(target_LB))\n",
    "            #print(\"ZZZZ len(target_LB[0]): \" , len(target_LB[0]))\n",
    "            #print(\"XXXX2: \" , len(source_tokens))\n",
    "            # Generate dictionaries        \n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "\n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            #print(\"encode_tokens1: \", encode_tokens)\n",
    "            encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_tokens2: \", encode_tokens)\n",
    "            encode_input = [list(map(lambda x: self.source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_input1: \", encode_input)\n",
    "            token_num = len(self.source_token_dict)\n",
    "            #print(\"token num: \", token_num)\n",
    "            #print(token_num)\n",
    "            #print(type(token_num))\n",
    "            #define save path and save dict  \n",
    "            dict_name = \"source_token_dict.pickle\"\n",
    "            saveDictionary(self.source_token_dict, model_for_training_path + \"/\" + dict_name)\n",
    "\n",
    "            #print(\"x.shape\", np.asarray(encode_input).shape)  #x.shape (2,  9)\n",
    "            \n",
    "            #x=[np.array(encode_input * 1)]\n",
    "            #y=[np.array(target_errors * 1),np.array(target_LB * 1)]\n",
    "\n",
    "            #print(\"x.shape\", np.asarray(x).shape)  #x.shape (2, 2048, 9)\n",
    "\n",
    "            ####  Split the data set into train and test_model\n",
    "            x = np.asarray(encode_input)\n",
    "            y = list(zip(np.asarray(target_errors), np.asarray(target_LB)))\n",
    "\n",
    "            #print(\"x.shape: \", x.shape)\n",
    "            #print(\"y length: \", len(y))\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=42)\n",
    "\n",
    "            y_train = list(zip(*y_train))  \n",
    "            y_train[0] = np.asarray(y_train[0])\n",
    "            y_train[1] = np.asarray(y_train[1])\n",
    "\n",
    "            y_train[1] = to_categorical(y_train[1], num_classes=max_javaline_length) #將類別向量轉換為二進制矩陣\n",
    "            #y_train = list(zip(y_train[0], y_train[1]))\n",
    "\n",
    "            #print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:\" , y_train[0].shape)\n",
    "            #print(\"y_train[1].shape\", y_train[1].shape)\n",
    "            #print(\"???????????: y_train.length \", len(y_train))\n",
    "            #print(\"???????????: y_train.[1] type \", type(y_train[1]))\n",
    "            y_train[1] = np.split(y_train[1], indices_or_sections=len(target_LB[0]), axis=1)\n",
    "            y_train[1] = [np.squeeze(elm, axis = 1) for elm in y_train[1]]\n",
    "            #print(\"after change->len(y_train[1].shape)\", len(y_train[1]) )\n",
    "\n",
    "            y_test = list(zip(*y_test))\n",
    "            y_test[0] = np.asarray(y_test[0])\n",
    "            y_test[1] = np.asarray(y_test[1])\n",
    "            y_test[1] = to_categorical(y_test[1], num_classes = max_javaline_length) \n",
    "            #y_test = list(zip(y_test[0], y_test[1])) \n",
    "            y_test[1] = np.split(y_test[1], indices_or_sections=len(target_LB[0]), axis=1) \n",
    "            y_test[1] = [np.squeeze(elm, axis = 1) for elm in y_test[1]]           \n",
    "            \n",
    "            #save model for training \n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            \n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "        print(\"Training model save successful...\")    \n",
    "        #'''\n",
    "\n",
    "        #start training\n",
    "        import DataGenerator2_2 as DG2\n",
    "        import DataBuffer as db\n",
    "        from random import randrange\n",
    "        #Set driver path\n",
    "        source_max_len = 920 #default : 2889\n",
    "        line_block_num = 360 #lbNum\n",
    "        source_token_dict_name = \"source_token_dict.pickle\"\n",
    "        #load source_token_dict\n",
    "        source_token_dict = loadDictionary(model_for_training_path + \"/\" + source_token_dict_name)\n",
    "        #Set model para    \n",
    "        model = tfr.get_model(max_input_len=(source_max_len),\n",
    "                              max_javaline_length=160,\n",
    "                              errNum=36,\n",
    "                              lbNum=line_block_num, #lbNum=len(target_LB[0]), #160\n",
    "                              token_num=len(source_token_dict),\n",
    "                              embed_dim=256, #32, try 32 or 64\n",
    "                              encoder_num=4, #2 max = 6\n",
    "                              head_num=4,#4\n",
    "                              hidden_dim=128, #128\n",
    "                              dropout_rate=0.05 #0.05\n",
    "                             )\n",
    "        #Set losses\n",
    "        losses = {\"error_feed_forward_output1\": \"binary_crossentropy\"}\n",
    "        #error type weight\n",
    "        lossWeights = {\"error_feed_forward_output1\": 1.0}\n",
    "        #error line weight\n",
    "        for i in range(line_block_num):\n",
    "            name = \"LNout\" + str(i)\n",
    "            losses[name] = \"categorical_crossentropy\"\n",
    "            lossWeights[name] = 1 #error_feed_forward_output2[] weight\n",
    "        \n",
    "        #set complie para\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0000001), loss=losses, loss_weights=lossWeights)\n",
    "        \n",
    "        #for output\n",
    "        #for x\n",
    "        input_buffer_params = { \n",
    "            \"data_path\": model_for_training_path,\n",
    "            \"data_number\": 14979, #or 14979????\n",
    "            \"data_type\": int,\n",
    "            \"block_size\": 14979 #???\n",
    "            }\n",
    "        \n",
    "        #for input\n",
    "        #for y\n",
    "        output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path, model_for_training_path],\n",
    "            \"data_number\": [14979, 14979],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [14979, 14979] #???\n",
    "            }\n",
    "        \n",
    "        #Create Generators\n",
    "        print(\"Creating generator...\")\n",
    "        training_generator = DG2.DataGenerator2_2(input_buffer_params,\n",
    "                                                  output_buffer_params,\n",
    "                                                  [list(range(14979)), list(range(14979))] #or 14979???\n",
    "                                                )\n",
    "        #Start training\n",
    "        print(\"Strat training...\")\n",
    "        history = model.fit_generator(generator = training_generator,\n",
    "                                      epochs = 2, #100 200 500\n",
    "                                      verbose = 2 #set visibility\n",
    "                                     )\n",
    "        \n",
    "        #show loss grapgh\n",
    "        plotTrainingLoss(history)\n",
    "        print(\"Model training completed...\")\n",
    "        #save model\n",
    "        print(\"Saving model...\")\n",
    "        model.save(Trained_model_Path + \"/\" + \"test_model1.h5\")\n",
    "        print(\"Model saving completed...\")\n",
    "        \n",
    "        '''\n",
    "        model, source_token_dict = load(\"test_model1.h5\")\n",
    "        \n",
    "        out1, out2 = tfr.decode(\n",
    "                model,\n",
    "                #encode_input,\n",
    "                x_test_loaded,max_len=source_max_len\n",
    "            )\n",
    "        '''\n",
    "        \n",
    "    def getsource_max_lan(self):\n",
    "        return self.sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n           #Build & fit model\\n           #Set model para    \\n           model = tfr.get_model(\\n               max_input_len= (source_max_len),\\n               max_javaline_length=max_javaline_length,\\n               errNum=36,\\n               lbNum=len(target_LB[0]),\\n               token_num=len(source_token_dict),\\n               embed_dim=32, #32,\\n               encoder_num=4, #2 max = 6\\n               head_num=4,#4\\n               hidden_dim=64, #128\\n               dropout_rate=0.05 #0.05\\n           )\\n           #\\'LNout0\\'\\n       \\n           losses = {    \\n               \"error_feed_forward_output1\": \"binary_crossentropy\"}\\n               #\"error_feed_forward_output2\": \"categorical_crossentropy\",            \\n       \\n       \\n           lossWeights = {\"error_feed_forward_output1\": 1.0  }#\"error_feed_forward_output2\": 1.0}\\n       \\n           for i in range(len(target_LB[0])):  #列出 len(target_LB[0]) 組網路層\\n               name = \"LNout\"+str(i)\\n               losses[name] = \"categorical_crossentropy\"\\n               lossWeights[name] = 1 #error_feed_forward_output2[] weight\\n\\n           \\n           model.compile(optimizer=Adam(learning_rate=0.001), loss=losses, loss_weights=lossWeights)\\n       \\n           history=model.fit(\\n               x=x_train,\\n               y= [y_train[0]] + y_train[1],\\n               epochs=1, #100 200 500\\n               batch_size=32,\\n           )\\n           plotTrainingLoss(history)\\n           model.save(\"test_model1.h5\")\\n           saveDictionary(source_token_dict, \"source_token_dict.pickle\")\\n       \\n           model, source_token_dict = load(\"test_model1.h5\")\\n\\n           # Predict\\n       \\n           x_train_loaded = loadTestTrainData(\"train_models/x_train_500.npy\") \\n           y_train_loaded_0 =   loadTestTrainData(\"train_models/y_train0_500.npy\")\\n           y_train_loaded_1 =   loadTestTrainData(\"train_models/y_train1_500.npy\") \\n\\n           x_test_loaded =  loadTestTrainData(\"test_models/x_test_500.npy\") \\n           y_test_loaded_0 =   loadTestTrainData(\"test_models/y_test0_500.npy\")\\n           y_test_loaded_1 =   loadTestTrainData(\"test_models/y_test1_500.npy\") \\n       \\n       \\n           out1, out2 = tfr.decode(\\n               model,\\n               #encode_input,\\n               x_test_loaded,max_len=source_max_len\\n           )\\n           print(\"out1: \", out1)\\n           #print(\"target_errors: \",target_errors) \\n           print(\"target_errors: \", y_test_loaded_0)        \\n           print(\"out2: \", out2)\\n           #print(\"target_LB: \", target_LB)\\n           print(\"target_LB: \", y_test_loaded_1)\\n       \\n           #model.summary()\\n       \\n           print(\"type(y_train_loaded_0)\",type(y_train_loaded_0))\\n           print(\"type(y_train_loaded_1)\",type(y_train_loaded_1))\\n           print(\"y_train_loaded[0].shape\",y_train_loaded_0.shape)\\n\\n           print(\"len(y_train_loaded[1])\",len(y_train_loaded_1))\\n           print(\"y_train_loaded[1][0].shape\",y_train_loaded_1[0].shape)\\n           train_loss = model.evaluate(x_train_loaded, [y_train_loaded_0] + list(y_train_loaded_1) )\\n           print(\"train_loss\",train_loss)\\n       \\n           test_loss = model.evaluate(x_test_loaded, [y_test_loaded_0] + list(y_test_loaded_1) )\\n           print(\"test_loss\",test_loss)\\n           '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " '''\n",
    "            #Build & fit model\n",
    "            #Set model para    \n",
    "            model = tfr.get_model(\n",
    "                max_input_len= (source_max_len),\n",
    "                max_javaline_length=max_javaline_length,\n",
    "                errNum=36,\n",
    "                lbNum=len(target_LB[0]),\n",
    "                token_num=len(source_token_dict),\n",
    "                embed_dim=32, #32,\n",
    "                encoder_num=4, #2 max = 6\n",
    "                head_num=4,#4\n",
    "                hidden_dim=64, #128\n",
    "                dropout_rate=0.05 #0.05\n",
    "            )\n",
    "            #'LNout0'\n",
    "        \n",
    "            losses = {    \n",
    "                \"error_feed_forward_output1\": \"binary_crossentropy\"}\n",
    "                #\"error_feed_forward_output2\": \"categorical_crossentropy\",            \n",
    "        \n",
    "        \n",
    "            lossWeights = {\"error_feed_forward_output1\": 1.0  }#\"error_feed_forward_output2\": 1.0}\n",
    "        \n",
    "            for i in range(len(target_LB[0])):  #列出 len(target_LB[0]) 組網路層\n",
    "                name = \"LNout\"+str(i)\n",
    "                losses[name] = \"categorical_crossentropy\"\n",
    "                lossWeights[name] = 1 #error_feed_forward_output2[] weight\n",
    "\n",
    "            \n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss=losses, loss_weights=lossWeights)\n",
    "        \n",
    "            history=model.fit(\n",
    "                x=x_train,\n",
    "                y= [y_train[0]] + y_train[1],\n",
    "                epochs=1, #100 200 500\n",
    "                batch_size=32,\n",
    "            )\n",
    "            plotTrainingLoss(history)\n",
    "            model.save(\"test_model1.h5\")\n",
    "            saveDictionary(source_token_dict, \"source_token_dict.pickle\")\n",
    "        \n",
    "            model, source_token_dict = load(\"test_model1.h5\")\n",
    "\n",
    "            # Predict\n",
    "        \n",
    "            x_train_loaded = loadTestTrainData(\"train_models/x_train_500.npy\") \n",
    "            y_train_loaded_0 =   loadTestTrainData(\"train_models/y_train0_500.npy\")\n",
    "            y_train_loaded_1 =   loadTestTrainData(\"train_models/y_train1_500.npy\") \n",
    "\n",
    "            x_test_loaded =  loadTestTrainData(\"test_models/x_test_500.npy\") \n",
    "            y_test_loaded_0 =   loadTestTrainData(\"test_models/y_test0_500.npy\")\n",
    "            y_test_loaded_1 =   loadTestTrainData(\"test_models/y_test1_500.npy\") \n",
    "        \n",
    "        \n",
    "            out1, out2 = tfr.decode(\n",
    "                model,\n",
    "                #encode_input,\n",
    "                x_test_loaded,max_len=source_max_len\n",
    "            )\n",
    "            print(\"out1: \", out1)\n",
    "            #print(\"target_errors: \",target_errors) \n",
    "            print(\"target_errors: \", y_test_loaded_0)        \n",
    "            print(\"out2: \", out2)\n",
    "            #print(\"target_LB: \", target_LB)\n",
    "            print(\"target_LB: \", y_test_loaded_1)\n",
    "        \n",
    "            #model.summary()\n",
    "        \n",
    "            print(\"type(y_train_loaded_0)\",type(y_train_loaded_0))\n",
    "            print(\"type(y_train_loaded_1)\",type(y_train_loaded_1))\n",
    "            print(\"y_train_loaded[0].shape\",y_train_loaded_0.shape)\n",
    "\n",
    "            print(\"len(y_train_loaded[1])\",len(y_train_loaded_1))\n",
    "            print(\"y_train_loaded[1][0].shape\",y_train_loaded_1[0].shape)\n",
    "            train_loss = model.evaluate(x_train_loaded, [y_train_loaded_0] + list(y_train_loaded_1) )\n",
    "            print(\"train_loss\",train_loss)\n",
    "        \n",
    "            test_loss = model.evaluate(x_test_loaded, [y_test_loaded_0] + list(y_test_loaded_1) )\n",
    "            print(\"test_loss\",test_loss)\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7aa3c824"
   },
   "outputs": [],
   "source": [
    "def saveDictionary(dt, file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"wb\")\n",
    "        pickle.dump(dt, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a6ddfb54"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"rb\")\n",
    "        dt = pickle.load(a_file)\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "20063beb"
   },
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "34dcfe7d"
   },
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ba969ac3"
   },
   "outputs": [],
   "source": [
    "def load(model_name):\n",
    "        import sys\n",
    "        #sys.path.append('/content/drive/MyDrive/Final_Edition_include_model')\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_layer_normalization\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_performer\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\keras_position_wise_feed_forward\")\n",
    "        sys.path.append(\"Performer\\Performer_local_V3\\tensorflow_fast_attention\")\n",
    "\n",
    "        from keras_performer import performer\n",
    "        from tensorflow import keras\n",
    "        from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "        from keras_pos_embd import TrigPosEmbedding\n",
    "        from tensorflow_fast_attention.fast_attention import  Attention, SelfAttention\n",
    "        from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "        co = performer.get_custom_objects()\n",
    "\n",
    "        model = keras.models.load_model(model_name, custom_objects= co)\n",
    "        source_token_dict = loadDictionary(\"source_token_dict.pickle\")\n",
    "       # t = loadDictionary(target_token_dict, 'target_token_dict.pickle')\n",
    "       # t_inv = loadDictionary(target_token_dict_inv, 'target_token_dict_inv.pickle')\n",
    "        return model, source_token_dict,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "083be668",
    "outputId": "bddf7fcb-a6cb-4669-fdff-181d62f1afba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First loop:  0\n",
      "source_max_len: 920\n",
      "Second loop:  0\n",
      "Training model save successful...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Creating generator...\n",
      "Strat training...\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W.R_Chen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "117/117 - 192s - loss: 1828.5807 - error_feed_forward_output1_loss: 0.5503 - LNout0_loss: 5.2262 - LNout1_loss: 5.2027 - LNout2_loss: 5.1712 - LNout3_loss: 5.2670 - LNout4_loss: 5.1854 - LNout5_loss: 5.0454 - LNout6_loss: 4.3985 - LNout7_loss: 4.9081 - LNout8_loss: 4.7834 - LNout9_loss: 5.2940 - LNout10_loss: 5.4431 - LNout11_loss: 4.6538 - LNout12_loss: 5.0334 - LNout13_loss: 6.0463 - LNout14_loss: 4.9005 - LNout15_loss: 5.5683 - LNout16_loss: 5.4818 - LNout17_loss: 4.8109 - LNout18_loss: 5.9980 - LNout19_loss: 4.1844 - LNout20_loss: 4.7874 - LNout21_loss: 4.4728 - LNout22_loss: 5.4635 - LNout23_loss: 5.0281 - LNout24_loss: 4.5135 - LNout25_loss: 4.8998 - LNout26_loss: 5.3174 - LNout27_loss: 4.5807 - LNout28_loss: 4.5258 - LNout29_loss: 5.0710 - LNout30_loss: 5.3580 - LNout31_loss: 4.9711 - LNout32_loss: 5.5417 - LNout33_loss: 4.8999 - LNout34_loss: 4.7875 - LNout35_loss: 4.2306 - LNout36_loss: 5.3078 - LNout37_loss: 4.3393 - LNout38_loss: 4.5377 - LNout39_loss: 5.3275 - LNout40_loss: 5.5113 - LNout41_loss: 5.4250 - LNout42_loss: 5.2337 - LNout43_loss: 5.9162 - LNout44_loss: 5.3897 - LNout45_loss: 5.1853 - LNout46_loss: 5.5546 - LNout47_loss: 4.7496 - LNout48_loss: 4.3416 - LNout49_loss: 4.4894 - LNout50_loss: 4.6863 - LNout51_loss: 4.3453 - LNout52_loss: 6.1573 - LNout53_loss: 4.3974 - LNout54_loss: 5.7718 - LNout55_loss: 5.5743 - LNout56_loss: 5.1764 - LNout57_loss: 4.4549 - LNout58_loss: 5.6284 - LNout59_loss: 5.5271 - LNout60_loss: 4.9674 - LNout61_loss: 5.3567 - LNout62_loss: 5.4403 - LNout63_loss: 4.9664 - LNout64_loss: 4.5838 - LNout65_loss: 5.2380 - LNout66_loss: 5.9475 - LNout67_loss: 5.1595 - LNout68_loss: 4.9940 - LNout69_loss: 6.5862 - LNout70_loss: 5.1045 - LNout71_loss: 4.5429 - LNout72_loss: 5.3043 - LNout73_loss: 5.6641 - LNout74_loss: 4.7543 - LNout75_loss: 4.5390 - LNout76_loss: 3.9872 - LNout77_loss: 4.9983 - LNout78_loss: 5.5820 - LNout79_loss: 5.7446 - LNout80_loss: 4.2086 - LNout81_loss: 5.4261 - LNout82_loss: 6.0165 - LNout83_loss: 4.4564 - LNout84_loss: 5.3446 - LNout85_loss: 5.4233 - LNout86_loss: 4.3835 - LNout87_loss: 4.6859 - LNout88_loss: 4.6592 - LNout89_loss: 4.6311 - LNout90_loss: 4.7404 - LNout91_loss: 4.9528 - LNout92_loss: 4.8725 - LNout93_loss: 4.2140 - LNout94_loss: 5.3753 - LNout95_loss: 5.0187 - LNout96_loss: 5.7917 - LNout97_loss: 4.7213 - LNout98_loss: 5.2008 - LNout99_loss: 5.7210 - LNout100_loss: 4.7650 - LNout101_loss: 5.2109 - LNout102_loss: 5.3552 - LNout103_loss: 5.6496 - LNout104_loss: 5.3850 - LNout105_loss: 4.6025 - LNout106_loss: 6.1986 - LNout107_loss: 5.1226 - LNout108_loss: 5.0163 - LNout109_loss: 5.2026 - LNout110_loss: 5.2842 - LNout111_loss: 4.4112 - LNout112_loss: 5.9699 - LNout113_loss: 4.9110 - LNout114_loss: 5.1436 - LNout115_loss: 5.2968 - LNout116_loss: 4.6055 - LNout117_loss: 4.6991 - LNout118_loss: 4.9322 - LNout119_loss: 6.2041 - LNout120_loss: 5.0810 - LNout121_loss: 4.9567 - LNout122_loss: 5.3623 - LNout123_loss: 4.3464 - LNout124_loss: 5.2576 - LNout125_loss: 5.2633 - LNout126_loss: 5.2881 - LNout127_loss: 5.3844 - LNout128_loss: 4.9069 - LNout129_loss: 5.2215 - LNout130_loss: 5.6186 - LNout131_loss: 5.0579 - LNout132_loss: 5.4912 - LNout133_loss: 4.8331 - LNout134_loss: 5.4539 - LNout135_loss: 5.3242 - LNout136_loss: 5.1086 - LNout137_loss: 5.0210 - LNout138_loss: 5.0582 - LNout139_loss: 4.5025 - LNout140_loss: 4.5680 - LNout141_loss: 6.0289 - LNout142_loss: 5.0472 - LNout143_loss: 4.7946 - LNout144_loss: 5.4903 - LNout145_loss: 4.6085 - LNout146_loss: 4.6605 - LNout147_loss: 4.5874 - LNout148_loss: 4.7581 - LNout149_loss: 5.4478 - LNout150_loss: 5.9787 - LNout151_loss: 4.6822 - LNout152_loss: 5.4238 - LNout153_loss: 4.8270 - LNout154_loss: 3.9971 - LNout155_loss: 5.6720 - LNout156_loss: 5.6556 - LNout157_loss: 4.5257 - LNout158_loss: 6.0546 - LNout159_loss: 4.7839 - LNout160_loss: 4.6968 - LNout161_loss: 5.2064 - LNout162_loss: 5.3532 - LNout163_loss: 4.3602 - LNout164_loss: 5.5815 - LNout165_loss: 5.4383 - LNout166_loss: 5.6756 - LNout167_loss: 5.3278 - LNout168_loss: 4.5051 - LNout169_loss: 4.9057 - LNout170_loss: 5.3274 - LNout171_loss: 4.9627 - LNout172_loss: 4.5419 - LNout173_loss: 5.4334 - LNout174_loss: 5.3987 - LNout175_loss: 5.4764 - LNout176_loss: 6.0782 - LNout177_loss: 5.0391 - LNout178_loss: 4.4618 - LNout179_loss: 5.2171 - LNout180_loss: 4.6218 - LNout181_loss: 4.9904 - LNout182_loss: 5.4539 - LNout183_loss: 4.7986 - LNout184_loss: 6.4834 - LNout185_loss: 4.9675 - LNout186_loss: 5.0864 - LNout187_loss: 4.3240 - LNout188_loss: 4.8156 - LNout189_loss: 5.1746 - LNout190_loss: 5.6779 - LNout191_loss: 5.1796 - LNout192_loss: 5.0469 - LNout193_loss: 4.7469 - LNout194_loss: 3.8074 - LNout195_loss: 5.2821 - LNout196_loss: 5.6367 - LNout197_loss: 4.9105 - LNout198_loss: 5.3788 - LNout199_loss: 5.8581 - LNout200_loss: 4.8016 - LNout201_loss: 5.0397 - LNout202_loss: 4.9667 - LNout203_loss: 4.9792 - LNout204_loss: 4.6892 - LNout205_loss: 4.3928 - LNout206_loss: 5.2402 - LNout207_loss: 4.3403 - LNout208_loss: 5.2018 - LNout209_loss: 5.2044 - LNout210_loss: 5.3662 - LNout211_loss: 6.0470 - LNout212_loss: 5.3601 - LNout213_loss: 4.8728 - LNout214_loss: 5.5824 - LNout215_loss: 5.2662 - LNout216_loss: 5.7384 - LNout217_loss: 5.3453 - LNout218_loss: 4.6971 - LNout219_loss: 4.6717 - LNout220_loss: 4.9622 - LNout221_loss: 5.4216 - LNout222_loss: 4.9627 - LNout223_loss: 5.5272 - LNout224_loss: 5.8950 - LNout225_loss: 5.3047 - LNout226_loss: 5.0273 - LNout227_loss: 5.9053 - LNout228_loss: 5.5528 - LNout229_loss: 4.1838 - LNout230_loss: 4.9715 - LNout231_loss: 5.0563 - LNout232_loss: 4.7550 - LNout233_loss: 4.7884 - LNout234_loss: 4.7566 - LNout235_loss: 4.7049 - LNout236_loss: 4.1162 - LNout237_loss: 4.9213 - LNout238_loss: 5.0823 - LNout239_loss: 4.9554 - LNout240_loss: 4.9556 - LNout241_loss: 6.0093 - LNout242_loss: 5.0889 - LNout243_loss: 4.6441 - LNout244_loss: 4.9065 - LNout245_loss: 5.2783 - LNout246_loss: 4.5543 - LNout247_loss: 4.6596 - LNout248_loss: 5.5135 - LNout249_loss: 5.1161 - LNout250_loss: 4.7312 - LNout251_loss: 5.4188 - LNout252_loss: 4.2697 - LNout253_loss: 4.8381 - LNout254_loss: 5.2557 - LNout255_loss: 4.9999 - LNout256_loss: 5.4088 - LNout257_loss: 5.2327 - LNout258_loss: 5.7516 - LNout259_loss: 5.3763 - LNout260_loss: 4.8219 - LNout261_loss: 4.7769 - LNout262_loss: 5.0813 - LNout263_loss: 5.0598 - LNout264_loss: 5.4490 - LNout265_loss: 4.8957 - LNout266_loss: 4.7278 - LNout267_loss: 5.8095 - LNout268_loss: 5.3444 - LNout269_loss: 4.5925 - LNout270_loss: 4.9437 - LNout271_loss: 4.8430 - LNout272_loss: 4.6194 - LNout273_loss: 4.8230 - LNout274_loss: 4.6970 - LNout275_loss: 4.3721 - LNout276_loss: 4.9479 - LNout277_loss: 4.8546 - LNout278_loss: 5.4314 - LNout279_loss: 4.8941 - LNout280_loss: 4.9550 - LNout281_loss: 4.9784 - LNout282_loss: 5.7580 - LNout283_loss: 4.1615 - LNout284_loss: 5.1375 - LNout285_loss: 5.8696 - LNout286_loss: 5.0923 - LNout287_loss: 4.7734 - LNout288_loss: 4.9596 - LNout289_loss: 5.3901 - LNout290_loss: 4.9491 - LNout291_loss: 5.3941 - LNout292_loss: 5.2825 - LNout293_loss: 5.4045 - LNout294_loss: 5.1927 - LNout295_loss: 5.6560 - LNout296_loss: 4.8660 - LNout297_loss: 4.5880 - LNout298_loss: 5.2734 - LNout299_loss: 4.4662 - LNout300_loss: 5.4300 - LNout301_loss: 5.1361 - LNout302_loss: 5.4088 - LNout303_loss: 4.9471 - LNout304_loss: 5.5356 - LNout305_loss: 5.3668 - LNout306_loss: 4.6563 - LNout307_loss: 5.2287 - LNout308_loss: 4.9854 - LNout309_loss: 4.4891 - LNout310_loss: 5.0381 - LNout311_loss: 5.4210 - LNout312_loss: 4.1297 - LNout313_loss: 5.6267 - LNout314_loss: 5.4093 - LNout315_loss: 4.8300 - LNout316_loss: 5.7221 - LNout317_loss: 5.4078 - LNout318_loss: 5.0416 - LNout319_loss: 5.1302 - LNout320_loss: 4.8660 - LNout321_loss: 4.9017 - LNout322_loss: 5.4024 - LNout323_loss: 4.2713 - LNout324_loss: 4.9925 - LNout325_loss: 5.0205 - LNout326_loss: 4.1768 - LNout327_loss: 4.8690 - LNout328_loss: 4.4223 - LNout329_loss: 5.0362 - LNout330_loss: 4.4444 - LNout331_loss: 4.0064 - LNout332_loss: 4.7607 - LNout333_loss: 5.4774 - LNout334_loss: 4.4012 - LNout335_loss: 5.6319 - LNout336_loss: 4.6759 - LNout337_loss: 5.1511 - LNout338_loss: 4.6914 - LNout339_loss: 5.4424 - LNout340_loss: 5.1559 - LNout341_loss: 5.2491 - LNout342_loss: 5.0456 - LNout343_loss: 5.6670 - LNout344_loss: 4.9242 - LNout345_loss: 4.9297 - LNout346_loss: 4.6847 - LNout347_loss: 5.4212 - LNout348_loss: 4.5472 - LNout349_loss: 5.2009 - LNout350_loss: 4.8426 - LNout351_loss: 4.8316 - LNout352_loss: 4.9496 - LNout353_loss: 5.1051 - LNout354_loss: 5.2215 - LNout355_loss: 5.1756 - LNout356_loss: 4.7995 - LNout357_loss: 5.7803 - LNout358_loss: 5.2372 - LNout359_loss: 5.1771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "index type:  <class 'int'>\n",
      "self.batch_size:  128\n",
      "117/117 - 116s - loss: 1801.7511 - error_feed_forward_output1_loss: 0.2948 - LNout0_loss: 5.2106 - LNout1_loss: 5.1455 - LNout2_loss: 5.0851 - LNout3_loss: 5.4151 - LNout4_loss: 5.1636 - LNout5_loss: 5.0136 - LNout6_loss: 4.6889 - LNout7_loss: 4.7078 - LNout8_loss: 5.0352 - LNout9_loss: 4.9262 - LNout10_loss: 5.2844 - LNout11_loss: 4.3594 - LNout12_loss: 4.8009 - LNout13_loss: 5.7129 - LNout14_loss: 4.8494 - LNout15_loss: 5.3630 - LNout16_loss: 5.6969 - LNout17_loss: 4.6007 - LNout18_loss: 5.4162 - LNout19_loss: 4.2210 - LNout20_loss: 5.0349 - LNout21_loss: 4.5342 - LNout22_loss: 5.1651 - LNout23_loss: 4.9976 - LNout24_loss: 4.0436 - LNout25_loss: 4.7768 - LNout26_loss: 5.2564 - LNout27_loss: 4.6100 - LNout28_loss: 4.5256 - LNout29_loss: 4.9332 - LNout30_loss: 5.1627 - LNout31_loss: 4.9056 - LNout32_loss: 5.4429 - LNout33_loss: 4.7476 - LNout34_loss: 4.6373 - LNout35_loss: 4.2070 - LNout36_loss: 5.5083 - LNout37_loss: 4.5648 - LNout38_loss: 4.5161 - LNout39_loss: 5.5263 - LNout40_loss: 5.4963 - LNout41_loss: 5.2713 - LNout42_loss: 5.3214 - LNout43_loss: 5.7431 - LNout44_loss: 5.6001 - LNout45_loss: 5.2095 - LNout46_loss: 5.5540 - LNout47_loss: 4.8738 - LNout48_loss: 4.5113 - LNout49_loss: 4.5359 - LNout50_loss: 4.6943 - LNout51_loss: 4.1669 - LNout52_loss: 5.5580 - LNout53_loss: 4.5077 - LNout54_loss: 5.7041 - LNout55_loss: 5.1110 - LNout56_loss: 5.2335 - LNout57_loss: 4.4659 - LNout58_loss: 5.4852 - LNout59_loss: 5.6431 - LNout60_loss: 4.9708 - LNout61_loss: 5.2635 - LNout62_loss: 5.3569 - LNout63_loss: 5.0174 - LNout64_loss: 4.6195 - LNout65_loss: 5.2004 - LNout66_loss: 5.6364 - LNout67_loss: 4.9432 - LNout68_loss: 4.8773 - LNout69_loss: 6.2107 - LNout70_loss: 5.0427 - LNout71_loss: 4.4362 - LNout72_loss: 5.1907 - LNout73_loss: 5.1911 - LNout74_loss: 4.6018 - LNout75_loss: 4.3232 - LNout76_loss: 4.1356 - LNout77_loss: 4.6409 - LNout78_loss: 5.4516 - LNout79_loss: 5.3558 - LNout80_loss: 4.0035 - LNout81_loss: 5.6272 - LNout82_loss: 5.9477 - LNout83_loss: 4.1565 - LNout84_loss: 5.4763 - LNout85_loss: 5.1924 - LNout86_loss: 4.7486 - LNout87_loss: 4.7761 - LNout88_loss: 4.9353 - LNout89_loss: 4.5247 - LNout90_loss: 5.0018 - LNout91_loss: 5.0521 - LNout92_loss: 4.7970 - LNout93_loss: 4.1660 - LNout94_loss: 5.3652 - LNout95_loss: 5.0382 - LNout96_loss: 5.8132 - LNout97_loss: 5.1160 - LNout98_loss: 5.1338 - LNout99_loss: 5.6219 - LNout100_loss: 4.7919 - LNout101_loss: 5.0429 - LNout102_loss: 5.2851 - LNout103_loss: 5.5695 - LNout104_loss: 5.1257 - LNout105_loss: 4.3432 - LNout106_loss: 6.1877 - LNout107_loss: 4.7325 - LNout108_loss: 5.2419 - LNout109_loss: 4.9796 - LNout110_loss: 5.1861 - LNout111_loss: 4.4841 - LNout112_loss: 5.9588 - LNout113_loss: 4.9176 - LNout114_loss: 4.8438 - LNout115_loss: 4.9703 - LNout116_loss: 4.8144 - LNout117_loss: 4.9181 - LNout118_loss: 4.5205 - LNout119_loss: 5.9833 - LNout120_loss: 5.1028 - LNout121_loss: 5.0352 - LNout122_loss: 5.3191 - LNout123_loss: 4.3639 - LNout124_loss: 5.1311 - LNout125_loss: 5.1768 - LNout126_loss: 5.1031 - LNout127_loss: 5.0548 - LNout128_loss: 4.7021 - LNout129_loss: 5.3578 - LNout130_loss: 5.4356 - LNout131_loss: 4.8097 - LNout132_loss: 4.8181 - LNout133_loss: 4.7673 - LNout134_loss: 5.5358 - LNout135_loss: 5.1651 - LNout136_loss: 5.3247 - LNout137_loss: 4.8810 - LNout138_loss: 5.0563 - LNout139_loss: 4.4578 - LNout140_loss: 4.1576 - LNout141_loss: 5.7301 - LNout142_loss: 4.8876 - LNout143_loss: 4.8923 - LNout144_loss: 5.2959 - LNout145_loss: 4.7418 - LNout146_loss: 4.6441 - LNout147_loss: 4.6088 - LNout148_loss: 4.6231 - LNout149_loss: 5.1313 - LNout150_loss: 6.0892 - LNout151_loss: 4.6891 - LNout152_loss: 5.2366 - LNout153_loss: 5.0645 - LNout154_loss: 4.2405 - LNout155_loss: 5.4010 - LNout156_loss: 5.3743 - LNout157_loss: 4.9720 - LNout158_loss: 5.6951 - LNout159_loss: 4.7590 - LNout160_loss: 4.7866 - LNout161_loss: 5.2311 - LNout162_loss: 5.4148 - LNout163_loss: 4.0788 - LNout164_loss: 5.6952 - LNout165_loss: 5.1370 - LNout166_loss: 5.1932 - LNout167_loss: 5.0992 - LNout168_loss: 4.4418 - LNout169_loss: 4.8997 - LNout170_loss: 4.9392 - LNout171_loss: 4.8790 - LNout172_loss: 4.3698 - LNout173_loss: 5.0944 - LNout174_loss: 5.1710 - LNout175_loss: 5.1984 - LNout176_loss: 5.6871 - LNout177_loss: 5.2293 - LNout178_loss: 4.2968 - LNout179_loss: 5.2556 - LNout180_loss: 4.8844 - LNout181_loss: 5.0209 - LNout182_loss: 5.2171 - LNout183_loss: 4.5523 - LNout184_loss: 6.4674 - LNout185_loss: 4.7662 - LNout186_loss: 5.1590 - LNout187_loss: 4.1505 - LNout188_loss: 4.7551 - LNout189_loss: 4.8315 - LNout190_loss: 5.2969 - LNout191_loss: 5.3999 - LNout192_loss: 4.7670 - LNout193_loss: 4.7997 - LNout194_loss: 4.0214 - LNout195_loss: 4.9364 - LNout196_loss: 5.8695 - LNout197_loss: 4.4301 - LNout198_loss: 5.2289 - LNout199_loss: 5.9578 - LNout200_loss: 4.6146 - LNout201_loss: 5.0269 - LNout202_loss: 4.9062 - LNout203_loss: 4.9180 - LNout204_loss: 4.7140 - LNout205_loss: 4.3267 - LNout206_loss: 4.7790 - LNout207_loss: 4.5235 - LNout208_loss: 5.1553 - LNout209_loss: 5.2261 - LNout210_loss: 5.5445 - LNout211_loss: 5.6279 - LNout212_loss: 5.2373 - LNout213_loss: 4.7098 - LNout214_loss: 5.7096 - LNout215_loss: 5.4924 - LNout216_loss: 5.3529 - LNout217_loss: 4.9670 - LNout218_loss: 5.0337 - LNout219_loss: 4.4760 - LNout220_loss: 4.8221 - LNout221_loss: 5.2601 - LNout222_loss: 4.5942 - LNout223_loss: 5.4006 - LNout224_loss: 5.8188 - LNout225_loss: 5.1904 - LNout226_loss: 4.7286 - LNout227_loss: 6.1861 - LNout228_loss: 5.2749 - LNout229_loss: 4.0813 - LNout230_loss: 5.0330 - LNout231_loss: 5.1572 - LNout232_loss: 4.7863 - LNout233_loss: 4.6499 - LNout234_loss: 5.0333 - LNout235_loss: 4.8478 - LNout236_loss: 4.0750 - LNout237_loss: 4.8720 - LNout238_loss: 5.0264 - LNout239_loss: 4.6750 - LNout240_loss: 4.9777 - LNout241_loss: 5.6712 - LNout242_loss: 5.3085 - LNout243_loss: 4.4070 - LNout244_loss: 5.0098 - LNout245_loss: 5.2221 - LNout246_loss: 4.6901 - LNout247_loss: 4.8114 - LNout248_loss: 5.3875 - LNout249_loss: 4.8744 - LNout250_loss: 4.7477 - LNout251_loss: 5.5922 - LNout252_loss: 4.3816 - LNout253_loss: 4.7196 - LNout254_loss: 5.0431 - LNout255_loss: 4.9693 - LNout256_loss: 5.2951 - LNout257_loss: 4.9660 - LNout258_loss: 5.3250 - LNout259_loss: 5.2116 - LNout260_loss: 4.9804 - LNout261_loss: 4.5790 - LNout262_loss: 5.0165 - LNout263_loss: 4.9090 - LNout264_loss: 5.4929 - LNout265_loss: 5.1085 - LNout266_loss: 4.9948 - LNout267_loss: 5.4825 - LNout268_loss: 5.4243 - LNout269_loss: 4.4086 - LNout270_loss: 5.2492 - LNout271_loss: 4.9563 - LNout272_loss: 4.7379 - LNout273_loss: 4.7285 - LNout274_loss: 4.8728 - LNout275_loss: 4.3668 - LNout276_loss: 5.0204 - LNout277_loss: 4.7014 - LNout278_loss: 5.3669 - LNout279_loss: 4.8656 - LNout280_loss: 4.7396 - LNout281_loss: 4.9775 - LNout282_loss: 5.4250 - LNout283_loss: 4.3136 - LNout284_loss: 4.8890 - LNout285_loss: 5.6779 - LNout286_loss: 4.9703 - LNout287_loss: 4.5164 - LNout288_loss: 5.1101 - LNout289_loss: 4.9733 - LNout290_loss: 4.9259 - LNout291_loss: 5.1563 - LNout292_loss: 5.1648 - LNout293_loss: 5.4962 - LNout294_loss: 5.0816 - LNout295_loss: 5.5790 - LNout296_loss: 4.9495 - LNout297_loss: 4.8240 - LNout298_loss: 5.0763 - LNout299_loss: 4.4926 - LNout300_loss: 5.2312 - LNout301_loss: 4.8612 - LNout302_loss: 4.9735 - LNout303_loss: 4.9149 - LNout304_loss: 5.2116 - LNout305_loss: 5.4297 - LNout306_loss: 4.6329 - LNout307_loss: 5.6409 - LNout308_loss: 4.6850 - LNout309_loss: 4.3606 - LNout310_loss: 4.8518 - LNout311_loss: 5.0513 - LNout312_loss: 4.4405 - LNout313_loss: 5.2464 - LNout314_loss: 5.4962 - LNout315_loss: 4.8590 - LNout316_loss: 5.1922 - LNout317_loss: 5.2600 - LNout318_loss: 4.5924 - LNout319_loss: 4.7862 - LNout320_loss: 4.4680 - LNout321_loss: 4.5239 - LNout322_loss: 5.0553 - LNout323_loss: 4.6440 - LNout324_loss: 5.0809 - LNout325_loss: 5.2571 - LNout326_loss: 4.5544 - LNout327_loss: 4.9822 - LNout328_loss: 4.3690 - LNout329_loss: 5.1232 - LNout330_loss: 4.3649 - LNout331_loss: 3.9281 - LNout332_loss: 4.8751 - LNout333_loss: 5.4587 - LNout334_loss: 4.6527 - LNout335_loss: 5.6114 - LNout336_loss: 4.9193 - LNout337_loss: 4.9103 - LNout338_loss: 4.8270 - LNout339_loss: 5.1338 - LNout340_loss: 5.0959 - LNout341_loss: 5.1684 - LNout342_loss: 4.9270 - LNout343_loss: 5.4874 - LNout344_loss: 4.9668 - LNout345_loss: 5.0890 - LNout346_loss: 4.1583 - LNout347_loss: 5.2093 - LNout348_loss: 4.4366 - LNout349_loss: 5.1719 - LNout350_loss: 4.9467 - LNout351_loss: 4.9246 - LNout352_loss: 4.8166 - LNout353_loss: 4.8284 - LNout354_loss: 5.1517 - LNout355_loss: 5.0331 - LNout356_loss: 4.8393 - LNout357_loss: 5.6906 - LNout358_loss: 5.2296 - LNout359_loss: 4.9861\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W.R_Chen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model para\n",
    "model_name = \"test_model1.h5\"\n",
    "x_test_model = \"x_test_500.npy\" \n",
    "y_test_mdodel1 = \"y_test0_500.npy\" \n",
    "y_test_mdodel2 = \"y_test1_500.npy\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "def loadmodel(model_name, x_test_model, y_test_mdodel1, y_test_mdodel2):\n",
    "    import numpy as np\n",
    "    #load model and dic ps. dic is not use\n",
    "    model, source_token_dict = load(model_name)\n",
    "    #print(model.summary())\n",
    "    #load\n",
    "    \n",
    "    '''\n",
    "    Para:\n",
    "        x_test_loaded  : answer model        (include type and lineblock)\n",
    "        y_test_loaded_0: predict model       (type)\n",
    "        y_test_loaded_1: predict model       (lineblock)\n",
    "        out1           : answer model output (type) \n",
    "        out2           : answer model output (lineblock)\n",
    "    '''\n",
    "    \n",
    "    x_test_loaded = loadTestTrainData(\"test_models/\" + x_test_model) \n",
    "    y_test_loaded_0 = loadTestTrainData(\"test_models/\" + y_test_mdodel1)\n",
    "    y_test_loaded_1 = loadTestTrainData(\"test_models/\" + y_test_mdodel2) \n",
    "    \n",
    "    ''' <-------dust switch\n",
    "    print(\"y_test_loaded_1 shape: \", y_test_loaded_1.shape)\n",
    "    print(\"x_test_loaded length: \", len(x_test_loaded))\n",
    "    #'''\n",
    "    \n",
    "    out1, out2 = tfr.decode(model,x_test_loaded, max_len = x.getsource_max_lan())\n",
    "    \n",
    "    #==============show org result================\n",
    "    ''' <-------dust switch\n",
    "    print(\"y_test_loaded_0 shape: \", y_test_loaded_0.shape)\n",
    "    print(\"y_test_loaded_1 shape: \", y_test_loaded_1.shape)\n",
    "    print(\"y_test_loaded_0[0] result:\", (y_test_loaded_0[0])) #Error_type #vs out1\n",
    "    print(\"y_test_loaded_1[0][1] result:\", (y_test_loaded_1[0][1])) #Line_Block #vs out2\n",
    "    print(\"out1 shape: \", (out1).shape)#prob upper then 0.5\n",
    "    print(\"out2[0] shape: \", (out2[0]).shape)#prob upper then 0.5\n",
    "    print(\"out2 length: \", len(out2))\n",
    "    print(\"out2[0] length: \", len(out2[0]))#prob lb\n",
    "    #'''\n",
    "    \n",
    "    #=============================================\n",
    "    test_ep = np.around(out1)\n",
    "    test_lb = np.around(out2)\n",
    "    ans_ep = np.around(y_test_loaded_0)\n",
    "    ans_lb = np.around(y_test_loaded_1)\n",
    "    #==============show toint result==============\n",
    "    \n",
    "    ''' <-------dust switch\n",
    "    print(test_ep[1])\n",
    "    print(test_lb[1])\n",
    "    print(ans_ep[1])\n",
    "    print([ans_lb[1]])\n",
    "    #'''\n",
    "    #=============================================\n",
    "    return test_ep, ans_ep, test_lb, ans_lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(pre_errortype, ans_errortype):\n",
    "    #print(\"inter :\", pre_errortype)\n",
    "    #print(\"inter: \", ans_errortype)\n",
    "    #ans_errortype = float(ans_errortype)\n",
    "    #ref = \"https://www.796t.com/post/Mjc4am8=.html\"\n",
    "    inter = [pre_value for pre_value, ans_value in zip(pre_errortype, ans_errortype) if (pre_value == ans_value == 1)]\n",
    "    inter_two = [pre_value for pre_value, ans_value in zip(pre_errortype, ans_errortype) if (pre_value == ans_value)]\n",
    "    #print(\"sort inter: \", inter)\n",
    "    return inter, inter_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new ans array contains only 1 in array \n",
    "def ans_typefilter(ans_errortype):\n",
    "    #print(\"org ans type: \", ans_errortype)\n",
    "    new_ans_errortype = [value for value in ans_errortype if value == 1]\n",
    "    #print(\"new ans errortype: \", new_ans_errortype)\n",
    "    return new_ans_errortype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new pre array contains only 1 in array \n",
    "def pre_typefilter(pre_errortype):\n",
    "    #print(\"org error type: \", pre_errortype)\n",
    "    new_pre_errortype = [value for value in pre_errortype if value == 1]\n",
    "    #print(\"new pre errortype: \", new_pre_errortype)\n",
    "    return new_pre_errortype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate error type score\n",
    "#Note:\n",
    "    #Predict score:\n",
    "        # score = inter of ans and pre divide by len of pre\n",
    "    #Recall score:\n",
    "        # score = inter of ans and pre divide by len of ans\n",
    "    #Accuracy score:\n",
    "        # score = inter_two of ans and pre divide by original len of pre\n",
    "def errortype_score(pre_errortype, ans_errortype):\n",
    "    #find pre length\n",
    "    new_pre_errortype = pre_typefilter(pre_errortype) #make new array for score\n",
    "    pre_length = len(new_pre_errortype) #get lenght from ans_type\n",
    "    #find ans length\n",
    "    new_ans_errortype = ans_typefilter(ans_errortype) #make new array for score\n",
    "    ans_length = len(new_ans_errortype) #get lenght from ans_type\n",
    "    #print(\"ans length: \", ans_length)\n",
    "    inter, inter_two = intersect(pre_errortype, ans_errortype) #get intersection\n",
    "    inter_length = len(inter)\n",
    "    inter_two_length = len(inter_two)\n",
    "    #print(\"inter_length: \", inter_length)\n",
    "    #print(\"pre length: \", pre_length) \n",
    "    \n",
    "    #calculate Predict score \n",
    "    if (inter_length == 0 and pre_length == 0):\n",
    "        pre_score = 1\n",
    "    elif (pre_length == 0):\n",
    "        pre_score = 0\n",
    "    else:\n",
    "        pre_score = inter_length/pre_length \n",
    "    #print(\"predict score: \", pre_score) #show pre score\n",
    "    \n",
    "    #calculate Recall score\n",
    "    if (inter_length == 0 and ans_length == 0):\n",
    "        rec_score = 1\n",
    "    elif (ans_length == 0):\n",
    "        rec_score = 0\n",
    "    else:\n",
    "        rec_score = inter_length/ans_length\n",
    "    #print(\"recall score: \", rec_score) #show ans score\n",
    "    \n",
    "    #calculate Accuarcy score\n",
    "    acc_score = inter_two_length/len(pre_errortype)\n",
    "    \n",
    "    return pre_score, rec_score , acc_score#return float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show total perdict score and recall score\n",
    "def errortype_totalscore(pre_errortype,ans_errortype):\n",
    "    #initial para\n",
    "    pre_total = 0.0\n",
    "    rec_total = 0.0\n",
    "    acc_total = 0.0\n",
    "    #get each score then get total and avg score\n",
    "    for i in range(len(pre_errortype)):\n",
    "        #print(\"Sample: \", i)\n",
    "        pre, rec, acc = errortype_score(pre_errortype[i], ans_errortype[i])\n",
    "        pre_total = pre_total + pre\n",
    "        rec_total = rec_total + rec\n",
    "        acc_total = acc_total + acc\n",
    "    print(\"pre_total: \", pre_total)\n",
    "    print(\"rec_total: \", rec_total)\n",
    "    print(\"acc_total: \", acc_total)\n",
    "    pre_avg_score = pre_total/len(pre_errortype)\n",
    "    rec_avg_socre = rec_total/len(ans_errortype)\n",
    "    acc_avg_score = acc_total/len(ans_errortype)\n",
    "    print(\"avg_pre: \", pre_avg_score)#pre_total/len(pre_errortype))\n",
    "    print(\"avg_rec: \", rec_avg_socre)#rec_total/len(ans_errortype))\n",
    "    print(\"avg_acc: \", acc_avg_score)#acc_total/len(ans_errortype))\n",
    "    return pre_avg_score, rec_avg_socre, acc_avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_line(pre_begin_index, pre_end_index, ans_begin_index, ans_end_index):\n",
    "    #Note:\n",
    "            #use softmax: [0.....150] total == 1\n",
    "            #begin: index[0]\n",
    "            #end: index[0]\n",
    "        \n",
    "    #get [value] first and get [value] again to get value\n",
    "    pre_begin_line = pre_begin_index[0]\n",
    "    pre_begin_line = pre_begin_line[0]\n",
    "    pre_end_line = pre_end_index[0]\n",
    "    pre_end_line = pre_end_line[0]\n",
    "    ans_begin_line = ans_begin_index[0]\n",
    "    ans_begin_line = ans_begin_line[0]\n",
    "    ans_end_line = ans_end_index[0]\n",
    "    ans_end_line = ans_end_line[0]\n",
    "    \n",
    "    return pre_begin_line, pre_end_line, ans_begin_line, ans_end_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_adjustment(line):\n",
    "    if(line > 1):\n",
    "        line = line-1\n",
    "        return line\n",
    "    else:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each pre and ans lineblocks and make them to pre begin/end ans begin/end array\n",
    "def make_lineblock(begin, end):\n",
    "    if (end < begin):\n",
    "        block = [value for value in range(end, begin+1)]\n",
    "        return block\n",
    "    else:\n",
    "        block = [value for value in range(begin, end+1)]\n",
    "        return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref = \"https://www.geeksforgeeks.org/python-intersection-two-lists/\"\n",
    "def lineblock_intersect(pre_block, ans_block):\n",
    "    inter = [value for value in pre_block if value in ans_block]\n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return index with int datatype \n",
    "def get_block_index(pre_begins, pre_ends, ans_begins, ans_ends):\n",
    "    pre_begin = int(pre_begins)\n",
    "    pre_end = int(pre_ends)\n",
    "    ans_begin = int(ans_begins)\n",
    "    ans_end = int(ans_ends)\n",
    "    #print(pre_begin, pre_end, ans_begin, ans_end)\n",
    "    return pre_begin, pre_end, ans_begin, ans_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new pre array contains only 1 in array \n",
    "def linefilter(line_block):\n",
    "    #print(\"org error type: \", pre_errortype)\n",
    "    new_line = [value for value in line_block if value == 1]\n",
    "    #print(\"new pre errortype: \", new_pre_errortype)\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Note:\n",
    "    #Predict score:\n",
    "        # score = inter of ans and pre divide by len of pre\n",
    "    #Recall score:\n",
    "        # score = inter of ans and pre divide by len of ans\n",
    "    #                 b   s   l\n",
    "    #line sturcture [84][36][150]\n",
    "def errorline_totalscore(pre_errorline, ans_errorline):\n",
    "    #get sample size\n",
    "    sample_size = len(pre_errorline[1])\n",
    "    #get block size\n",
    "    lineblock_size = len(pre_errorline)\n",
    "    total_pre = 0.0\n",
    "    total_rec = 0.0\n",
    "    total_sample_pre = 0.0\n",
    "    total_sample_rec = 0.0\n",
    "    for sample in range(sample_size):\n",
    "        sample_totalline_pre = 0.0\n",
    "        sample_totalline_rec = 0.0\n",
    "        for lineblock in range(0, lineblock_size, 2):\n",
    "            #give index blocks value\n",
    "            pre_begin = pre_errorline[lineblock][sample]\n",
    "            pre_end = pre_errorline[lineblock+1][sample]\n",
    "            ans_begin = ans_errorline[lineblock][sample]\n",
    "            ans_end = ans_errorline[lineblock+1][sample]\n",
    "            ''' <-------dust switch\n",
    "            #show error block value\n",
    "            print(\"pre_errorline begin[lineblock][sample]: \", lineblock, \" \", sample)\n",
    "            print(pre_begin)\n",
    "            print(\"pre_errorline begin[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "            print(pre_end)\n",
    "            print(\"ans_errorline begin [lineblock][sample]: \", lineblock, \" \", sample)\n",
    "            print(ans_begin)\n",
    "            print(\"ans_errorline end[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "            print(ans_end)\n",
    "            #'''\n",
    "            #get block start/end index\n",
    "            pre_begin_index = np.where(pre_begin == 1)\n",
    "            pre_end_index = np.where(pre_end == 1)\n",
    "            ans_begin_index = np.where(ans_begin == 1)\n",
    "            ans_end_index = np.where(ans_end == 1)\n",
    "            ''' <-------dust switch\n",
    "            #show start/end index\n",
    "            print(\"pre_begin_index: \", pre_begin_index)\n",
    "            print(\"pre_begin_index type: \", type(pre_begin_index))\n",
    "            print(\"pre_end_index: \", pre_end_index)\n",
    "            print(\"ans_begin_index: \", ans_begin_index)\n",
    "            print(\"ans_end_index: \", ans_end_index)\n",
    "            #'''\n",
    "            #give start value\n",
    "            #Note:\n",
    "                #use softmax: [0.....150] total == 1\n",
    "                #begin: index[0]\n",
    "                #end: index[0]\n",
    "\n",
    "            #get [value] first and get [value] again to get value\n",
    "            pre_begin_line, pre_end_line, ans_begin_line, ans_end_line = get_start_end_line(\n",
    "                                                                                            pre_begin_index, \n",
    "                                                                                            pre_end_index, \n",
    "                                                                                            ans_begin_index, \n",
    "                                                                                            ans_end_index\n",
    "                                                                                            )\n",
    "            ''' <-------dust switch\n",
    "            #show start/end index\n",
    "            print(\"pre_begin_line: \", pre_begin_line)\n",
    "            print(\"pre_end_line: \", pre_end_line)\n",
    "            print(\"ans_begin_line: \", ans_begin_line)\n",
    "            print(\"ans_end_line: \", ans_end_line)\n",
    "            #'''\n",
    "            ''' <-------dust switch\n",
    "            #'''\n",
    "            #make line downgrade for 1\n",
    "            #Note:\n",
    "            #before:  0 -1  1  2  3  4\n",
    "            #after:   0  1 <-----abs  \n",
    "                        #1  2  3  4\n",
    "                    #[0, 1, 2, 3, 4, 5]\n",
    "            pre_begin_line = line_adjustment(pre_begin_line)\n",
    "            pre_end_line = line_adjustment(pre_end_line)\n",
    "            ans_begin_line = line_adjustment(ans_begin_line)\n",
    "            ans_end_line = line_adjustment(ans_end_line)\n",
    "            ''' <-------dust switch\n",
    "            print(\"pre_begin_line: \", pre_begin_line)\n",
    "            print(\"pre_end_line: \", pre_end_line)\n",
    "            print(\"ans_begin_line: \",ans_begin_line)\n",
    "            print(\"ans_end_line: \", ans_end_line)\n",
    "            #'''\n",
    "            pre_lineblock = make_lineblock(pre_begin_line, pre_end_line)\n",
    "            ans_lineblock = make_lineblock(ans_begin_line, ans_end_line)\n",
    "            #print(\"pre_lineblock: \", pre_lineblock)\n",
    "            #print(\"ans_lineblock: \", ans_lineblock)\n",
    "            inter = lineblock_intersect(pre_lineblock, ans_lineblock)\n",
    "            #print(\"inter: \", inter)\n",
    "            inter_length = len(inter)\n",
    "            pre_length = len(pre_lineblock)\n",
    "            ans_length = len(ans_lineblock)\n",
    "            #print(\"inter length: \", inter_length)\n",
    "            #print(\"pre_length: \", pre_length)\n",
    "            #print(\"ans_length: \", ans_length)\n",
    "            #count pre_score\n",
    "            if (inter_length == 0 and pre_length == 0):\n",
    "                pre_score = 1\n",
    "            elif (pre_length == 0):\n",
    "                pre_score = 0\n",
    "            else :\n",
    "                pre_score = inter_length/pre_length\n",
    "            #count rec_score\n",
    "            if (inter_length == 0 and ans_length == 0):\n",
    "                rec_score = 1  \n",
    "                #\n",
    "            elif (ans_length == 0):\n",
    "                rec_score = 0\n",
    "            else:\n",
    "                rec_score = inter_length/ans_length\n",
    "            #print(\"Sample pre_score: \", pre_score)\n",
    "            #print(\"Sample rec_score: \", rec_score)\n",
    "            #cal each sample lineblock score\n",
    "            sample_totalline_pre += pre_score\n",
    "            sample_totalline_rec += rec_score\n",
    "            #print(\"Sample total_pre: \", sample_pre)\n",
    "            #print(\"Sample total_rec: \", sample_rec)\n",
    "        #cal total sample score\n",
    "        total_sample_pre += sample_totalline_pre/len(lineblock_size/2)\n",
    "        total_sample_rec += sample_totalline_rec/len(lineblock_size/2)\n",
    "    avg_pre = total_sample_pre/sample_size\n",
    "    avg_rec = total_sample_rec/sample_size\n",
    "    return avg_pre, avg_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_type_F_score(pre_score, rec_score):\n",
    "    f_one = (2*pre_score*rec_score)/(pre_score + rec_score)\n",
    "    f_two = (3*pre_score*rec_score)/((2*pre_score) + rec_score)\n",
    "    f_pointfive = (3*pre_score*rec_score)/(pre_score + (2*rec_score))\n",
    "    print(\"F_one: \", f_one)\n",
    "    print(\"F_two: \", f_two)\n",
    "    print(\"F_pointfive: \", f_pointfive)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_line_F_score(pre_score, rec_score):\n",
    "    f_one = (2*pre_score*rec_score)/(pre_score + rec_score)\n",
    "    f_two = (3*pre_score*rec_score)/((2*pre_score) + rec_score)\n",
    "    f_pointfive = (3*pre_score*rec_score)/(pre_score + (2*rec_score))\n",
    "    print(\"F_one: \", f_one)\n",
    "    print(\"F_two: \", f_two)\n",
    "    print(\"F_pointfive: \", f_pointfive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'source_token_dict.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-cc603c6b3897>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpre_errortype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans_errortype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_errorline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans_errorline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_mdodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_mdodel2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(len(pre_errortype))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m '''\n\u001b[0;32m      5\u001b[0m \u001b[0mwork\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-f3d072c5897b>\u001b[0m in \u001b[0;36mloadmodel\u001b[1;34m(model_name, x_test_model, y_test_mdodel1, y_test_mdodel2)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#load model and dic ps. dic is not use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_token_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(model.summary())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-209a7b72a5b5>\u001b[0m in \u001b[0;36mload\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mco\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msource_token_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"source_token_dict.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m        \u001b[1;31m# t = loadDictionary(target_token_dict, 'target_token_dict.pickle')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m        \u001b[1;31m# t_inv = loadDictionary(target_token_dict_inv, 'target_token_dict_inv.pickle')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-9c64b76d5a05>\u001b[0m in \u001b[0;36mloadDictionary\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloadDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0ma_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'source_token_dict.pickle'"
     ]
    }
   ],
   "source": [
    "pre_errortype, ans_errortype, pre_errorline, ans_errorline = loadmodel(model_name, x_test_model, y_test_mdodel1, y_test_mdodel2)\n",
    "#print(len(pre_errortype))\n",
    "\n",
    "'''\n",
    "work : \n",
    "    fix 3 dim prob: \n",
    "        out2 only show two dim\n",
    "        score need three dim: out2: 2D, y_test_loaded_1: 3D\n",
    "    issue:\n",
    "        y_test_loaded_1: dim is not expected...\n",
    "    ==================================================================================\n",
    "    expect :\n",
    "    y_test_loaded_0 : [sample = 36][error type = 36] correct v\n",
    "    out1 :            [sample = 36][error type = 36] correct v\n",
    "    y_test_loaded_1 : [sample = 36][block = 84][begin and end :line total = 150]\n",
    "    out2 :            [sample = 36][block = 84][begin and end :line total = 150]\n",
    "    ==================================================================================\n",
    "    real : \n",
    "            out2 shape            = [36][150]    ------>dim should be [36][84][150]\n",
    "            y_test_loaded_1 shape = [84][36][150]------>dim should be [36][84][150]\n",
    "    ==================================================================================\n",
    "    ref in line work space\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all sample error type score\n",
    "avg_pre, avg_rec, avg_acc = errortype_totalscore(pre_errortype, ans_errortype)\n",
    "error_type_F_score(avg_pre, avg_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all sample error line score\n",
    "avg_pre, avg_rec = errorline_totalscore(pre_errorline, ans_errorline)\n",
    "error_line_F_score(avg_pre, avg_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work space...\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "#len(ans_errortype)\n",
    "sample_size = len(pre_errorline[1])\n",
    "lineblock_size = len(pre_errorline)\n",
    "'''\n",
    "Note:\n",
    "    pre_errorline[lineblock][sample]\n",
    "    ans_errorline[lineblock][sample]\n",
    "Para:\n",
    "'''\n",
    "\n",
    "\n",
    "#''' <-------dust switch\n",
    "print(\"sample size: \", sample_size)\n",
    "print(\"lineblock size: \", lineblock_size)\n",
    "#'''\n",
    "total_pre_score = 0.0\n",
    "total_rec_score = 0.0\n",
    "for sample in range(sample_size):\n",
    "    for lineblock in range(0, lineblock_size, 2):\n",
    "        #give index blocks value\n",
    "        pre_begin = pre_errorline[lineblock][sample]\n",
    "        pre_end = pre_errorline[lineblock+1][sample]\n",
    "        ans_begin = ans_errorline[lineblock][sample]\n",
    "        ans_end = ans_errorline[lineblock+1][sample]\n",
    "        \n",
    "        ''' <-------dust switch\n",
    "        #show error block value\n",
    "        print(\"pre_errorline begin[lineblock][sample]: \", lineblock, \" \", sample)\n",
    "        print(pre_begin)\n",
    "        print(\"pre_errorline begin[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "        print(pre_end)\n",
    "        print(\"ans_errorline begin [lineblock][sample]: \", lineblock, \" \", sample)\n",
    "        print(ans_begin)\n",
    "        print(\"ans_errorline end[lineblock][sample]: \", lineblock+1, \" \", sample)\n",
    "        print(ans_end)\n",
    "        #'''\n",
    "        #get block start/end index\n",
    "        pre_begin_index = np.where(pre_begin == 1)\n",
    "        pre_end_index = np.where(pre_end == 1)\n",
    "        ans_begin_index = np.where(ans_begin == 1)\n",
    "        ans_end_index = np.where(ans_end == 1)\n",
    "        \n",
    "        ''' <-------dust switch\n",
    "        #show start/end index\n",
    "        print(\"pre_begin_index: \", pre_begin_index)\n",
    "        print(\"pre_begin_index type: \", type(pre_begin_index))\n",
    "        print(\"pre_end_index: \", pre_end_index)\n",
    "        print(\"ans_begin_index: \", ans_begin_index)\n",
    "        print(\"ans_end_index: \", ans_end_index)\n",
    "        #'''\n",
    "        \n",
    "        #give start value\n",
    "        #Note:\n",
    "            #use softmax: [0.....150] total == 1\n",
    "            #begin: index[0]\n",
    "            #end: index[0]\n",
    "        \n",
    "        #get [value] first and get [value] again to get value\n",
    "        #pre_begin_line, pre_end_line, ans_begin_line, ans_end_line = get_start_end_line(pre_begin_index, pre_end_index, ans_begin_index, ans_end_index)\n",
    "        \n",
    "        pre_begin_line = pre_begin_index[0]\n",
    "        #pre_begin_line = pre_begin_line[0]\n",
    "        pre_end_line = pre_end_index[0]\n",
    "        #pre_end_line = pre_end_line[0]\n",
    "        ans_begin_line = ans_begin_index[0]\n",
    "        ans_begin_line = ans_begin_line[0]\n",
    "        ans_end_line = ans_end_index[0]\n",
    "        ans_end_line = ans_end_line[0]\n",
    "        \n",
    "        #''' <-------dust switch\n",
    "        #show start/end index\n",
    "        print(\"pre_begin_line: \", pre_begin_line)\n",
    "        print(\"pre_end_line: \", pre_end_line)\n",
    "        print(\"ans_begin_line: \", ans_begin_line)\n",
    "        print(\"ans_end_line: \", ans_end_line)\n",
    "        #'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        #make intersection\n",
    "        \n",
    "        ''' <-------dust switch\n",
    "        print(\"inter_begin: \", inter_begin)\n",
    "        print(\"inter_all_begin: \", inter_all_begin)\n",
    "        #'''\n",
    "        \n",
    "        \n",
    "'''\n",
    "final decide: find index and build ----> [1 2 3 4 5 6] vs [2 3 4 5 6 7] to make intercestion\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note:\n",
    "    #Predict score:\n",
    "        # score = inter of ans and pre divide by len of pre\n",
    "    #Recall score:\n",
    "        # score = inter of ans and pre divide by len of ans\n",
    "    #Accuracy score:\n",
    "        # score = inter_two of ans and pre divide by original len of pre\n",
    "\n",
    "#case 1:  n  n vs  n  n \n",
    "#case 2:  n  n vs -1 -1\n",
    "#case 3: -1 -1 vs -1 -1\n",
    "#case 4: -1  n vs  n  n\n",
    "#case 5: -1  n vs -1  n\n",
    "pre_begin_line = 5\n",
    "pre_end_line = 7\n",
    "ans_begin_line = 3\n",
    "ans_end_line = 2\n",
    "pre_begin_line = line_adjustment(pre_begin_line)\n",
    "pre_end_line = line_adjustment(pre_end_line)\n",
    "ans_begin_line = line_adjustment(ans_begin_line)\n",
    "ans_end_line = line_adjustment(ans_end_line)\n",
    "\n",
    "print(\"pre_begin_line: \", pre_begin_line)\n",
    "print(\"pre_end_line: \", pre_end_line)\n",
    "print(\"ans_begin_line: \",ans_begin_line)\n",
    "print(\"ans_end_line: \", ans_end_line)\n",
    "pre_lineblock = make_lineblock(pre_begin_line, pre_end_line)\n",
    "ans_lineblock = make_lineblock(ans_begin_line, ans_end_line)\n",
    "print(\"pre_lineblock: \", pre_lineblock)\n",
    "print(\"ans_lineblock: \", ans_lineblock)\n",
    "inter = lineblock_intersect(pre_lineblock, ans_lineblock)\n",
    "print(\"inter: \", inter)\n",
    "inter_length = len(inter)\n",
    "pre_length = len(pre_lineblock)\n",
    "ans_length = len(ans_lineblock)\n",
    "print(\"inter length: \", inter_length)\n",
    "print(\"pre_length: \", pre_length)\n",
    "print(\"ans_length: \", ans_length)\n",
    "if (inter_length == 0 and pre_length == 0):\n",
    "    pre_score = 1\n",
    "    # issue 7.12ver \n",
    "elif (pre_length == 0):\n",
    "    pre_score = 0\n",
    "else :\n",
    "    pre_score = inter_length/pre_length\n",
    "#    \n",
    "if (inter_length == 0 and ans_length == 0):\n",
    "    rec_score = 1  \n",
    "    #\n",
    "elif (ans_length == 0):\n",
    "    rec_score = 0\n",
    "else:\n",
    "    rec_score = inter_length/ans_length\n",
    "#print(\"Sample pre_score: \", pre_score)\n",
    "total_pre += pre_score\n",
    "#print(\"Sample rec_score: \", rec_score)\n",
    "total_rec += rec_score\n",
    "#print(\"Sample total_pre: \", total_pre)\n",
    "#print(\"Sample total_rec: \", total_rec)\n",
    "avg_pre = total_pre/len(pre_begins)\n",
    "avg_rec = total_rec/len(ans_begins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "Input_Path = \"Trianing/InputTxt\"\n",
    "Input_Path = (glob.glob(Input_Path+\"/**/**/*.txt\"))\n",
    "for i in range(10):\n",
    "    print(Input_Path[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y-JMBpEFcJWh",
    "psq7nyP0ca_Z"
   ],
   "name": "Main_Colab擴增版.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
