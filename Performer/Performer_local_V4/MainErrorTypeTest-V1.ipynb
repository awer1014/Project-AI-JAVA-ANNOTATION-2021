{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d94ffe56",
    "outputId": "1249cd6b-bff9-44bc-f32e-212835586c4d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performerErrorTypeTest_V1 as tfr\n",
    "import nltk\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22361e8f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(file_name):\n",
    "    errlist=[]\n",
    "    LBlist=[]\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "    #讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows: \n",
    "            RL=list(row.values())\n",
    "            #print(\"RL[0]: \", type(RL[0]), \"RL[1]: \", type(RL[1]))\n",
    "            RL[1:] = list(map(int, RL[1:]))\n",
    "            errs=RL[1:37]\n",
    "            LB=RL[37:]\n",
    "            errlist.append(errs)\n",
    "            LBlist.append(LB)\n",
    "    return errlist,LBlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'\n",
    "    with open(filename, \"rb\") as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b92cf71a"
   },
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b88909b"
   },
   "outputs": [],
   "source": [
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abb41dde"
   },
   "outputs": [],
   "source": [
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'NUM_INT', '>'], [\"<NUM_INT>\"])\n",
    "    replace_sublist(x, ['<', 'NUM_FLOAT', '>'], [\"<NUM_FLOAT>\"])\n",
    "    replace_sublist(x, ['<', 'STRING', '>'], [\"<STRING>\"])\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0db5e0b1"
   },
   "outputs": [],
   "source": [
    "def parseSentence(x):\t\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        #print(ord(x[i]))\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"ASCII\":\n",
    "            if inp==\"E\":\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  # nltk.word_tokenize(chrs) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e88de533"
   },
   "outputs": [],
   "source": [
    "def readcode(fname):\n",
    "    with open(fname,encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def listdir_fullpath(d):\n",
    "    return [f for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec120d95"
   },
   "outputs": [],
   "source": [
    "#save model for training\n",
    "class TestTranslate(unittest.TestCase):\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<CR>': 5,\n",
    "            '<NUM_INT>': 6,\n",
    "            '<NUM_FLOAT>': 7,\n",
    "            '<STRING>': 8,\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "    \n",
    "    def test_translate(self):\n",
    "        #print(\"i am here: \" )\n",
    "        #source_file=[]\n",
    "        #Set Para\n",
    "        max_javaline_length = 160 #Max number of lines\n",
    "        #set path\n",
    "        Output_Path = \"Trianing\\InputCSV\\Split-500-reduce-binary-accuracy\"\n",
    "        Input_Path = \"Trianing\\InputTxt\\Split-500-reduce-binary-accuracy\"\n",
    "        model_for_training_org_path = \"Model-for-training-org\\ErrorType-Test\\Split-500-reduce-binary-accuracy\\Max-len-1000\"\n",
    "        model_for_training_path = \"Model-for-training\\ErrorType-Test\\Split-500-reduce-binary-accuracy\\Max-len-1000\"\n",
    "        Trained_model_Path = \"Trained_models\\ErrorType-Test\\Split-500-reduce-binary-accuracy\\Max-len-1000\\Learning-rate-3x0\\weight-1vs1-v1\"\n",
    "        source_max_len_name = \"source_max_len\"\n",
    "        type_weight = 1\n",
    "        line_weight = 1\n",
    "        learning_rate_value = 0.0001\n",
    "        #get all txt file in input path\n",
    "        target_max_len = 0\n",
    "        token_num = 0\n",
    "\n",
    "        #start training\n",
    "        import DataGeneratorTrainErrorTypeEdition as DGTrain\n",
    "        import DataGeneratorValidationErrorTypeEdition as DGValidation\n",
    "        import DataBuffer as db\n",
    "        from random import randrange\n",
    "        source_token_dict_name = \"source_token_dict.pickle\"\n",
    "        #load source_token_dict\n",
    "        source_token_dict = loadDictionary(model_for_training_path + \"/\" + source_token_dict_name)\n",
    "        #load source_max_len\n",
    "        source_max_len = loadDictionary(Trained_model_Path + \"/\" + source_max_len_name)\n",
    "        #load training models and their len\n",
    "        x_train = loadTestTrainData(model_for_training_path + \"/\" + \"x_train_0.npy\")\n",
    "        x_validation = loadTestTrainData(model_for_training_path + \"/\" + \"x_validation_0.npy\")\n",
    "        \n",
    "        buffer_train_num = len(x_train)\n",
    "        buffer_val_train_num = len(x_validation)\n",
    "        #Set model para    \n",
    "        model = tfr.get_model(max_input_len=(source_max_len),\n",
    "                              errNum=36,\n",
    "                              token_num=len(source_token_dict),\n",
    "                              embed_dim=32, #32, try 32 or 64\n",
    "                              encoder_num=6, #2 max = 6\n",
    "                              head_num=4,#4\n",
    "                              hidden_dim=128, #128\n",
    "                              dropout_rate=0.05 #0.05\n",
    "                             )\n",
    "        #Set losses\n",
    "        losses = {\"error_feed_forward_output1\": \"binary_crossentropy\"}\n",
    "        #error type weight\n",
    "        lossWeights = {\"error_feed_forward_output1\": type_weight}\n",
    "        metrics = {\"error_feed_forward_output1\": \"binary_accuracy\"}\n",
    "        #metrics = {\"error_feed_forward_output1\": tf.keras.metrics.Accuracy()}\n",
    "        \n",
    "        #set complie para\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss=losses, loss_weights=lossWeights, metrics=metrics)\n",
    "        \n",
    "        \n",
    "        #for output\n",
    "        #for x\n",
    "        #data_number and block_size = data num\n",
    "        input_buffer_params = { \n",
    "            \"data_path\": model_for_training_path,\n",
    "            \"data_number\": buffer_train_num,\n",
    "            \"data_type\": int,\n",
    "            \"block_size\": buffer_train_num \n",
    "            }\n",
    "        \n",
    "        #for input\n",
    "        #for y\n",
    "        #data_number and block_size = data num\n",
    "        \"\"\"\n",
    "        output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path, model_for_training_path],\n",
    "            \"data_number\": [buffer_train_num, buffer_train_num],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [buffer_train_num, buffer_train_num] \n",
    "            }\n",
    "        \"\"\"\n",
    "        output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path],\n",
    "            \"data_number\": [buffer_train_num],\n",
    "            \"data_type\": [int],\n",
    "            \"block_size\": [buffer_train_num] \n",
    "            }\n",
    "        \n",
    "        #===========================================\n",
    "        #for output\n",
    "        #for x\n",
    "        #data_number and block_size = validation data num\n",
    "        \n",
    "        validation_input_buffer_params = { \n",
    "            \"data_path\": model_for_training_path,\n",
    "            \"data_number\": buffer_val_train_num,\n",
    "            \"data_type\": int,\n",
    "            \"block_size\": buffer_val_train_num \n",
    "            }\n",
    "        \n",
    "        #for input\n",
    "        #for y\n",
    "        #data_number and block_size = validation data num\n",
    "        \"\"\"\n",
    "        validation_output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path, model_for_training_path],\n",
    "            \"data_number\": [buffer_val_train_num, buffer_val_train_num],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [buffer_val_train_num, buffer_val_train_num] \n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "        validation_output_buffer_params = {\n",
    "            \"data_path\": [model_for_training_path],\n",
    "            \"data_number\": [buffer_val_train_num],\n",
    "            \"data_type\": [int],\n",
    "            \"block_size\": [buffer_val_train_num] \n",
    "            }\n",
    "        \n",
    "        \n",
    "        #Create Generators\n",
    "        print(\"Creating training generator...\")\n",
    "        #give training data num\n",
    "        \"\"\"training_generator = DGTrain.DataGeneratorTrain(input_buffer_params,\n",
    "                                                           output_buffer_params,\n",
    "                                                           [list(range(buffer_val_train_num)), list(range(buffer_val_train_num))] \n",
    "                                                )\"\"\"\n",
    "        training_generator = DGTrain.DataGeneratorTrainErrorTypeEdition(input_buffer_params,\n",
    "                                                                        output_buffer_params,\n",
    "                                                                        [list(range(buffer_val_train_num))] \n",
    "                                                )\n",
    "        #Create Generators\n",
    "        print(\"Creating validation generator...\")\n",
    "        #give valitdation data num\n",
    "        \"\"\"validation_generator = DGValidation.DataGeneratorValidation(validation_input_buffer_params,\n",
    "                                                                       validation_output_buffer_params,\n",
    "                                                                      [list(range(buffer_val_train_num)), list(range(buffer_val_train_num))] \n",
    "                                                )\"\"\"\n",
    "        validation_generator = DGValidation.DataGeneratorValidationErrorTypeEdition(validation_input_buffer_params,\n",
    "                                                  validation_output_buffer_params,\n",
    "                                                  [list(range(buffer_val_train_num))] \n",
    "                                                )\n",
    "        \n",
    "        \n",
    "        #''' <-----traing switch\n",
    "        #Start training\n",
    "        print(\"Strat training...\")\n",
    "        \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = Trained_model_Path + \"/\" + \"checkpoint_model.h5\",\n",
    "                                                                       monitor = \"val_loss\",\n",
    "                                                                       mode = \"min\",\n",
    "                                                                       save_best_only = True\n",
    "                                                                      )\n",
    "        \n",
    "        history = model.fit_generator(generator = training_generator,\n",
    "                                      epochs = 1000, #100 200 500 3000\n",
    "                                      verbose = 2, #set visibility\n",
    "                                      validation_data = validation_generator,\n",
    "                                      callbacks = [model_checkpoint_callback],\n",
    "                                     )\n",
    "        \n",
    "        \n",
    "        print(\"Model training completed...\")\n",
    "        #save history\n",
    "        print(\"Saving history...\")\n",
    "        saveDictionary(history.history, Trained_model_Path + \"/\" + \"model_history\")\n",
    "        print(\"History saving completed...\")\n",
    "        \n",
    "        #save model\n",
    "        print(\"Saving model...\")\n",
    "        model.save(Trained_model_Path + \"/\" + \"test_model1.h5\")\n",
    "        print(\"Model saving completed...\")\n",
    "        \n",
    "        #print(\"history.history.keys: \", history.history.keys())\\\n",
    "        #'''\n",
    "                \n",
    "    def getsource_max_lan(self):\n",
    "        return self.sl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aa3c824"
   },
   "outputs": [],
   "source": [
    "def saveDictionary(dt, file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"wb\")\n",
    "        pickle.dump(dt, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6ddfb54"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"rb\")\n",
    "        dt = pickle.load(a_file)\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20063beb"
   },
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34dcfe7d"
   },
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y-JMBpEFcJWh",
    "psq7nyP0ca_Z"
   ],
   "name": "Main_Colab擴增版.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
