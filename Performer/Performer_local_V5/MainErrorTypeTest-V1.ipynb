{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d94ffe56",
    "outputId": "1249cd6b-bff9-44bc-f32e-212835586c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\W.R_Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performerErrorTypeTest_V1 as tfr\n",
    "import nltk\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'\n",
    "    with open(filename, \"rb\") as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(file_name):\n",
    "    errlist=[]\n",
    "    LBlist=[]\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "    #讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows: \n",
    "            RL=list(row.values())\n",
    "            #print(\"RL[0]: \", type(RL[0]), \"RL[1]: \", type(RL[1]))\n",
    "            RL[1:] = list(map(int, RL[1:]))\n",
    "            errs=RL[1:37]\n",
    "            LB=RL[37:]\n",
    "            errlist.append(errs)\n",
    "            LBlist.append(LB)\n",
    "    return errlist,LBlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'NUM_INT', '>'], [\"<NUM_INT>\"])\n",
    "    replace_sublist(x, ['<', 'NUM_FLOAT', '>'], [\"<NUM_FLOAT>\"])\n",
    "    replace_sublist(x, ['<', 'STRING', '>'], [\"<STRING>\"])\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSentence(x):\t\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        #print(ord(x[i]))\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"ASCII\":\n",
    "            if inp==\"E\":\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  # nltk.word_tokenize(chrs) \n",
    "    return replaceTAGS(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readcode(fname):\n",
    "    with open(fname,encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def listdir_fullpath(d):\n",
    "    return [f for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ec120d95"
   },
   "outputs": [],
   "source": [
    "#save model for training\n",
    "class TestTranslate(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<CR>': 5,\n",
    "            '<NUM_INT>': 6,\n",
    "            '<NUM_FLOAT>': 7,\n",
    "            '<STRING>': 8,\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "    \n",
    "    def test_translate(self):\n",
    "        #print(\"i am here: \" )\n",
    "        #source_file=[]\n",
    "        #set path\n",
    "        model_for_training_path = \"Model-for-training\\ErrorType-Test\\Split-500-reduce-binary-accuracy\\Max-len-1000\"\n",
    "        Trained_model_Path = \"Trained_models\\ErrorType-Test\\Split-500-reduce-binary-accuracy\\Max-len-1000\\Learning-rate-3x0\\weight-1vs1-V1\\Drop-out-0.2\\Epochs-1000\"\n",
    "        \n",
    "        #Set Para\n",
    "        line_block_num = 360 #lbNum\n",
    "        max_javaline_length = 160 #Max number of lines\n",
    "        type_weight = 1\n",
    "        line_weight = 1\n",
    "        learning_rate_value = 0.0001\n",
    "        epochs_value = 1000\n",
    "        #get all txt file in input path\n",
    "        target_max_len = 0\n",
    "        token_num = 0\n",
    "        batch_size_value = 64\n",
    "        \n",
    "        #start training\n",
    "        source_token_dict_name = \"source_token_dict.pickle\"\n",
    "        source_max_len_name = \"source_max_len\"\n",
    "        #load source_token_dict\n",
    "        source_token_dict = loadDictionary(model_for_training_path + \"/\" + source_token_dict_name)\n",
    "        #load source_max_len\n",
    "        source_max_len = loadDictionary(Trained_model_Path + \"/\" + source_max_len_name)\n",
    "        \n",
    "        #Set model para    \n",
    "        model = tfr.get_model(max_input_len=(source_max_len),\n",
    "                              errNum=36,\n",
    "                              token_num=len(source_token_dict),\n",
    "                              embed_dim=128, #32, try 32 or 64\n",
    "                              encoder_num=6, #2 max = 6\n",
    "                              head_num=4,#4\n",
    "                              hidden_dim=512, #128\n",
    "                              dropout_rate=0.05 #0.05\n",
    "                             )\n",
    "        #Set losses\n",
    "        losses = {\"error_feed_forward_output1\": \"binary_crossentropy\"}\n",
    "        #error type weight\n",
    "        lossWeights = {\"error_feed_forward_output1\": type_weight}\n",
    "        metrics = {\"error_feed_forward_output1\": \"binary_accuracy\"}\n",
    "        #metrics = {\"error_feed_forward_output1\": tf.keras.metrics.Accuracy()}\n",
    "        \n",
    "        #set complie para\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss=losses, loss_weights=lossWeights, metrics=metrics)\n",
    "        \n",
    "        #''' <-----traing switch\n",
    "        #Start training\n",
    "        \n",
    "        print(\"Strat training...\")\n",
    "        \n",
    "        #reload model for training \n",
    "        x_model_name = \"x_train_0.npy\"\n",
    "        y0_model_name = \"y_train[0]_0.npy\"\n",
    "        x_val_name = \"x_validation_0.npy\"\n",
    "        y0_val_name = \"y_validation[0]_0.npy\"\n",
    "        x_train_model = loadTestTrainData(model_for_training_path + \"/\" + x_model_name)\n",
    "        x_validation_model = loadTestTrainData(model_for_training_path + \"/\" + x_val_name)\n",
    "        y0_train_model = loadTestTrainData(model_for_training_path + \"/\" + y0_model_name)\n",
    "        y0_validation_model = loadTestTrainData(model_for_training_path + \"/\" + y0_val_name)\n",
    "        \n",
    "        #set check point \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = Trained_model_Path + \"/\" + \"checkpoint_model.h5\",\n",
    "                                                                       monitor = \"val_loss\",\n",
    "                                                                       mode = \"min\",\n",
    "                                                                       save_best_only = True\n",
    "                                                                      )\n",
    "        \n",
    "        history = model.fit(\n",
    "                            x = x_train_model,\n",
    "                            y = [y0_train_model],\n",
    "                            epochs = epochs_value, #100 200 500 3000\n",
    "                            verbose = 2, #set visibility\n",
    "                            callbacks = [model_checkpoint_callback],\n",
    "                            validation_data = (x_validation_model, [y0_validation_model]),\n",
    "                            batch_size = batch_size_value\n",
    "                            )\n",
    "        \n",
    "        \n",
    "        print(\"Model training completed...\")\n",
    "        #save history\n",
    "        print(\"Saving history...\")\n",
    "        saveDictionary(history.history, Trained_model_Path + \"/\" + \"model_history\")\n",
    "        print(\"History saving completed...\")\n",
    "        \n",
    "        #save model\n",
    "        print(\"Saving model...\")\n",
    "        model.save(Trained_model_Path + \"/\" + \"test_model1.h5\")\n",
    "        print(\"Model saving completed...\")\n",
    "        \n",
    "        #print(\"history.history.keys: \", history.history.keys())\\\n",
    "        #'''\n",
    "                \n",
    "    def getsource_max_lan(self):\n",
    "        return self.sl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7aa3c824"
   },
   "outputs": [],
   "source": [
    "def saveDictionary(dt, file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"wb\")\n",
    "        pickle.dump(dt, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "a6ddfb54"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"rb\")\n",
    "        dt = pickle.load(a_file)\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "20063beb"
   },
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "34dcfe7d"
   },
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Warpping...\n"
     ]
    }
   ],
   "source": [
    "x = TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y-JMBpEFcJWh",
    "psq7nyP0ca_Z"
   ],
   "name": "Main_Colab擴增版.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
