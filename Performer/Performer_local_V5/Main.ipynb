{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d94ffe56",
    "outputId": "1249cd6b-bff9-44bc-f32e-212835586c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performer as tfr\n",
    "import nltk\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'\n",
    "    with open(filename, \"rb\") as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "22361e8f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(file_name):\n",
    "    errlist=[]\n",
    "    LBlist=[]\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "    #讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows: \n",
    "            RL=list(row.values())\n",
    "            #print(\"RL[0]: \", type(RL[0]), \"RL[1]: \", type(RL[1]))\n",
    "            RL[1:] = list(map(int, RL[1:]))\n",
    "            errs=RL[1:37]\n",
    "            LB=RL[37:]\n",
    "            errlist.append(errs)\n",
    "            LBlist.append(LB)\n",
    "    return errlist,LBlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b92cf71a"
   },
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1b88909b"
   },
   "outputs": [],
   "source": [
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "abb41dde"
   },
   "outputs": [],
   "source": [
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'NUM_INT', '>'], [\"<NUM_INT>\"])\n",
    "    replace_sublist(x, ['<', 'NUM_FLOAT', '>'], [\"<NUM_FLOAT>\"])\n",
    "    replace_sublist(x, ['<', 'STRING', '>'], [\"<STRING>\"])\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0db5e0b1"
   },
   "outputs": [],
   "source": [
    "def parseSentence(x):\t\n",
    "    tokenizer = RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        #print(ord(x[i]))\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"ASCII\":\n",
    "            if inp==\"E\":\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs)  # nltk.word_tokenize(chrs) \n",
    "    return replaceTAGS(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e88de533"
   },
   "outputs": [],
   "source": [
    "def readcode(fname):\n",
    "    with open(fname,encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def listdir_fullpath(d):\n",
    "    return [f for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ec120d95"
   },
   "outputs": [],
   "source": [
    "#save model for training\n",
    "class TestTranslate(unittest.TestCase):\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<CR>': 5,\n",
    "            '<NUM_INT>': 6,\n",
    "            '<NUM_FLOAT>': 7,\n",
    "            '<STRING>': 8,\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "    \n",
    "    def test_translate(self):\n",
    "        #print(\"i am here: \" )\n",
    "        #source_file=[]\n",
    "        #Set Para\n",
    "        max_javaline_length = 160 #Max number of lines\n",
    "        #set path\n",
    "        Output_Path = \"Trianing\\InputCSV\\Split-500-reduce-binary-accuracy\"\n",
    "        Input_Path = \"Trianing\\InputTxt\\Split-500-reduce-binary-accuracy\"\n",
    "        model_for_training_org_path = \"Model-for-training\\Default\\Split-500-reduce-binary-accuracy\\Max-len-1000\"\n",
    "        model_for_training_path = \"Model-for-training-org\\Default\\Split-500-reduce-binary-accuracy\\Max-len-1000\"\n",
    "        Trained_model_Path = \"Trained_models\\Default\\Split-500-reduce-binary-accuracy\\Max-len-1000\\Learning-rate-3x0\\weight-1vs1\"\n",
    "        source_max_len_name = \"source_max_len\"\n",
    "        type_weight = 1\n",
    "        line_weight = 1\n",
    "        learning_rate_value = 0.0001\n",
    "        #get all txt file in input path\n",
    "        in_path = (glob.glob(Input_Path + \"/**/*.txt\"))\n",
    "        #print(\"in_path: \", in_path)\n",
    "        #cases = listdir_fullpath(Input_Path)\n",
    "        source_max_len = 0\n",
    "        target_max_len = 0\n",
    "        token_num = 0\n",
    "        all_sample_num = 16644 #all sample number\n",
    "        block_num = 16644 #sample num e.g 10000 sample have 10*1000\n",
    "        #Set file cutting size\n",
    "        training_source_max_len = 1000\n",
    "        self.sl = 0\n",
    "        \n",
    "        import math\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version \n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"First loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            #print(\"dirs: \", dirs)\n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])                    \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "                    #print(\"source_tokens length: \", len(source_tokens))\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            #get csv file     \n",
    "            out_path = Output_Path + \"/\" + \"test\" + str(loop)+\".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            \n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err, lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "                #if len(source_tokens)>max_files: break\n",
    "            dd = np.asarray(target_errors)\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]     \n",
    "            \n",
    "            #change source token length\n",
    "            source_tokens2 = []\n",
    "            target_errors2 = []\n",
    "            target_LB2 = []\n",
    "\n",
    "            THRESHOLD_FILE_LEN = training_source_max_len\n",
    "            for i in range(len(source_tokens)):\n",
    "                src = source_tokens[i]\n",
    "                target_error = target_errors[i]\n",
    "                target_LBs = target_LB[i]\n",
    "                if (len(src)<=THRESHOLD_FILE_LEN):# and  len(target)<=THRESHOLD_FILE_LEN):\n",
    "                    source_tokens2.append(src)\n",
    "                    target_errors2.append(target_error)\n",
    "                    target_LB2.append(target_LBs)\n",
    "            source_tokens = source_tokens2\n",
    "            target_errors = target_errors2 #list of intgers, error types\n",
    "            target_LB = target_LB2\n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            \n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            \n",
    "            #output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens] \n",
    "            \n",
    "            self.sl = max(list(map(len, encode_tokens))+[self.sl])\n",
    "            source_max_len = self.sl\n",
    "\n",
    "        #padding here\n",
    "        print(\"source_max_len:\", source_max_len)\n",
    "        saveDictionary(source_max_len, Trained_model_Path + \"/\" + source_max_len_name)\n",
    "        for loop in range(0, math.ceil(all_sample_num/block_num)): #new version\n",
    "        #for loop in range(0, round(336/block_num)): #old version\n",
    "            print(\"Second loop: \", loop)\n",
    "            source_tokens = []\n",
    "            target_errors = []\n",
    "            target_LB = []\n",
    "            if(all_sample_num % block_num == 0):\n",
    "                dirs = block_num\n",
    "            else:\n",
    "                dirs = block_num if loop < all_sample_num // block_num else all_sample_num % block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            for i in range(dirs):\n",
    "                Input_fullpath.append(in_path[loop*block_num + i])\n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "\n",
    "            out_path = Output_Path + \"/\" + \"test\"+ str(loop)+ \".csv\"\n",
    "            Output_fullpath = glob.glob(out_path)\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):\n",
    "                    err, lb = readCSV(f)\n",
    "                    target_errors.append(err)\n",
    "                    target_LB.append(lb)\n",
    "            dd = np.asarray(target_errors)\n",
    "            target_errors = target_errors[0]  \n",
    "            target_LB = target_LB[0]\n",
    "            \n",
    "            #change source token length\n",
    "            source_tokens2 = []\n",
    "            target_errors2 = []\n",
    "            target_LB2 = []\n",
    "\n",
    "            THRESHOLD_FILE_LEN = training_source_max_len\n",
    "            for i in range(len(source_tokens)):\n",
    "                src = source_tokens[i]\n",
    "                target_error = target_errors[i]\n",
    "                target_LBs = target_LB[i]\n",
    "                if (len(src)<=THRESHOLD_FILE_LEN):# and  len(target)<=THRESHOLD_FILE_LEN):\n",
    "                    source_tokens2.append(src)\n",
    "                    target_errors2.append(target_error)\n",
    "                    target_LB2.append(target_LBs)\n",
    "            source_tokens = source_tokens2\n",
    "            target_errors = target_errors2 #list of intgers, error types\n",
    "            target_LB = target_LB2\n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "\n",
    "            # Add special tokens\n",
    "            #encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            encode_tokens = [tokens for tokens in source_tokens]\n",
    "            #print(\"encode_tokens1: \", encode_tokens)\n",
    "            encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_tokens2: \", encode_tokens)\n",
    "            encode_input = [list(map(lambda x: self.source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "            #print(\"encode_input1: \", encode_input)\n",
    "            token_num = len(self.source_token_dict)\n",
    "            #print(\"token num: \", token_num)\n",
    "            #print(token_num)\n",
    "            #print(type(token_num))\n",
    "            #define save path and save dict  \n",
    "            dict_name = \"source_token_dict.pickle\"\n",
    "            #save for org\n",
    "            saveDictionary(self.source_token_dict, model_for_training_org_path + \"/\" + dict_name)\n",
    "            #save for training\n",
    "            saveDictionary(self.source_token_dict, model_for_training_path + \"/\" + dict_name)\n",
    "            #print(\"x.shape\", np.asarray(encode_input).shape)  #x.shape (2,  9)\n",
    "            \n",
    "            #x=[np.array(encode_input * 1)]\n",
    "            #y=[np.array(target_errors * 1),np.array(target_LB * 1)]\n",
    "\n",
    "            #print(\"x.shape\", np.asarray(x).shape)  #x.shape (2, 2048, 9)\n",
    "\n",
    "            ####  Split the data set into train and test_model\n",
    "            x = np.asarray(encode_input)\n",
    "            y = list(zip(np.asarray(target_errors), np.asarray(target_LB)))\n",
    "            print(\"x.shape: \", x.shape)\n",
    "            ynparray = np.asarray(y)\n",
    "            print(\"y.shape: \", ynparray.shape)\n",
    "            #save x y file \n",
    "            print(\"=============Saving x&y file=============\")\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x\" + str(loop) + \".npy\", x)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y\" + str(loop) + \".npy\", ynparray)\n",
    "            print(\"===========File save completed===========\")\n",
    "            #print(\"x.shape: \", x.shape)\n",
    "            #print(\"y length: \", len(y))\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=42)\n",
    "            \n",
    "            y_test = list(zip(*y_test))\n",
    "            y_test[0] = np.asarray(y_test[0])\n",
    "            y_test[1] = np.asarray(y_test[1])\n",
    "            y_test[1] = to_categorical(y_test[1], num_classes = max_javaline_length) \n",
    "            #y_test = list(zip(y_test[0], y_test[1])) \n",
    "            y_test[1] = np.split(y_test[1], indices_or_sections = len(target_LB[0]), axis = 1) \n",
    "            y_test[1] = [np.squeeze(elm, axis = 1) for elm in y_test[1]]           \n",
    "            \n",
    "            #=============================#\n",
    "            x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.1, random_state=42)\n",
    "            \"\"\"\n",
    "            print(\"Split x_train shape: \", (x_train).shape)\n",
    "            print(\"Split x_validation shape: \", (x_validation).shape)\n",
    "            print(\"Split x_train length: \", len(x_train))\n",
    "            print(\"Split x_validation lenght: \", len(x_validation))\n",
    "            \"\"\"\n",
    "            buffer_train_num = len(x_train)\n",
    "            buffer_val_train_num = len(x_validation)\n",
    "            \n",
    "            y_train = list(zip(*y_train))  \n",
    "            y_train[0] = np.asarray(y_train[0])\n",
    "            y_train[1] = np.asarray(y_train[1])\n",
    "\n",
    "            y_train[1] = to_categorical(y_train[1], num_classes=max_javaline_length) #make error type vector to binary matrix\n",
    "            #y_train = list(zip(y_train[0], y_train[1]))\n",
    "\n",
    "            #print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:\" , y_train[0].shape)\n",
    "            #print(\"y_train[1].shape\", y_train[1].shape)\n",
    "            #print(\"???????????: y_train.length \", len(y_train))\n",
    "            #print(\"???????????: y_train.[1] type \", type(y_train[1]))\n",
    "            y_train[1] = np.split(y_train[1], indices_or_sections=len(target_LB[0]), axis=1)\n",
    "            y_train[1] = [np.squeeze(elm, axis = 1) for elm in y_train[1]]\n",
    "            #print(\"after change->len(y_train[1].shape)\", len(y_train[1]) )\n",
    "\n",
    "            y_validation = list(zip(*y_validation))\n",
    "            y_validation[0] = np.asarray(y_validation[0])\n",
    "            y_validation[1] = np.asarray(y_validation[1])\n",
    "            y_validation[1] = to_categorical(y_validation[1], num_classes = max_javaline_length) \n",
    "            #y_test = list(zip(y_test[0], y_test[1])) \n",
    "            y_validation[1] = np.split(y_validation[1], indices_or_sections=len(target_LB[0]), axis=1) \n",
    "            y_validation[1] = [np.squeeze(elm, axis = 1) for elm in y_validation[1]]           \n",
    "            \n",
    "        #''' <----save switch, if you need to create npy model for traing open this\n",
    "            #save org model for training \n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            \n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "            \n",
    "            #=========================================\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"x_validation_\" + str(loop) + \".npy\", x_validation)\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_validation[0]_\" + str(loop) + \".npy\", y_validation[0])\n",
    "            saveTestTrainData(model_for_training_org_path + \"/\" + \"y_validation[1]_\" + str(loop) + \".npy\", y_validation[1])\n",
    "            \n",
    "            #transform x_train\n",
    "            \n",
    "            print(\"org x_train shape: \", (x_train).shape)\n",
    "            print(\"org x_test shape: \", (x_test).shape)\n",
    "            \n",
    "            \n",
    "            #save split model for training\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            \n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "            \n",
    "            #=====================================\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"x_validation_\" + str(loop) + \".npy\", x_validation)\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_validation[0]_\" + str(loop) + \".npy\", y_validation[0])\n",
    "            saveTestTrainData(model_for_training_path + \"/\" + \"y_validation[1]_\" + str(loop) + \".npy\", y_validation[1])\n",
    "        print(\"Training model save successful...\")    \n",
    "        #'''\n",
    "        \n",
    "        \n",
    "        #start training\n",
    "        from Data_buffer import DataGeneratorTrain as DGTrain\n",
    "        from Data_buffer import DataGeneratorValidation as DGValidation\n",
    "        from Data_buffer import DataBuffer as db\n",
    "        from random import randrange\n",
    "        #Set driver path\n",
    "        #source_max_len = 1200 #set max len  #default : 2889\n",
    "        line_block_num = 360 #lbNum\n",
    "        source_token_dict_name = \"source_token_dict.pickle\"\n",
    "        #load source_token_dict\n",
    "        source_token_dict = loadDictionary(model_for_training_path + \"/\" + source_token_dict_name)\n",
    "        #Set model para    \n",
    "        model = tfr.get_model(max_input_len=(source_max_len),\n",
    "                              max_javaline_length=160,\n",
    "                              errNum=36,\n",
    "                              lbNum=line_block_num, #lbNum=len(target_LB[0]), #160\n",
    "                              token_num=len(source_token_dict),\n",
    "                              embed_dim=32, #32, try 32 or 64\n",
    "                              encoder_num=6, #2 max = 6\n",
    "                              head_num=4,#4\n",
    "                              hidden_dim=128, #128\n",
    "                              dropout_rate=0.05 #0.05\n",
    "                             )\n",
    "        #Set losses\n",
    "        losses = {\"error_feed_forward_output1\": \"binary_crossentropy\"}\n",
    "        #error type weight\n",
    "        lossWeights = {\"error_feed_forward_output1\": type_weight}\n",
    "        metrics = {\"error_feed_forward_output1\": \"binary_accuracy\"}\n",
    "        #metrics = {\"error_feed_forward_output1\": tf.keras.metrics.Accuracy()}\n",
    "        \n",
    "        #error line weight\n",
    "        for i in range(line_block_num):\n",
    "            name = \"LNout\" + str(i)\n",
    "            losses[name] = \"categorical_crossentropy\"\n",
    "            lossWeights[name] = line_weight #error_feed_forward_output2[] weight # 100\n",
    "            metrics[name] = tf.keras.metrics.CategoricalAccuracy()\n",
    "        \n",
    "        \n",
    "        #set complie para\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss=losses, loss_weights=lossWeights, metrics=metrics)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #''' <-----traing switch\n",
    "        #Start training\n",
    "        \n",
    "        print(\"Strat training...\")    \n",
    "        #set check point \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = Trained_model_Path + \"/\" + \"checkpoint_model.h5\",\n",
    "                                                                       monitor = \"val_loss\",\n",
    "                                                                       mode = \"min\",\n",
    "                                                                       save_best_only = True\n",
    "                                                                      )\n",
    "        \n",
    "        history = model.fit(\n",
    "                            x = x_train,\n",
    "                            y = [y_train[0]] + y_train[1],\n",
    "                            epochs = 1000, #100 200 500 3000\n",
    "                            verbose = 2, #set visibility\n",
    "                            callbacks = [model_checkpoint_callback],\n",
    "                            validation_split = 0.1,\n",
    "                            #validation_data = (x_validation, [y_validation[0]], y_validation[1]) #-> has issue \n",
    "                            )\n",
    "        \n",
    "        \n",
    "        print(\"Model training completed...\")\n",
    "        #save history\n",
    "        print(\"Saving history...\")\n",
    "        saveDictionary(history.history, Trained_model_Path + \"/\" + \"model_history\")\n",
    "        print(\"History saving completed...\")\n",
    "        \n",
    "        #save model\n",
    "        print(\"Saving model...\")\n",
    "        model.save(Trained_model_Path + \"/\" + \"test_model1.h5\")\n",
    "        print(\"Model saving completed...\")\n",
    "        \n",
    "        #print(\"history.history.keys: \", history.history.keys())\\\n",
    "        #'''\n",
    "                \n",
    "    def getsource_max_lan(self):\n",
    "        return self.sl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7aa3c824"
   },
   "outputs": [],
   "source": [
    "def saveDictionary(dt, file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"wb\")\n",
    "        pickle.dump(dt, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "a6ddfb54"
   },
   "outputs": [],
   "source": [
    "def loadDictionary(file):\n",
    "        import pickle\n",
    "        a_file = open(file, \"rb\")\n",
    "        dt = pickle.load(a_file)\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "20063beb"
   },
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "34dcfe7d"
   },
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First loop:  0\n",
      "source_max_len: 997\n",
      "Second loop:  0\n",
      "x.shape:  (16166, 997)\n",
      "y.shape:  (16166, 2)\n",
      "=============Saving x&y file=============\n",
      "===========File save completed===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org x_train shape:  (13094, 997)\n",
      "org x_test shape:  (1617, 997)\n",
      "Training model save successful...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Start Warpping...\n",
      "Strat training...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1323 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1314 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2825 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3600 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1307 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1268 test_step\n        self.compiled_loss(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:156 __call__\n        return losses_utils.compute_weighted_loss(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:312 compute_weighted_loss\n        losses, _, sample_weight = squeeze_or_expand_dimensions(  # pylint: disable=unbalanced-tuple-unpacking\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:204 squeeze_or_expand_dimensions\n        sample_weight = array_ops.squeeze(sample_weight, [-1])\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:535 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:4471 squeeze\n        return gen_array_ops.squeeze(input, axis, name)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10191 squeeze\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3557 _create_op_internal\n        ret = Operation(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 160 for '{{node binary_crossentropy/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [?,160].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-823d92b62b87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTestTranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-00052e93fe69>\u001b[0m in \u001b[0;36mtest_translate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m                                                                       )\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         history = model.fit(\n\u001b[0m\u001b[0;32m    345\u001b[0m                             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1214\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 753\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    754\u001b[0m             *args, **kwds))\n\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1323 test_function  *\n        return step_function(self, iterator)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1314 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2825 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3600 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1307 run_step  **\n        outputs = model.test_step(data)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1268 test_step\n        self.compiled_loss(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:156 __call__\n        return losses_utils.compute_weighted_loss(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:312 compute_weighted_loss\n        losses, _, sample_weight = squeeze_or_expand_dimensions(  # pylint: disable=unbalanced-tuple-unpacking\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:204 squeeze_or_expand_dimensions\n        sample_weight = array_ops.squeeze(sample_weight, [-1])\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:535 new_func\n        return func(*args, **kwargs)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:4471 squeeze\n        return gen_array_ops.squeeze(input, axis, name)\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10191 squeeze\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3557 _create_op_internal\n        ret = Operation(\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\W.R_Chen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 160 for '{{node binary_crossentropy/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [?,160].\n"
     ]
    }
   ],
   "source": [
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y-JMBpEFcJWh",
    "psq7nyP0ca_Z"
   ],
   "name": "Main_Colab擴增版.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
