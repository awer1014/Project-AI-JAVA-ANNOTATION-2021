{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embed_Dim = \"128\"\n",
    "Head_Num = \"8\"\n",
    "Learning_Rate = \"0.00001\"\n",
    "LossWeights = \"1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#drive_path = 'C:/模型 ver. 2/encoder_num=4, decoder_num=4/embed_dim=' + Embed_Dim + '/head_num=' + Head_Num + '/learning_rate=' + Learning_Rate + '/lossWeights_1-' + LossWeights + '/'\n",
    "drive_path = 'D:/model/'\n",
    "if not os.path.isdir(drive_path):\n",
    "    os.makedirs(drive_path)\n",
    "    \n",
    "save_path = \"D:/save_model/\"    \n",
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a2744\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "from __future__ import unicode_literals\n",
    "import sys\n",
    "sys.path.append('D:/local_training')\n",
    "sys.path.append('D:/local_training/keras_position_wise_feed_forward')\n",
    "sys.path.append('D:/local_training/tensorflow_fast_attention')\n",
    "sys.path.append('D:/local_training/keras_performer')\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_performer import performerVer2 as tfr\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import glob\n",
    "\n",
    "import DataGenerator1 as DG\n",
    "\n",
    "import DataBuffer as db\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_sublist(seq, sublist, start=0):\n",
    "    length = len(sublist)\n",
    "    for index in range(start, len(seq)):\n",
    "        if seq[index:index+length] == sublist:\n",
    "            return index, index+length\n",
    "\n",
    "def replace_sublist(seq, sublist, replacement):\n",
    "    length = len(replacement)\n",
    "    index = 0\n",
    "    for start, end in iter(lambda: find_first_sublist(seq, sublist, index), None):\n",
    "        seq[start:end] = replacement\n",
    "        index = start + length\n",
    "        \n",
    "def replaceTAGS(x):\n",
    "    replace_sublist(x, ['<', 'BOC', '>'], [\"<BOC>\"])\n",
    "    replace_sublist(x, ['<', 'EOC', '>'], [\"<EOC>\"])\n",
    "    replace_sublist(x, ['<', 'BOTM', '>'], [\"<BOTM>\"])\n",
    "    replace_sublist(x, ['<', 'BOT', '>'], [\"<BOT>\"])\n",
    "    replace_sublist(x, ['<', 'EOT', '>'], [\"<EOT>\"])\n",
    "    replace_sublist(x, ['<', 'BOM', '>'], [\"<BOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOM', '>'], [\"<EOM>\"])\n",
    "    replace_sublist(x, ['<', 'EOTM', '>'], [\"<EOTM>\"])\n",
    "    replace_sublist(x, ['<', 'CR', '>'], [\"<CR>\"])\n",
    "    return x\n",
    "\n",
    "def parseSentence(x):\n",
    "    tokenizer =  RegexpTokenizer(r\"[\\w']+|[].,:!?;=+-\\\\*/@#$%^&_(){}~|\\\"[]\")\n",
    "    tokens=[]\n",
    "    state=\"START\"\n",
    "    chrs=\"\"\n",
    "    for i in range(len(x)):\n",
    "        if (ord(x[i])>255):\n",
    "            inp=\"U\"\n",
    "        elif (ord(x[i])>=48 and ord(x[i])<=57):\n",
    "            inp=\"D\"\n",
    "        else:\n",
    "            inp=\"E\"\n",
    "\n",
    "        if state==\"START\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])                \n",
    "            \n",
    "        elif state==\"ASCII\":\t\n",
    "            if inp==\"D\" or inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs += x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs) #nltk.word_tokenize(chrs)\n",
    "                chrs=\"\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "        elif state==\"NUMBER\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\t\t\n",
    "\n",
    "        elif state==\"UNICODE\":\n",
    "            if inp==\"D\":\n",
    "                state=\"NUMBER\"\n",
    "                tokens.append(x[i])\n",
    "            elif inp==\"E\":\n",
    "                state=\"ASCII\"\n",
    "                chrs=x[i]\n",
    "            else:#U\n",
    "                state=\"UNICODE\"\n",
    "                tokens.append(x[i])\n",
    "\n",
    "    if len(chrs)>0:\n",
    "        tokens += tokenizer.tokenize(chrs) #wordpunct_tokenize(chrs) #nltk.word_tokenize(chrs)\n",
    "    return replaceTAGS(tokens)\n",
    "\n",
    "def readcode(fname):\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        return data\n",
    "\n",
    "def outputsplit(txt): #txt: \"<BOTM><BOT>32<EOT><BOM>XXX<EOM><EOTM>....\"\n",
    "    pattern = re.compile(r'<BOTM>(.*?)<EOTM>')\n",
    "    x = re.findall(r\"<BOTM> *<BOT>(.*?)<EOT>(.*?)<EOTM>\", txt, re.DOTALL)\n",
    "    y=list(zip((*x)))#[('32', ...), ('<BOM>XXX<EOM>', ...)]\n",
    "    err_codes = [int(code) for code in y[0]]\n",
    "    return err_codes, y[1] #erro int codes, messages\n",
    "\n",
    "def listdir_fullpath(d):\n",
    "    return [f for f in os.listdir(d)]\n",
    "    \n",
    "def saveMaxLen(filename, data): \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(str(data))\n",
    "        f.close()\n",
    "\n",
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    #print(len(data))\n",
    "    #print(data[0].shape)\n",
    "    #print(data[1].shape)\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)\n",
    "\n",
    "def saveDictionary(dt, file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"wb\")\n",
    "    pickle.dump(dt, a_file)\n",
    "    a_file.close()\n",
    "        \n",
    "def loadTestTrainData(filename): # e.g., 'test.npy'    \n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "        return a\n",
    "\n",
    "def loadDictionary(file):\n",
    "    import pickle\n",
    "    a_file = open(file, \"rb\")\n",
    "    dt = pickle.load(a_file)\n",
    "    return dt\n",
    "\n",
    "def load(model_name):\n",
    "    from keras_performer import performerVer2\n",
    "    from tensorflow import keras\n",
    "    from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "    from keras_pos_embd import TrigPosEmbedding\n",
    "    from tensorflow_fast_attention.fast_attention import  Attention, SelfAttention\n",
    "    from keras_position_wise_feed_forward.feed_forward import FeedForward  \n",
    "\n",
    "    co = performerVer2.get_custom_objects()\n",
    "\n",
    "    model = keras.models.load_model(model_name, custom_objects= co)\n",
    "    s = loadDictionary(drive_path + 'source_token_dict.pickle')\n",
    "    t = loadDictionary(drive_path + 'target_token_dict.pickle')\n",
    "    t_inv = loadDictionary(drive_path + 'target_token_dict_inv.pickle')\n",
    "    return model, s, t, t_inv\n",
    "\n",
    "def saveloss(epochs, train_loss, test_loss):\n",
    "    path = drive_path + 'loss.txt'\n",
    "    with open(path, 'a') as f:\n",
    "        print(epochs, file=f)\n",
    "        f.write('train_loss\\n')\n",
    "        print(\"[loss, error_feed_forward_output1_loss, Decoder-Output_loss] =\", train_loss, file=f)\n",
    "        f.write('test_loss\\n')\n",
    "        print(\"[loss, error_feed_forward_output1_loss, Decoder-Output_loss] =\", test_loss, file=f)\n",
    "        f.write('\\n')\n",
    "        \n",
    "def loadMaxLen(filename):     \n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "    \n",
    "def plotTrainingLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_max_len: 2769\n",
      "target_max_len: 2769\n",
      "loop: 0\n",
      "XXXX:  120120\n",
      "YYYY:  120120\n",
      "ZZZZ:  120120\n",
      "source_token_dict len:  1291\n",
      "target_token_dict len:  1132\n",
      "target_token_dict_inv len:  1132\n",
      "loop: 1\n",
      "XXXX:  120120\n",
      "YYYY:  120120\n",
      "ZZZZ:  120120\n",
      "source_token_dict len:  1291\n",
      "target_token_dict len:  1132\n",
      "target_token_dict_inv len:  1132\n",
      "loop: 2\n",
      "XXXX:  98098\n",
      "YYYY:  98098\n",
      "ZZZZ:  98098\n",
      "source_token_dict len:  1291\n",
      "target_token_dict len:  1132\n",
      "target_token_dict_inv len:  1132\n"
     ]
    }
   ],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "    def __init__(self):\n",
    "        self.source_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<BOM>': 5,\n",
    "            '<EOM>': 6,\n",
    "            '<CR>': 7,\n",
    "        }\n",
    "        \n",
    "        self.target_token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<BOC>': 3,\n",
    "            '<EOC>': 4,\n",
    "            '<BOM>': 5,\n",
    "            '<EOM>': 6,\n",
    "            '<CR>': 7,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_dict, token_list):\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "\n",
    "    def test_translate(self):\n",
    "        Input_Path = \"D:/Augmentation/input\"\n",
    "        Output_Path = \"D:/Augmentation/output\" \n",
    "        cases = listdir_fullpath(Input_Path)\n",
    "        \n",
    "        source_max_len = 0\n",
    "        target_max_len = 0\n",
    "        token_num = 0\n",
    "        block_num = 120\n",
    "        for loop in range(0, round(338/block_num)):\n",
    "            source_tokens = []\n",
    "            target_errors=[]\n",
    "            target_tokens = []     \n",
    "            dirs = block_num if loop < 338//block_num else 338%block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            for i in range(dirs):\n",
    "                in_path = Input_Path + \"/\" + cases[loop + i] + \"/*.txt\"\n",
    "                Input_fullpath += glob.glob(in_path)\n",
    "                \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "        \n",
    "            for i in range(dirs):\n",
    "                out_path = Output_Path + \"/\" + cases[loop + i] + \"/*.txt\"\n",
    "                Output_fullpath += glob.glob(out_path)\n",
    "\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):  \n",
    "                    o1, o2 = outputsplit(readcode(f))#o1: list of error codes\n",
    "                    o2 = \"\".join(o2)\n",
    "                    ps = parseSentence(o2) ##<-----parse messages\n",
    "                    target_errors.append(o1)\n",
    "                    target_tokens.append(ps)\n",
    "            \n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            self._build_token_dict(self.target_token_dict, target_tokens)\n",
    "            target_token_dict_inv = {v: k for k, v in self.target_token_dict.items()}\n",
    "\n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "            output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "            \n",
    "            for i in range(len(encode_tokens)):\n",
    "                if encode_tokens[i] is None or len(encode_tokens[i]) == 0:\n",
    "                    print(\"i:\", i)\n",
    "                         \n",
    "            sl = max(map(len, encode_tokens))\n",
    "            tl = max(map(len, decode_tokens))\n",
    "            source_max_len = max(sl, tl, source_max_len)\n",
    "            saveMaxLen(drive_path + \"source_max_len.txt\", source_max_len)\n",
    "            target_max_len = max(sl, tl, target_max_len)\n",
    "            saveMaxLen(drive_path + \"target_max_len.txt\", target_max_len)\n",
    "         \n",
    "        print(\"source_max_len:\", source_max_len)\n",
    "        print(\"target_max_len:\", target_max_len)\n",
    "        #ready to pad and save data\n",
    "        for loop in range(0, round(338/block_num)):\n",
    "            print(\"loop:\", loop)\n",
    "            source_tokens = []\n",
    "            target_errors=[]\n",
    "            target_tokens = []     \n",
    "            dirs = block_num if loop < 338//block_num else 338%block_num\n",
    "            Input_fullpath = []\n",
    "            Output_fullpath = []\n",
    "            for i in range(dirs):\n",
    "                in_path = Input_Path + \"/\" + cases[loop + i] + \"/*.txt\"\n",
    "                Input_fullpath += glob.glob(in_path)\n",
    "                \n",
    "            for f in Input_fullpath:\n",
    "                if isfile(f):\n",
    "                    source_tokens.append(parseSentence(readcode(f)))\n",
    "        \n",
    "            for i in range(dirs):\n",
    "                out_path = Output_Path + \"/\" + cases[loop + i] + \"/*.txt\"\n",
    "                Output_fullpath += glob.glob(out_path)\n",
    "\n",
    "            for f in Output_fullpath:\n",
    "                if isfile(f):  \n",
    "                    o1, o2 = outputsplit(readcode(f))#o1: list of error codes\n",
    "                    o2 = \"\".join(o2)\n",
    "                    ps = parseSentence(o2) ##<-----parse messages\n",
    "                    target_errors.append(o1)\n",
    "                    target_tokens.append(ps)\n",
    "                           \n",
    "            print(\"XXXX: \" , len(source_tokens))\n",
    "            print(\"YYYY: \" , len(target_errors))\n",
    "            print(\"ZZZZ: \" , len(target_tokens))\n",
    "\n",
    "            # Generate dictionaries\n",
    "            self._build_token_dict(self.source_token_dict, source_tokens)\n",
    "            self._build_token_dict(self.target_token_dict, target_tokens)\n",
    "            target_token_dict_inv = {v: k for k, v in self.target_token_dict.items()}\n",
    "\n",
    "            # Add special tokens\n",
    "            encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "            decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "            output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "            \n",
    "            for i in range(len(encode_tokens)):\n",
    "                if encode_tokens[i] is None or len(encode_tokens[i]) == 0:\n",
    "                    print(\"i:\", i)\n",
    "                    \n",
    "            # Padding\n",
    "            encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "            decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "            output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "            encode_input = [list(map(lambda x: self.source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "            decode_input = [list(map(lambda x: self.target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "            decode_output2 = [list(map(lambda x: [self.target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
    "            \n",
    "            print(\"source_token_dict len: \", len(self.source_token_dict))\n",
    "            print(\"target_token_dict len: \", len(self.target_token_dict))\n",
    "            print(\"target_token_dict_inv len: \", len(target_token_dict_inv))\n",
    "            \n",
    "            saveDictionary(self.source_token_dict, drive_path + 'source_token_dict.pickle')\n",
    "            saveDictionary(self.target_token_dict, drive_path + 'target_token_dict.pickle')\n",
    "            saveDictionary(target_token_dict_inv, drive_path + 'target_token_dict_inv.pickle')            \n",
    "\n",
    "            #print(\"encode_input\", np.asarray(encode_input).shape) #(271, 798)\n",
    "            #print(\"decode_input\", np.asarray(decode_input).shape) #(271, 798)\n",
    "            #print(\"decode_output2\",  np.asarray(decode_output2).shape) #(271, 798, 1)\n",
    "            #target errors: into 0/1 arrays from target_errors\n",
    "            decode_output1 =[ [0]*36 for i in range(len(target_errors))]\n",
    "            for i in range(len(target_errors)):\n",
    "                    codes= target_errors[i]\n",
    "                    for code in codes:  \n",
    "                            decode_output1[i][code-1] = 1\n",
    "            #print(decode_output1)\n",
    "\n",
    "\n",
    "            x=list(zip(np.array(encode_input), np.array(decode_input)))\n",
    "            y=list(zip(np.array(decode_output1), np.array(decode_output2))) #np.array(decode_output2)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            x_train = list(zip(*x_train))\n",
    "            x_train[0] = np.array(x_train[0]) #encode_input\n",
    "            x_train[1] = np.array(x_train[1]) #decode_input\n",
    "            #print(x_train[0].shape)\n",
    "            #print(x_train[1].shape)\n",
    "\n",
    "            y_train = list(zip(*y_train))\n",
    "            y_train[0] = np.array(y_train[0]) #decode_output1\n",
    "            y_train[1] = np.array(y_train[1]) #decode_output2\n",
    "            #print(y_train[0].shape)\n",
    "            #print(y_train[1].shape)\n",
    "\n",
    "            x_test = list(zip(*x_test))\n",
    "            x_test[0] = np.array(x_test[0])\n",
    "            x_test[1] = np.array(x_test[1])\n",
    "\n",
    "            y_test = list(zip(*y_test))\n",
    "            y_test[0] = np.array(y_test[0]) #decode_output1\n",
    "            y_test[1] = np.array(y_test[1]) #decode_output2\n",
    "            #print(y_test[0].shape)\n",
    "            #print(y_test[1].shape)\n",
    "\n",
    "            #x=[np.array(encode_input * 1), np.array(decode_input * 1)] #(2, 271, 798)\n",
    "            #y=np.array(decode_output2 * 1) #(271, 798, 1)\n",
    "\n",
    "            saveTestTrainData(drive_path + \"x_train_\" + str(loop) + \".npy\", x_train)\n",
    "            saveTestTrainData(drive_path + \"x_test_\" + str(loop) + \".npy\", x_test)\n",
    "            saveTestTrainData(drive_path + \"y_train[0]_\" + str(loop) + \".npy\", y_train[0])\n",
    "            saveTestTrainData(drive_path + \"y_train[1]_\" + str(loop) + \".npy\", y_train[1])\n",
    "            saveTestTrainData(drive_path + \"y_test[0]_\" + str(loop) + \".npy\", y_test[0])\n",
    "            saveTestTrainData(drive_path + \"y_test[1]_\" + str(loop) + \".npy\", y_test[1])\n",
    "        \n",
    "            # x_train, x_test: [array, array ]\n",
    "            # y_train, y_test: array               \n",
    "\n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_token_dict len:  1291\n",
      "target_token_dict len:  1132\n",
      "target_token_dict_inv len:  1132\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder-Input (InputLayer)      [(None, 2769)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Token-Embedding (Embedd [(None, 2769, 32), ( 41312       Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, 2769, 32)     0           Encoder-Token-Embedding[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_1 (SelfAttention (None, 2769, 32)     4096        Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 2769, 32)     0           self_attention_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 2769, 32)     0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 2769, 32)     64          Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 2769, 32)     8352        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 2769, 32)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 2769, 32)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 2769, 32)     64          Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_3 (SelfAttention (None, 2769, 32)     4096        Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 2769, 32)     0           self_attention_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Input (InputLayer)      [(None, 2769)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 2769, 32)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Token-Embedding (Embedd [(None, 2769, 32), ( 41312       Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 2769, 32)     64          Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, 2769, 32)     0           Decoder-Token-Embedding[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 2769, 32)     8352        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "self_attention_5 (SelfAttention (None, 2769, 32)     4096        Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 2769, 32)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, 2769, 32)     0           self_attention_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 2769, 32)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, 2769, 32)     0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 2769, 32)     64          Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, 2769, 32)     64          Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 2769, 32)     4096        Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, 2769, 32)     0           attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, 2769, 32)     0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, 2769, 32)     64          Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, 2769, 32)     8352        Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, 2769, 32)     0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, 2769, 32)     0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, 2769, 32)     64          Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_7 (SelfAttention (None, 2769, 32)     4096        Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, 2769, 32)     0           self_attention_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, 2769, 32)     0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, 2769, 32)     64          Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 2769, 32)     4096        Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, 2769, 32)     0           attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, 2769, 32)     0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, 2769, 32)     64          Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, 2769, 32)     8352        Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, 2769, 32)     0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 88608)        0           Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, 2769, 32)     0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          11341952    reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, 2769, 32)     64          Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "error_feed_forward_output1 (Den (None, 36)           4644        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Output (EmbeddingSim)   (None, 2769, 1291)   1291        Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-Token-Embedding[0][1]    \n",
      "==================================================================================================\n",
      "Total params: 11,489,135\n",
      "Trainable params: 11,489,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "call DataGenerator1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a2744\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    def test_translate(self):\n",
    "\n",
    "        source_token_dict = loadDictionary(drive_path + 'source_token_dict.pickle')\n",
    "        target_token_dict = loadDictionary(drive_path + 'target_token_dict.pickle')\n",
    "        target_token_dict_inv = loadDictionary(drive_path + 'target_token_dict_inv.pickle')\n",
    "        \n",
    "        print(\"source_token_dict len: \", len(source_token_dict))\n",
    "        print(\"target_token_dict len: \", len(target_token_dict))\n",
    "        print(\"target_token_dict_inv len: \", len(target_token_dict_inv))\n",
    "        \n",
    "        source_max_len_loaded = loadMaxLen(drive_path + \"source_max_len.txt\")\n",
    "        source_max_len = int(source_max_len_loaded[0])\n",
    "        \n",
    "        target_max_len_loaded = loadMaxLen(drive_path + \"target_max_len.txt\")\n",
    "        target_max_len = int(target_max_len_loaded[0])\n",
    "\n",
    "        # Build & fit model      \n",
    "        model = tfr.get_model(\n",
    "            max_input_len=(source_max_len, target_max_len),\n",
    "            token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "            embed_dim=32,\n",
    "            encoder_num=2,\n",
    "            decoder_num=2,\n",
    "            head_num=4,\n",
    "            hidden_dim=128,\n",
    "            dropout_rate=0.05,\n",
    "            use_same_embed=False  # Use different embeddings for different languages\n",
    "        )\n",
    "\n",
    "        losses = {\n",
    "                \"error_feed_forward_output1\": \"binary_crossentropy\",\n",
    "                \"Decoder-Output\": \"sparse_categorical_crossentropy\",\n",
    "        }\n",
    "        lossWeights = {\"error_feed_forward_output1\": 1.0, \"Decoder-Output\": 1000.0}\n",
    "        model.compile(optimizer=Adam(learning_rate=0.00001), loss=losses, loss_weights=lossWeights) \n",
    "        model.summary()\n",
    "        \n",
    "        #for output \n",
    "        output_buffer_params = { \n",
    "            \"data_path\": [\"D:\\\\model\\\\\", \"D:\\\\model\\\\\"],\n",
    "            \"data_number\":[270670, 270670],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [96096, 96096]\n",
    "            }\n",
    "        #for input\n",
    "        input_buffer_params = {\n",
    "            \"data_path\": [\"D:\\\\model\\\\\", \"D:\\\\model\\\\\"],\n",
    "            \"data_number\":[270670, 270670],\n",
    "            \"data_type\": [int, int],\n",
    "            \"block_size\": [96096, 96096]\n",
    "            }\n",
    "        \n",
    "        # Generators\n",
    "        print(\"call DataGenerator1...\")\n",
    "        training_generator = DG.DataGenerator1(input_buffer_params, output_buffer_params,\n",
    "                                               [list(range(270670)), list(range(270670))]\n",
    "                                              )\n",
    "        \n",
    "        #validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
    "        \n",
    "        # Train model on dataset\n",
    "        history = model.fit_generator(\n",
    "            generator=training_generator,        \n",
    "            epochs=2,      \n",
    "        )\n",
    "        \n",
    "        plotTrainingLoss(history)\n",
    "        \n",
    "        model.save(save_path + \"test_model_2.h5\")   \n",
    "        print(\"model save end...\")\n",
    "        \n",
    "        print(\" \")\n",
    "        print(\"train_loss\")\n",
    "        train_loss = model.evaluate(x_train, y_train)\n",
    "        print(\"test_loss\")\n",
    "        test_loss = model.evaluate(x_test, y_test) \n",
    "        saveloss(2, train_loss, test_loss)\n",
    "\n",
    "x=TestTranslate()\n",
    "x.test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
