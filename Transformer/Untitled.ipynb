{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "from keras_transformer import transformer as tfr\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def parseSentence1(x):\t#甲班用\n",
    "\ttokens=[]\n",
    "\tstate=\"START\"\n",
    "\tfor i in range(len(x)):\n",
    "\t\t#print(ord(x[i]))\n",
    "\t\tif (ord(x[i])>255):\n",
    "\t\t\tinp=\"U\"\n",
    "\t\telif (ord(x[i])>=48 and ord(x[i])<=57)\n",
    "            inp=\"D\"\n",
    "        else:\n",
    "\t\t\tinp=\"E\"\n",
    "\t\n",
    "\t\tif state==\"START\":\n",
    "\t\t\tif inp==\"E\":\n",
    "\t\t\t\tstate=\"ASCII\"\n",
    "\t\t\t\tchrs=x[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstate=\"UNICODE\"\n",
    "\t\t\t\ttokens.append(x[i])\n",
    "\t\t\t\n",
    "\t\telif state==\"ASCII\":\n",
    "\t\t\tif inp==\"E\":\n",
    "\t\t\t\tchrs += x[i]\n",
    "\t\t\telse:#U\n",
    "\t\t\t\tstate=\"UNICODE\"\n",
    "\t\t\t\ttokens += nltk.word_tokenize(chrs)\n",
    "\t\t\t\tchrs=\"\"\n",
    "\t\t\t\ttokens.append(x[i])\n",
    "\t\n",
    "\t\telif state==\"UNICODE\":\n",
    "\t\t\tif inp==\"E\":\n",
    "\t\t\t\tstate=\"ASCII\"\n",
    "\t\t\t\tchrs=x[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstate=\"UNICODE\"\n",
    "\t\t\t\ttokens.append(x[i])\n",
    "\tif len(chrs)>0:\n",
    "\t\ttokens += nltk.word_tokenize(chrs) \n",
    "\treturn tokens\n",
    "def parseSentence2(x):\t#乙班用\n",
    "\ttokens=[]\n",
    "\tstate=\"START\"\n",
    "\tfor i in range(len(x)):\n",
    "\t\t#print(ord(x[i]))\n",
    "\t\tif (ord(x[i])>255):\n",
    "\t\t\tinp=\"U\"\n",
    "        else:\n",
    "\t\t\tinp=\"E\"\n",
    "\t\n",
    "\t\tif state==\"START\":\n",
    "\t\t\tif inp==\"E\":\n",
    "\t\t\t\tstate=\"ASCII\"\n",
    "\t\t\t\tchrs=x[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstate=\"UNICODE\"\n",
    "\t\t\t\ttokens.append(x[i])\n",
    "\t\t\t\n",
    "\t\telif state==\"ASCII\":\n",
    "\t\t\tif inp==\"E\":\n",
    "\t\t\t\tchrs += x[i]\n",
    "\t\t\telse:#U\n",
    "\t\t\t\tstate=\"UNICODE\"\n",
    "\t\t\t\ttokens += nltk.word_tokenize(chrs)\n",
    "\t\t\t\tchrs=\"\"\n",
    "\t\t\t\ttokens.append(x[i])\n",
    "\t\n",
    "\t\telif state==\"UNICODE\":\n",
    "\t\t\tif inp==\"E\":\n",
    "\t\t\t\tstate=\"ASCII\"\n",
    "\t\t\t\tchrs=x[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstate=\"UNICODE\"\n",
    "\t\t\t\ttokens.append(x[i])\n",
    "\tif len(chrs)>0:\n",
    "\t\ttokens += nltk.word_tokenize(chrs) \n",
    "\treturn tokens\n",
    "   \n",
    "def readcode(fname):\n",
    "    with open(fname) as f:\n",
    "        data = f.read()\n",
    "        return data\n",
    "        \n",
    "class TestTranslate(unittest.TestCase):\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_token_dict(token_list):\n",
    "        token_dict = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "        }\n",
    "        for tokens in token_list:\n",
    "            for token in tokens:\n",
    "                if token not in token_dict:\n",
    "                    token_dict[token] = len(token_dict)\n",
    "        return token_dict\n",
    "\n",
    "    def test_translate(self):\n",
    "        source_tokens = [\n",
    "            parseSentence2(readcode('codes/newTest0.java')),\n",
    "            parseSentence2(readcode('codes/code 3.txt')),\n",
    "        ]\n",
    "        target_tokens = [\n",
    "            parseSentence2(readcode('labels/label1.txt')),\n",
    "            parseSentence2(readcode('labels/label2.txt')),\n",
    "        ]\n",
    "\n",
    "        # Generate dictionaries\n",
    "        source_token_dict = self._build_token_dict(source_tokens)\n",
    "        target_token_dict = self._build_token_dict(target_tokens)\n",
    "        target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "\n",
    "        # Add special tokens\n",
    "        encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "        decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "        output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "\n",
    "        # Padding\n",
    "        source_max_len = max(map(len, encode_tokens))\n",
    "        target_max_len = max(map(len, decode_tokens))\n",
    "\n",
    "        encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "        decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "        output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "        encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "        decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "        decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
    "\n",
    "        # Build & fit model\n",
    "        model = tfr.get_model(\n",
    "            token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "            embed_dim=32,\n",
    "            encoder_num=2,\n",
    "            decoder_num=2,\n",
    "            head_num=4,\n",
    "            hidden_dim=128,\n",
    "            dropout_rate=0.05,\n",
    "            use_same_embed=False,  # Use different embeddings for different languages\n",
    "        )\n",
    "        losses = {\n",
    "                \"error_feed_forward_output\": \"binary_crossentropy\",\n",
    "                \"Decoder-Output\": \"sparse_categorical_crossentropy\",\n",
    "        }\n",
    "        lossWeights = {\"error_feed_forward_output-Norm\": 1.0, \"Decoder-Output\": 1.0}\n",
    "        model.compile('adam', loss=losses, loss_weights=lossWeights)\n",
    "        model.summary()\n",
    "        model.fit(\n",
    "            x=[np.array(encode_input * 1024), np.array(decode_input * 1024)],\n",
    "            y=np.array(decode_output * 1024),\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        decoded = tfr.decode(\n",
    "            model,\n",
    "            encode_input,\n",
    "            start_token=target_token_dict['<START>'],\n",
    "            end_token=target_token_dict['<END>'],\n",
    "            pad_token=target_token_dict['<PAD>'],\n",
    "        )\n",
    "        #for i in range(len(encode_input)):\n",
    "        predicted = ''.join(map(lambda x: target_token_dict_inv[x], decoded[0][1:-1]))\n",
    "        print(''.join(target_tokens[0]), predicted)\n",
    "        predicted = ''.join(map(lambda x: target_token_dict_inv[x], decoded[1][1:-1]))\n",
    "        print(''.join(target_tokens[1]), predicted)\n",
    "x=TestTranslate()\n",
    "x.test_translate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
