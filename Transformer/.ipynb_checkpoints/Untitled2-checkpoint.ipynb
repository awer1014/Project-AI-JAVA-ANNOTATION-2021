{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-pos-embd in c:\\users\\danny\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: Keras in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-pos-embd) (2.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-pos-embd) (1.18.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-pos-embd) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-pos-embd) (1.5.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-pos-embd) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\danny\\anaconda3\\lib\\site-packages (from h5py->Keras->keras-pos-embd) (1.15.0)\n",
      "Requirement already satisfied: keras-multi-head in c:\\users\\danny\\anaconda3\\lib\\site-packages (0.27.0)\n",
      "Requirement already satisfied: keras-self-attention==0.46.0 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-multi-head) (0.46.0)\n",
      "Requirement already satisfied: Keras in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-multi-head) (2.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-multi-head) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-multi-head) (1.5.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-multi-head) (5.3.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-multi-head) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\danny\\anaconda3\\lib\\site-packages (from h5py->Keras->keras-multi-head) (1.15.0)\n",
      "Requirement already satisfied: keras-layer-normalization in c:\\users\\danny\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-layer-normalization) (1.18.5)\n",
      "Requirement already satisfied: Keras in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-layer-normalization) (2.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-layer-normalization) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-layer-normalization) (1.5.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-layer-normalization) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\danny\\anaconda3\\lib\\site-packages (from h5py->Keras->keras-layer-normalization) (1.15.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward in c:\\users\\danny\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-position-wise-feed-forward) (1.18.5)\n",
      "Requirement already satisfied: Keras in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-position-wise-feed-forward) (2.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-position-wise-feed-forward) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-position-wise-feed-forward) (1.5.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-position-wise-feed-forward) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\danny\\anaconda3\\lib\\site-packages (from h5py->Keras->keras-position-wise-feed-forward) (1.15.0)\n",
      "Requirement already satisfied: keras-embed-sim in c:\\users\\danny\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-embed-sim) (1.18.5)\n",
      "Requirement already satisfied: Keras in c:\\users\\danny\\anaconda3\\lib\\site-packages (from keras-embed-sim) (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-embed-sim) (1.5.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-embed-sim) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danny\\anaconda3\\lib\\site-packages (from Keras->keras-embed-sim) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\danny\\anaconda3\\lib\\site-packages (from h5py->Keras->keras-embed-sim) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras-pos-embd\n",
    "!pip install keras-multi-head\n",
    "!pip install keras-layer-normalization\n",
    "!pip install keras-position-wise-feed-forward\n",
    "!pip install keras-embed-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 1398,
     "status": "error",
     "timestamp": 1620185198154,
     "user": {
      "displayName": "07360106 07360106",
      "photoUrl": "",
      "userId": "01051155957349821809"
     },
     "user_tz": -480
    },
    "id": "jh2eKM3CFUAE",
    "outputId": "2892e501-7246-425b-adf8-21c05ed76df6"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f7d1e597f9da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_layer_normalization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_multi_head\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_position_wise_feed_forward\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_pos_embd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrigPosEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_embed_sim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbeddingRet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbeddingSim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_layer_normalization\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlayer_normalization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'0.14.0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_layer_normalization\\layer_normalization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_layer_normalization\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_position_wise_feed_forward import FeedForward\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "from .backend import keras\n",
    "from .gelu import gelu\n",
    " \n",
    " \n",
    "__all__ = [\n",
    "    'get_custom_objects', 'get_encoders', 'get_decoders', 'get_model', 'decode',\n",
    "    'attention_builder', 'feed_forward_builder', 'get_encoder_component', 'get_decoder_component',\n",
    "]\n",
    " \n",
    " \n",
    "def get_custom_objects():\n",
    "    return {\n",
    "        'gelu': gelu,\n",
    "        'LayerNormalization': LayerNormalization,\n",
    "        'MultiHeadAttention': MultiHeadAttention,\n",
    "        'FeedForward': FeedForward,\n",
    "        'TrigPosEmbedding': TrigPosEmbedding,\n",
    "        'EmbeddingRet': EmbeddingRet,\n",
    "        'EmbeddingSim': EmbeddingSim,\n",
    "    }\n",
    " \n",
    " \n",
    "def _wrap_layer(name,\n",
    "                input_layer,\n",
    "                build_func,\n",
    "                dropout_rate=0.0,\n",
    "                trainable=True):\n",
    "    \"\"\"Wrap layers with residual, normalization and dropout.\n",
    " \n",
    "    :param name: Prefix of names for internal layers.\n",
    "    :param input_layer: Input layer.\n",
    "    :param build_func: A callable that takes the input tensor and generates the output tensor.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :return: Output layer.\n",
    "    \"\"\"\n",
    "    build_output = build_func(input_layer)\n",
    "    if dropout_rate > 0.0:\n",
    "        dropout_layer = keras.layers.Dropout(\n",
    "            rate=dropout_rate,\n",
    "            name='%s-Dropout' % name,\n",
    "        )(build_output)\n",
    "    else:\n",
    "        dropout_layer = build_output\n",
    "    if isinstance(input_layer, list):\n",
    "        input_layer = input_layer[0]\n",
    "    add_layer = keras.layers.Add(name='%s-Add' % name)([input_layer, dropout_layer])\n",
    "    normal_layer = LayerNormalization(\n",
    "        trainable=trainable,\n",
    "        name='%s-Norm' % name,\n",
    "    )(add_layer)\n",
    "    return normal_layer\n",
    " \n",
    " \n",
    "def attention_builder(name,\n",
    "                      head_num,\n",
    "                      activation,\n",
    "                      history_only,\n",
    "                      trainable=True):\n",
    "    \"\"\"Get multi-head self-attention builder.\n",
    " \n",
    "    :param name: Prefix of names for internal layers.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param activation: Activation for multi-head self-attention.\n",
    "    :param history_only: Only use history data.\n",
    "    :param trainable: Whether the layer is trainable.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def _attention_builder(x):\n",
    "        return MultiHeadAttention(\n",
    "            head_num=head_num,\n",
    "            activation=activation,\n",
    "            history_only=history_only,\n",
    "            trainable=trainable,\n",
    "            name=name,\n",
    "        )(x)\n",
    "    return _attention_builder\n",
    " \n",
    " \n",
    "def feed_forward_builder(name,\n",
    "                         hidden_dim,\n",
    "                         activation,\n",
    "                         trainable=True):\n",
    "    \"\"\"Get position-wise feed-forward layer builder.\n",
    " \n",
    "    :param name: Prefix of names for internal layers.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param activation: Activation for feed-forward layer.\n",
    "    :param trainable: Whether the layer is trainable.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def _feed_forward_builder(x):\n",
    "        return FeedForward(\n",
    "            units=hidden_dim,\n",
    "            activation=activation,\n",
    "            trainable=trainable,\n",
    "            name=name,\n",
    "        )(x)\n",
    "    return _feed_forward_builder\n",
    " \n",
    " \n",
    "def get_encoder_component(name,\n",
    "                          input_layer,\n",
    "                          head_num,\n",
    "                          hidden_dim,\n",
    "                          attention_activation=None,\n",
    "                          feed_forward_activation=gelu,\n",
    "                          dropout_rate=0.0,\n",
    "                          trainable=True,):\n",
    "    \"\"\"Multi-head self-attention and feed-forward layer.\n",
    " \n",
    "    :param name: Prefix of names for internal layers.\n",
    "    :param input_layer: Input layer.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :return: Output layer.\n",
    "    \"\"\"\n",
    "    attention_name = '%s-MultiHeadSelfAttention' % name\n",
    "    feed_forward_name = '%s-FeedForward' % name\n",
    "    attention_layer = _wrap_layer(\n",
    "        name=attention_name,\n",
    "        input_layer=input_layer,\n",
    "        build_func=attention_builder(\n",
    "            name=attention_name,\n",
    "            head_num=head_num,\n",
    "            activation=attention_activation,\n",
    "            history_only=False,\n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    feed_forward_layer = _wrap_layer(\n",
    "        name=feed_forward_name,\n",
    "        input_layer=attention_layer,\n",
    "        build_func=feed_forward_builder(\n",
    "            name=feed_forward_name,\n",
    "            hidden_dim=hidden_dim,\n",
    "            activation=feed_forward_activation,\n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    return feed_forward_layer\n",
    " \n",
    " \n",
    "def get_decoder_component(name,\n",
    "                          input_layer,\n",
    "                          encoded_layer,\n",
    "                          head_num,\n",
    "                          hidden_dim,\n",
    "                          attention_activation=None,\n",
    "                          feed_forward_activation=gelu,\n",
    "                          dropout_rate=0.0,\n",
    "                          trainable=True):\n",
    "    \"\"\"Multi-head self-attention, multi-head query attention and feed-forward layer.\n",
    " \n",
    "    :param name: Prefix of names for internal layers.\n",
    "    :param input_layer: Input layer.\n",
    "    :param encoded_layer: Encoded layer from encoder.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :return: Output layer.\n",
    "    \"\"\"\n",
    "    self_attention_name = '%s-MultiHeadSelfAttention' % name\n",
    "    query_attention_name = '%s-MultiHeadQueryAttention' % name\n",
    "    feed_forward_name = '%s-FeedForward' % name\n",
    "    self_attention_layer = _wrap_layer(\n",
    "        name=self_attention_name,\n",
    "        input_layer=input_layer,\n",
    "        build_func=attention_builder(\n",
    "            name=self_attention_name,\n",
    "            head_num=head_num,\n",
    "            activation=attention_activation,\n",
    "            history_only=True,\n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    query_attention_layer = _wrap_layer(\n",
    "        name=query_attention_name,\n",
    "        input_layer=[self_attention_layer, encoded_layer, encoded_layer],\n",
    "        build_func=attention_builder(\n",
    "            name=query_attention_name,\n",
    "            head_num=head_num,\n",
    "            activation=attention_activation,\n",
    "            history_only=False,\n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    feed_forward_layer = _wrap_layer(\n",
    "        name=feed_forward_name,\n",
    "        input_layer=query_attention_layer,\n",
    "        build_func=feed_forward_builder(\n",
    "            name=feed_forward_name,\n",
    "            hidden_dim=hidden_dim,\n",
    "            activation=feed_forward_activation,\n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    return feed_forward_layer\n",
    " \n",
    " \n",
    "def get_encoders(encoder_num,\n",
    "                 input_layer,\n",
    "                 head_num,\n",
    "                 hidden_dim,\n",
    "                 attention_activation=None,\n",
    "                 feed_forward_activation=gelu,\n",
    "                 dropout_rate=0.0,\n",
    "                 trainable=True):\n",
    "    \"\"\"Get encoders.\n",
    " \n",
    "    :param encoder_num: Number of encoder components.\n",
    "    :param input_layer: Input layer.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :return: Output layer.\n",
    "    \"\"\"\n",
    "    last_layer = input_layer\n",
    "    for i in range(encoder_num):\n",
    "        last_layer = get_encoder_component(\n",
    "            name='Encoder-%d' % (i + 1),\n",
    "            input_layer=last_layer,\n",
    "            head_num=head_num,\n",
    "            hidden_dim=hidden_dim,\n",
    "            attention_activation=attention_activation,\n",
    "            feed_forward_activation=feed_forward_activation,\n",
    "            dropout_rate=dropout_rate,\n",
    "            trainable=trainable,\n",
    "        )\n",
    "    return last_layer\n",
    " \n",
    " \n",
    "def get_decoders(decoder_num,\n",
    "                 input_layer,\n",
    "                 encoded_layer,\n",
    "                 head_num,\n",
    "                 hidden_dim,\n",
    "                 attention_activation=None,\n",
    "                 feed_forward_activation=gelu,\n",
    "                 dropout_rate=0.0,\n",
    "                 trainable=True):\n",
    "    \"\"\"Get decoders.\n",
    " \n",
    "    :param decoder_num: Number of decoder components.\n",
    "    :param input_layer: Input layer.\n",
    "    :param encoded_layer: Encoded layer from encoder.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :return: Output layer.\n",
    "    \"\"\"\n",
    "    last_layer = input_layer\n",
    "    for i in range(decoder_num):\n",
    "        last_layer = get_decoder_component(\n",
    "            name='Decoder-%d' % (i + 1),\n",
    "            input_layer=last_layer,\n",
    "            encoded_layer=encoded_layer,\n",
    "            head_num=head_num,\n",
    "            hidden_dim=hidden_dim,\n",
    "            attention_activation=attention_activation,\n",
    "            feed_forward_activation=feed_forward_activation,\n",
    "            dropout_rate=dropout_rate,\n",
    "            trainable=trainable,\n",
    "        )\n",
    "    return last_layer\n",
    " \n",
    "#張小姐游先生請看此處\n",
    "def get_model(token_num,\n",
    "              embed_dim,\n",
    "              encoder_num,\n",
    "              decoder_num,\n",
    "              head_num,\n",
    "              hidden_dim,\n",
    "              attention_activation=None,\n",
    "              feed_forward_activation=gelu,\n",
    "              dropout_rate=0.0,\n",
    "              use_same_embed=True,\n",
    "              embed_weights=None,\n",
    "              embed_trainable=None,\n",
    "              trainable=True):\n",
    "    \"\"\"Get full model without compilation.\n",
    " \n",
    "    :param token_num: Number of distinct tokens.\n",
    "    :param embed_dim: Dimension of token embedding.\n",
    "    :param encoder_num: Number of encoder components.\n",
    "    :param decoder_num: Number of decoder components.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param use_same_embed: Whether to use the same token embedding layer. `token_num`, `embed_weights` and\n",
    "                           `embed_trainable` should be lists of two elements if it is False.\n",
    "    :param embed_weights: Initial weights of token embedding.\n",
    "    :param embed_trainable: Whether the token embedding is trainable. It will automatically set to False if the given\n",
    "                            value is None when embedding weights has been provided.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :return: Keras model.\n",
    "    \"\"\"\n",
    "    if not isinstance(token_num, list):\n",
    "        token_num = [token_num, token_num]\n",
    "    encoder_token_num, decoder_token_num = token_num\n",
    " \n",
    "    if not isinstance(embed_weights, list):\n",
    "        embed_weights = [embed_weights, embed_weights]\n",
    "    encoder_embed_weights, decoder_embed_weights = embed_weights\n",
    "    if encoder_embed_weights is not None:\n",
    "        encoder_embed_weights = [encoder_embed_weights]\n",
    "    if decoder_embed_weights is not None:\n",
    "        decoder_embed_weights = [decoder_embed_weights]\n",
    " \n",
    "    if not isinstance(embed_trainable, list):\n",
    "        embed_trainable = [embed_trainable, embed_trainable]\n",
    "    encoder_embed_trainable, decoder_embed_trainable = embed_trainable\n",
    "    if encoder_embed_trainable is None:\n",
    "        encoder_embed_trainable = encoder_embed_weights is None\n",
    "    if decoder_embed_trainable is None:\n",
    "        decoder_embed_trainable = decoder_embed_weights is None\n",
    " \n",
    "    if use_same_embed:\n",
    "        encoder_embed_layer = decoder_embed_layer = EmbeddingRet(\n",
    "            input_dim=encoder_token_num,\n",
    "            output_dim=embed_dim,\n",
    "            mask_zero=True,\n",
    "            weights=encoder_embed_weights,\n",
    "            trainable=encoder_embed_trainable,\n",
    "            name='Token-Embedding',\n",
    "        )\n",
    "    else:\n",
    "        encoder_embed_layer = EmbeddingRet(\n",
    "            input_dim=encoder_token_num,\n",
    "            output_dim=embed_dim,\n",
    "            mask_zero=True,\n",
    "            weights=encoder_embed_weights,\n",
    "            trainable=encoder_embed_trainable,\n",
    "            name='Encoder-Token-Embedding',\n",
    "        )\n",
    "        decoder_embed_layer = EmbeddingRet(\n",
    "            input_dim=decoder_token_num,\n",
    "            output_dim=embed_dim,\n",
    "            mask_zero=True,\n",
    "            weights=decoder_embed_weights,\n",
    "            trainable=decoder_embed_trainable,\n",
    "            name='Decoder-Token-Embedding',\n",
    "        )\n",
    "    encoder_input = keras.layers.Input(shape=(None,), name='Encoder-Input')\n",
    "    encoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-Embedding',\n",
    "    )(encoder_embed_layer(encoder_input)[0])\n",
    "    encoded_layer = get_encoders(#The final encoder\n",
    "        encoder_num=encoder_num,\n",
    "        input_layer=encoder_embed,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    #add error classification mlp network\n",
    "    error_feed_forward_layer1 = _wrap_layer(\n",
    "        name=\"error_feed_forward_layer1\",\n",
    "        input_layer=encoded_layer,\n",
    "        build_func=feed_forward_builder(\n",
    "            name=\"error_feed_forward_layer1\",\n",
    "            hidden_dim=hidden_dim,\n",
    "            activation=\"relu\", \n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    error_feed_forward_output = _wrap_layer(\n",
    "        name=\"error_feed_forward_output\",\n",
    "        input_layer=error_feed_forward_layer1,\n",
    "        build_func=feed_forward_builder(\n",
    "            name=\"error_feed_forward_output\",\n",
    "            hidden_dim=36,\n",
    "            activation=\"sigmoid\",\n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    #分類器2\n",
    "    error_feed_forward_layer2 = _wrap_layer(\n",
    "        name=\"error_feed_forward_layer2\",\n",
    "        input_layer=encoded_layer,\n",
    "        build_func=feed_forward_builder(\n",
    "            name=\"error_feed_forward_layer2\",\n",
    "            hidden_dim=hidden_dim,\n",
    "            activation=\"relu\", \n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    error_feed_forward_output = _wrap_layer(\n",
    "        name=\"error_feed_forward_output\",\n",
    "        input_layer=error_feed_forward_layer2,\n",
    "        build_func=feed_forward_builder(\n",
    "            name=\"error_feed_forward_output\",\n",
    "            hidden_dim=60, #段落數量\n",
    "            activation=\"sigmoid\",\n",
    "            trainable=trainable,\n",
    "        ),\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    \n",
    "######################################此處可免\n",
    "    \"\"\"\n",
    "    decoder_input = keras.layers.Input(shape=(None,), name='Decoder-Input')\n",
    "    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n",
    "    decoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Decoder-Embedding',\n",
    "    )(decoder_embed)\n",
    "    decoded_layer = get_decoders(\n",
    "        decoder_num=decoder_num,\n",
    "        input_layer=decoder_embed,\n",
    "        encoded_layer=encoded_layer,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    output_layer = EmbeddingSim(\n",
    "        trainable=trainable,\n",
    "        name='Decoder-Output',\n",
    "    )([decoded_layer, decoder_embed_weights])\n",
    "    \"\"\"\n",
    "    return keras.models.Model(inputs=[encoder_input, decoder_input], outputs=[error_feed_forward_output, output_layer])\n",
    " \n",
    " \n",
    "def _get_max_suffix_repeat_times(tokens, max_len):\n",
    "    detect_len = min(max_len, len(tokens))\n",
    "    next = [-1] * detect_len\n",
    "    k = -1\n",
    "    for i in range(1, detect_len):\n",
    "        while k >= 0 and tokens[len(tokens) - i - 1] != tokens[len(tokens) - k - 2]:\n",
    "            k = next[k]\n",
    "        if tokens[len(tokens) - i - 1] == tokens[len(tokens) - k - 2]:\n",
    "            k += 1\n",
    "        next[i] = k\n",
    "    max_repeat = 1\n",
    "    for i in range(2, detect_len):\n",
    "        if next[i] >= 0 and (i + 1) % (i - next[i]) == 0:\n",
    "            max_repeat = max(max_repeat, (i + 1) // (i - next[i]))\n",
    "    return max_repeat\n",
    " \n",
    " \n",
    "def decode(model,\n",
    "           tokens,\n",
    "           start_token,\n",
    "           end_token,\n",
    "           pad_token,\n",
    "           top_k=1,\n",
    "           temperature=1.0,\n",
    "           max_len=10000,\n",
    "           max_repeat=10,\n",
    "           max_repeat_block=10):\n",
    "    \"\"\"Decode with the given model and input tokens.\n",
    " \n",
    "    :param model: The trained model.\n",
    "    :param tokens: The input tokens of encoder.\n",
    "    :param start_token: The token that represents the start of a sentence.\n",
    "    :param end_token: The token that represents the end of a sentence.\n",
    "    :param pad_token: The token that represents padding.\n",
    "    :param top_k: Choose the last token from top K.\n",
    "    :param temperature: Randomness in boltzmann distribution.\n",
    "    :param max_len: Maximum length of decoded list.\n",
    "    :param max_repeat: Maximum number of repeating blocks.\n",
    "    :param max_repeat_block: Maximum length of the repeating block.\n",
    "    :return: Decoded tokens.\n",
    "    \"\"\"\n",
    "    is_single = not isinstance(tokens[0], list)\n",
    "    if is_single:\n",
    "        tokens = [tokens]\n",
    "    batch_size = len(tokens)\n",
    "    decoder_inputs = [[start_token] for _ in range(batch_size)]\n",
    "    outputs = [None for _ in range(batch_size)]\n",
    "    output_len = 1\n",
    "    while len(list(filter(lambda x: x is None, outputs))) > 0:\n",
    "        output_len += 1\n",
    "        batch_inputs, batch_outputs = [], []\n",
    "        max_input_len = 0\n",
    "        index_map = {}\n",
    "        for i in range(batch_size):\n",
    "            if outputs[i] is None:\n",
    "                index_map[len(batch_inputs)] = i\n",
    "                batch_inputs.append(tokens[i][:])\n",
    "                batch_outputs.append(decoder_inputs[i])\n",
    "                max_input_len = max(max_input_len, len(tokens[i]))\n",
    "        for i in range(len(batch_inputs)):\n",
    "            batch_inputs[i] += [pad_token] * (max_input_len - len(batch_inputs[i]))\n",
    "        predicts = model.predict([np.array(batch_inputs), np.array(batch_outputs)])\n",
    "        for i in range(len(predicts)):\n",
    "            if top_k == 1:\n",
    "                last_token = predicts[i][-1].argmax(axis=-1)\n",
    "            else:\n",
    "                probs = [(prob, j) for j, prob in enumerate(predicts[i][-1])]\n",
    "                probs.sort(reverse=True)\n",
    "                probs = probs[:top_k]\n",
    "                indices, probs = list(map(lambda x: x[1], probs)), list(map(lambda x: x[0], probs))\n",
    "                probs = np.array(probs) / temperature\n",
    "                probs = probs - np.max(probs)\n",
    "                probs = np.exp(probs)\n",
    "                probs = probs / np.sum(probs)\n",
    "                last_token = np.random.choice(indices, p=probs)\n",
    "            decoder_inputs[index_map[i]].append(last_token)\n",
    "            if last_token == end_token or\\\n",
    "                    (max_len is not None and output_len >= max_len) or\\\n",
    "                    _get_max_suffix_repeat_times(decoder_inputs[index_map[i]],\n",
    "                                                 max_repeat * max_repeat_block) >= max_repeat:\n",
    "                outputs[index_map[i]] = decoder_inputs[index_map[i]]\n",
    "    if is_single:\n",
    "        outputs = outputs[0]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20301,
     "status": "ok",
     "timestamp": 1620127313426,
     "user": {
      "displayName": "07360345 07360345",
      "photoUrl": "",
      "userId": "11894900185646097944"
     },
     "user_tz": -480
    },
    "id": "qJB9UE4bHJY3",
    "outputId": "341f7fcf-7e9f-4db1-96f4-cedd738aafba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
