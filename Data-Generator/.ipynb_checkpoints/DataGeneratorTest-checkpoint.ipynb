{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import DataGenerator2_2 as DG2\n",
    "import math\n",
    "import os\n",
    "import DataBuffer as db\n",
    "from random import randrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTestTrainData(filename, data): # e.g., 'test.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestTrainData(filename): # e.g., 'test.npy'\n",
    "    with open(filename, 'rb') as f:\n",
    "        a = np.load(f)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateInputBLKData(datafilename, destDir1, filename1=\"blk_\", block_size=1000, fext=\".npy\", offset=0):\n",
    "    if not os.path.exists(destDir1):\n",
    "        os.mkdir(destDir1)  #create patchbackfiledir case dir\n",
    "        \n",
    "    data = list( loadTestTrainData(datafilename) )\n",
    "    print(\"type(train):\", type(data))\n",
    "    print(\"train[0].shape:\", data[0].shape) #encoder inputs\n",
    "    lines = data[0].shape[0] # 279 lines\n",
    "    \n",
    "    block_numer = math.ceil( lines / block_size )\n",
    "    \n",
    "    for blk_id in range(block_numer):\n",
    "        print(\"blk_idx: \", blk_id)\n",
    "        blk_offset = blk_id*block_size\n",
    "        blk_end = min((blk_id+1)*block_size, lines)\n",
    "        saveTestTrainData(destDir1 + filename1 +str(offset+blk_id) + \".npy\", data[0][blk_offset:blk_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateOutputBLKData(datafilename, destDir1, filename=\"blk_\", block_size=1000, fext=\".npy\"):\n",
    "    if not os.path.exists(destDir1):\n",
    "        os.mkdir(destDir1)  #create patchbackfiledir case dir\n",
    "    \n",
    "    data = list( loadTestTrainData(datafilename) )\n",
    "    print(\"len(data):\", len(data))\n",
    "    print(\"type(data):\", type(data))\n",
    "    print(\"data.shape:\", data[0].shape) #encoder inputs\n",
    "    lines = len(data) # 279 lines\n",
    "    print(\"lines:\", lines) #encoder inputs\n",
    "    \n",
    "    block_numer = math.ceil( lines / block_size )\n",
    "    \n",
    "    for blk_id in range(block_numer):\n",
    "        print(\"blk_idx: \", blk_id)\n",
    "        blk_offset = blk_id*block_size\n",
    "        blk_end = min((blk_id+1)*block_size, lines)\n",
    "        saveTestTrainData(destDir1 + \"blk_\"+str(blk_id) + \".npy\", data[blk_offset:blk_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataID:  87967\n",
      "dataID:  87967\n",
      "dataID:  87967\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d8a25d4917f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_db1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mdata3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_db1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mdata4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_db2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data1 :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data2 :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Code\\Project-AI-JAVA-ANNOTATION-2021\\Data-Generator\\DataBuffer.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, dataID)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock_access_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblk_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataID: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata_block\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_data_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#### Split data to blocks  ##########\n",
    "    \"\"\"\n",
    "    destDir1 = \"model\\\\\"\n",
    "    \n",
    "    for i in range(6):\n",
    "        generateInputBLKData(i, \"x_train_\"+ str(i)+ \".npy\", destDir1, destDir2, block_size = 48048, filename1=\"x_train[0]_\", filename2=\"x_train[1]_\")\n",
    "    \n",
    "    destDir3 = \"Data\\\\DecoderOutput1\\\\\"\n",
    "    destDir4 = \"Data\\\\DecoderOutput2\\\\\"\n",
    "    generateOutputBLKData(\"y_train[0].npy\", destDir3, block_size = 50)\n",
    "    generateOutputBLKData(\"y_train[1].npy\", destDir4, block_size = 50)\n",
    "    \"\"\"\n",
    "######################################################\n",
    "    #for output\n",
    "    input_buffer_params = { \n",
    "        \"data_path\": \"Model-for-training-org\", \n",
    "        \"data_number\":302400,\n",
    "        \"data_type\": int,\n",
    "        \"block_size\": 45000\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # for input\n",
    "    output_buffer_params = {\n",
    "        \"data_path\": [\"Model-for-training-org\", \"Model-for-training-org\"],\n",
    "        \"data_number\":[302400, 302400],\n",
    "        \"data_type\": [int, int],\n",
    "        \"block_size\": [45000, 45000]\n",
    "        }\n",
    "    \n",
    "    input_data_path = input_buffer_params[\"data_path\"]\n",
    "    input_data_number = input_buffer_params[\"data_number\"]\n",
    "    input_data_type = input_buffer_params[\"data_type\"]\n",
    "    input_block_size = input_buffer_params[\"block_size\"]\n",
    "    \n",
    "    output_data_path = output_buffer_params[\"data_path\"]\n",
    "    output_data_number = output_buffer_params[\"data_number\"]\n",
    "    output_data_type = output_buffer_params[\"data_type\"]\n",
    "    output_block_size = output_buffer_params[\"block_size\"]\n",
    "    \n",
    "    input_db1 = db.DataBuffer(input_data_path, input_data_number, input_data_type, input_block_size, file_name = \"x_train_\" )\n",
    "    \n",
    "    output_db1 = db.DataBuffer(output_data_path[0], output_data_number[0],output_data_type[0], output_block_size[0], file_name=\"y_train[0]_\" )\n",
    "    \n",
    "    output_db2 = db.DataBuffer(output_data_path[1], output_data_number[1],output_data_type[1], output_block_size[1], file_name=\"y_train[1]_\")\n",
    "\n",
    "    for i in range(10):\n",
    "        data_id  = randrange(45000)\n",
    "        data1 = input_db1.get_data(data_id)\n",
    "        data3 = output_db1.get_data(data_id)\n",
    "        data4 = output_db2.get_data(data_id)\n",
    "        print(\"data1 :\", (data1))\n",
    "        print(\"data2 :\", (data2))\n",
    "        print(\"data3 :\", (data3))\n",
    "        print(\"data4 :\", (data4))\n",
    "\n",
    "        \"\"\"\n",
    "        data = db1.get_data(200)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        #print(\"data :\", (data))\n",
    "        \n",
    "        data = db1.get_data(10)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        #print(\"data :\", (data))\n",
    "\n",
    "        data = db1.get_data(110)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        #print(\"data :\", (data))\n",
    "        \n",
    "        data = db1.get_data(113)\n",
    "        print(\"data type:\", type(data), \"data.shape: \", data.shape)\n",
    "        \"\"\"\n",
    "    #print(\"data :\", (data))\n",
    "    \n",
    "    #x = DG2.DataGenerator1(buffer_params, [list(range(279)),list(range(279))] , None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
